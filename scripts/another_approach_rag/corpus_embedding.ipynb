{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff421c5-5863-4c21-b70f-10a9daed0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import sent_tokenize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=100, window_size=50):\n",
    "    \"\"\"Split a long text into multiple chunks (passages) with managable sizes.\n",
    "    \n",
    "    Args:\n",
    "        chunk_size (int): Maximum size of a chunk.\n",
    "        window_size (int): Decide how many words are overlapped between two consecutive chunks. Basically #overlapped_words = chunk_size - window_size.\n",
    "    Returns:\n",
    "        str: Multiple chunks of text splitted from initial document text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "\n",
    "    while True:\n",
    "        end_idx = start_idx + chunk_size\n",
    "        chunk = \" \".join(words[start_idx:end_idx])\n",
    "        chunks.append(chunk)\n",
    "        if end_idx >= num_words:\n",
    "            break\n",
    "        start_idx += window_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_corpus(data_dir=\"data/hainong_raw/\"):\n",
    "    \"\"\"Transform a corpus of documents into a corpus of passages.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): directory that contains .txt files, each file contains text content of a wikipedia page.\n",
    "    Returns:\n",
    "        str: A corpus of chunks splitted from multiple initial documents. Each chunk will contain information about (id, title, passage)\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    meta_corpus = []\n",
    "    data_dir = \"data/hainong_raw/\"\n",
    "    filenames = os.listdir(data_dir)\n",
    "    filenames = sorted(filenames)\n",
    "    \n",
    "    _id = 0\n",
    "    docs = {}\n",
    "    for filename in tqdm(filenames):\n",
    "        filepath = data_dir + filename\n",
    "        title = filename.strip(\".md\")\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "            docs[title] = text\n",
    "            text = text.lstrip(title).strip()\n",
    "\n",
    "            # No overlap.\n",
    "            chunks = split_text_into_chunks(text, chunk_size=150, window_size=100)\n",
    "            chunks = [f\"{chunk}\" for chunk in chunks]\n",
    "            meta_chunks = [{\n",
    "                \"title\": title,\n",
    "                \"passage\": chunks[i],\n",
    "                \"id\": _id + i,\n",
    "                \"len\": len(chunks[i].split())\n",
    "            } for i in range(len(chunks))]\n",
    "            _id += len(chunks)\n",
    "            corpus.extend(chunks)\n",
    "            meta_corpus.extend(meta_chunks)\n",
    "    return meta_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf4da0f-df77-4f3e-adb1-7dbae684745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 164.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Corpus size: 184\n",
      ">>> Example passage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meta_corpus = get_corpus(\"../../app/api/data/training_data/\")\n",
    "print(f\">>> Corpus size: {len(meta_corpus)}\")\n",
    "print(f\">>> Example passage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "159b0f8b-51c3-4f66-997e-d3652b800f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "chunk_size = 150\n",
    "chunk_overlap = 50\n",
    "with open(f\"data/corpus_chunks_{chunk_size}_{chunk_overlap}.jsonl\", \"w\") as outfile:\n",
    "    for chunk in meta_corpus:\n",
    "        d = json.dumps(chunk, ensure_ascii=False) + \"\\n\"\n",
    "        outfile.write(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06f7264a-069a-4cd3-82e5-92e6af06093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name VoVanPhuc/sup-SimCSE-VietNamese-phobert-base. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:00<00:00, 330.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "# model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')\n",
    "model = SentenceTransformer('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base')\n",
    "\n",
    "segmented_corpus = [tokenize(example[\"passage\"]) for example in tqdm(meta_corpus)]\n",
    "embeddings_output = model.encode(segmented_corpus)\n",
    "embeddings = embeddings_output/(np.linalg.norm(embeddings_output, axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a8ebaa2-9e0e-40ea-b80f-2d00b6650213",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size\n",
    "chunk_overlap\n",
    "import pickle\n",
    "with open(f'data/corpus_embedding_w150_{chunk_size}_{chunk_overlap}.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e097c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863d01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7c20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
