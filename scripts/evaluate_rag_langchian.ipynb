{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE EVALUATE DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 1: https://huggingface.co/learn/cookbook/rag_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test = [\n",
    "\"2nong_introduction.md\",\n",
    "\"category_trees_C√†_ph√™.md\",\n",
    "\"category_trees_Chu·ªëi_r·∫Ω_qu·∫°t.md\",\n",
    "\"category_trees_D·ª´a.md\",\n",
    "\"category_trees_D·ª´a_n∆∞·ªõc__d·ª´a_l√°.md\",\n",
    "\"category_trees_ƒê·ªßng_ƒë·ªânh.md\",\n",
    "\"category_trees_Hoa_ly.md\",\n",
    "\"category_trees_H·ªì_ti√™u.md\",\n",
    "\"category_trees_Lan_Cattleya.md\",\n",
    "\"category_trees_Lan_Chi.md\",\n",
    "\"category_trees_Lan_chu_ƒë√≠nh.md\",\n",
    "\"category_trees_Lan_d·∫°_h∆∞∆°ng.md\",\n",
    "\"category_trees_Lan_Vanda.md\",\n",
    "\"category_trees_Lay_∆°n.md\",\n",
    "\"category_trees_L·∫ª_b·∫°n__soÃÄ_huy√™ÃÅt__bang_hoa.md\",\n",
    "\"category_trees_L√¥_h√¥Ã£i__nha_ƒëam.md\",\n",
    "\"category_trees_L√∫a.md\",\n",
    "\"category_trees_L·ª•c_biÃÄnh.md\",\n",
    "\"category_trees_M√≠a.md\",\n",
    "\"category_trees_Phong_L·ªôc_Hoa.md\",\n",
    "\"category_trees_S·∫ßu_ri√™ng.md\",\n",
    "\"category_trees_Sen_ƒë√°.md\",\n",
    "\"category_trees_Thi√™n_tu√™ÃÅ.md\",\n",
    "\"category_trees_Th·ªët_n·ªët.md\",\n",
    "\"category_trees_Th·ªßy_tr√∫c__l√°c_d√π.md\",\n",
    "\"category_trees_Th·ªßy_t√πng.md\",\n",
    "\"category_trees_Tr·∫Øc_b√°_di·ªáp.md\",\n",
    "\"category_trees_Tre_m·∫°nh_t√¥ng.md\",\n",
    "\"knowledge_handbooks_3_b∆∞·ªõc_c·∫£i_t·∫°o_ƒë·∫•t_sau_thu_ho·∫°ch_ƒë·ªëi_v·ªõi_v∆∞·ªùn_c√¢y_ƒÉn_tr√°i.md\",\n",
    "\"knowledge_handbooks_B√≥n_ƒë·∫°m_cho_l√∫a_v√†o_th·ªùi_k·ª≥_n√†o_l√†_t·ªët_nh·∫•t.md\",\n",
    "\"knowledge_handbooks_B√≥n_v√¥i_ƒë√∫ng_quy_tr√¨nh_cho_v∆∞·ªùn_c√¢y_ƒÉn_tr√°i.md\",\n",
    "\"knowledge_handbooks_C√°ch_b√≥n_l√≥t_cho_c√†_ph√™_tr·ªìng_m·ªõi.md\",\n",
    "\"knowledge_handbooks_C√°ch_b√≥n_ph√¢n_chu·ªìng_cho_rau_m√†u_hi·ªáu_qu·∫£_nh·∫•t.md\",\n",
    "\"knowledge_handbooks_C√°ch_chƒÉm_s√≥c_l√∫a_giai_ƒëo·∫°n_ƒë√≤ng_tr·ªï_gi√∫p_tƒÉng_nƒÉng_su·∫•t_hi·ªáu_qu·∫£.md\",\n",
    "\"knowledge_handbooks_C√°ch_kh·∫Øc_ph·ª•c_b∆∞·ªüi_da_xanh_b·ªã_v√†ng_ƒë·ªçt.md\",\n",
    "\"knowledge_handbooks_C√°ch_kh·∫Øc_ph·ª•c_hi·ªán_t∆∞·ª£ng_n·ª©t_tr√°i_tr√™n_c√¢y_tr·ªìng.md\",\n",
    "\"knowledge_handbooks_C√°ch_kh·∫Øc_ph·ª•c_m√≠t_x∆°_ƒëen.md\",\n",
    "\"knowledge_handbooks_C√°ch_l√†m_cho_hoa_c√†_ph√™_ra_ƒë·ªìng_lo·∫°t.md\",\n",
    "\"knowledge_handbooks_C√°ch_ph√≤ng_tr·ªã_s√¢u_v·∫Ω_b√πa_tr√™n_c√¢y_cam.md\",\n",
    "\"knowledge_handbooks_C√°ch_ph√≤ng_tr·ª´_b·ªánh_kh√¥_c√†nh_kh√¥_qu·∫£_g√¢y_h·∫°i_c√¢y_c√†_ph√™.md\",\n",
    "\"knowledge_handbooks_C√°ch_tr·ªã_s√¢u_v·∫Ω_b√πa_tr√™n_b∆∞·ªüi.md\",\n",
    "\"knowledge_handbooks_C√°ch_t∆∞·ªõi_n∆∞·ªõc_cho_m√¥_h√¨nh_tr·ªìng_h·ªì_ti√™u_xen_c√†_ph√™.md\",\n",
    "\"knowledge_handbooks_Cam_s√†nh_ra_b√¥ng__b·ªã_m∆∞a_nhi·ªÅu_c·∫ßn_l√†m_g√¨.md\",\n",
    "\"knowledge_handbooks_C·∫ßn_l√†m_g√¨_sau_khi_thu_ho·∫°ch_s·∫ßu_ri√™ng.md\",\n",
    "\"knowledge_handbooks_C√¢y_c√†_ph√™_gi√†_c·ªói_th√¨_ph·∫£i_l√†m_th·∫ø_n√†o.md\",\n",
    "\"knowledge_handbooks_C√≥_c·∫ßn_b√≥n_ph√¢n_h·ªØu_c∆°_cho_ƒë·∫•t_ph√®n_kh√¥ng.md\",\n",
    "\"knowledge_handbooks_D·ª©t_ƒëi·ªÉm_r·ªáp_s√°p__r·∫ßy_tr·∫Øng_·ªü_ph·∫ßn_r·ªÖ.md\",\n",
    "\"knowledge_handbooks_Gi√¥ÃÅng_caÃÄ_ph√™_v·ªëi_naÃÄo_ch√¢ÃÅt_l∆∞∆°Ã£ng_t·ªët_nh√¢ÃÅt_hi·ªán_nay.md\",\n",
    "\"knowledge_handbooks_Gi·ªØ_·∫©m_cho_ƒë·∫•t_trong_m√πa_kh√¥_nh∆∞_th·∫ø_n√†o.md\",\n",
    "\"knowledge_handbooks_K√≠ch_th∆∞·ªõc_ph·∫≥ng_c·ªßa_b·∫ßu_∆∞∆°m_c√†_ph√™_ra_sao.md\",\n",
    "\"knowledge_handbooks_Kinh_nghi·ªám_chƒÉm_s√≥c_s·∫ßu_ri√™ng_giai_ƒëo·∫°n_nu√¥i_tr√°i_non_hi·ªáu_qu·∫£.md\",\n",
    "\"knowledge_handbooks_K·ªπ_thu·∫≠t_c·∫Øt_t·ªâa_c√†nh_v√†_t·∫°o_t√°n_cho_c√†_ph√™.md\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "list_total_doc = os.listdir(folder_doc)\n",
    "for each_doc in list_total_doc:\n",
    "    if each_doc not in documents_test:\n",
    "        os.remove(folder_doc + each_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "documents_content = []\n",
    "for name_doc in documents_test:\n",
    "    with open(folder_doc + name_doc, \"r\") as f:\n",
    "        documents_content.append((name_doc,f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs = [LangchainDocument(page_content=doc[1], metadata={\"source\": doc[0]}) for doc in tqdm(documents_content)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# repo_id = \"noah-ai/mt5-base-question-generation-vi\"\n",
    "# repo_id= \"NlpHUST/gpt2-vietnamese\"\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token='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',\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={ \n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( docs_processed[5].page_content, \"\\n________________\", call_llm(llm_client, docs_processed[5].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_generation_prompt = \"\"\"\n",
    "# Your task is to write a factoid question and an answer given a context.\n",
    "# Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "# Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "# This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Output:::\n",
    "# Factoid question: (your factoid question)\n",
    "# Answer: (your answer to the factoid question)\n",
    "\n",
    "# Now here is the context.\n",
    "\n",
    "# Context: {context}\\n\n",
    "# Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\" Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt m·ªôt c√¢u h·ªèi th·ª±c t·∫ø v√† m·ªôt c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh.\n",
    "C√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n ph·∫£i ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng m·ªôt ƒëo·∫°n th√¥ng tin th·ª±c t·∫ø c·ª• th·ªÉ, ng·∫Øn g·ªçn t·ª´ ng·ªØ c·∫£nh.\n",
    "C√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n ph·∫£i ƒë∆∞·ª£c x√¢y d·ª±ng theo phong c√°ch gi·ªëng nh∆∞ nh·ªØng c√¢u h·ªèi m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi trong c√¥ng c·ª• t√¨m ki·∫øm.\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† c√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n KH√îNG PH·∫¢I ƒë·ªÅ c·∫≠p ƒë·∫øn nh·ªØng th·ª© nh∆∞ \"theo ƒëo·∫°n vƒÉn\" ho·∫∑c \"ng·ªØ c·∫£nh\".\n",
    "To√†n b·ªô c√¢u tr·∫£ l·ªùi v√† c√¢u h·ªèi ph·∫£i ƒë∆∞·ª£c vi·∫øt b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "K·∫øt qu·∫£:::\n",
    "C√¢u h·ªèi th·ª±c t·∫ø: (c√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n)\n",
    "Tr·∫£ l·ªùi: (c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n cho c√¢u h·ªèi th·ª±c t·∫ø)\n",
    "\n",
    "ƒê√¢y l√† b·ªëi c·∫£nh.\n",
    "B·ªëi c·∫£nh: {context}\\n\n",
    "K·∫øt qu·∫£:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 50  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    # print(output_QA_couple)\n",
    "    # print(\"_______________________________________\")\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"C√¢u h·ªèi th·ª±c t·∫ø: \")[-1].split(\"Tr·∫£ l·ªùi: \")[0]\n",
    "        answer = output_QA_couple.split(\"Tr·∫£ l·ªùi: \")[-1]\n",
    "        print(question,answer )\n",
    "        # assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pd.DataFrame(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_groundedness_critique_prompt = \"\"\"\n",
    "# You will be given a context and a question.\n",
    "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here are the question and context.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Context: {context}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_relevance_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_standalone_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt b·ªëi c·∫£nh v√† m·ªôt c√¢u h·ªèi.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë∆∞a ra 'ƒêi·ªÉm ƒë√°nh gi√°' cho ƒëi·ªÉm m·ª©c ƒë·ªô m·ªôt ng∆∞·ªùi c√≥ th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi ƒë√£ cho m·ªôt c√°ch r√µ r√†ng v·ªõi b·ªëi c·∫£nh nh·∫•t ƒë·ªãnh.\n",
    "ƒê∆∞a ra c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n theo thang ƒëi·ªÉm t·ª´ 1 ƒë·∫øn 5, trong ƒë√≥ 1 c√≥ nghƒ©a l√† c√¢u h·ªèi ho√†n to√†n kh√¥ng th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c trong b·ªëi c·∫£nh v√† 5 c√≥ nghƒ©a l√† c√¢u h·ªèi c√≥ th·ªÉ tr·∫£ l·ªùi r√µ r√†ng v√† r√µ r√†ng v·ªõi b·ªëi c·∫£nh.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "Tr·∫£ l·ªùi:::\n",
    "ƒê√°nh gi√°: (l√Ω do ƒë√°nh gi√° c·ªßa b·∫°n, d∆∞·ªõi d·∫°ng vƒÉn b·∫£n)\n",
    "ƒêi·ªÉm ƒë√°nh gi√°: (ƒêi·ªÉm ƒë√°nh gi√° c·ªßa b·∫°n, t√≠nh b·∫±ng s·ªë t·ª´ 1 ƒë·∫øn 5)\n",
    "\n",
    "B·∫°n PH·∫¢I cung c·∫•p c√°c gi√° tr·ªã cho 'ƒê√°nh gi√°:' v√† 'ƒêi·ªÉm ƒë√°nh gi√°:' trong c√¢u tr·∫£ l·ªùi c·ªßa m√¨nh.\n",
    "\n",
    "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi v√† b·ªëi c·∫£nh.\n",
    "\n",
    "C√¢u h·ªèi: {question}\\n\n",
    "B·ªëi c·∫£nh: {context}\\n\n",
    "Tr·∫£ l·ªùi::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "B·∫°n s·∫Ω ƒë∆∞·ª£c ƒë∆∞a ra m·ªôt c√¢u h·ªèi.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p 'ƒêi·ªÉm x·∫øp h·∫°ng' th·ªÉ hi·ªán m·ª©c ƒë·ªô h·ªØu √≠ch c·ªßa c√¢u h·ªèi n√†y ƒë·ªëi v·ªõi c√°c nh√† ph√°t tri·ªÉn m√°y h·ªçc ƒëang x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng NLP v·ªõi h·ªá sinh th√°i Hugging Face.\n",
    "ƒê∆∞a ra c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n theo thang ƒëi·ªÉm t·ª´ 1 ƒë·∫øn 5, trong ƒë√≥ 1 c√≥ nghƒ©a l√† c√¢u h·ªèi kh√¥ng h·ªØu √≠ch ch√∫t n√†o v√† 5 c√≥ nghƒ©a l√† c√¢u h·ªèi c·ª±c k·ª≥ h·ªØu √≠ch.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "Tr·∫£ l·ªùi:::\n",
    "ƒê√°nh gi√°: (l√Ω do ƒë√°nh gi√° c·ªßa b·∫°n, d∆∞·ªõi d·∫°ng vƒÉn b·∫£n)\n",
    "ƒêi·ªÉm ƒë√°nh gi√°: (ƒêi·ªÉm ƒë√°nh gi√° c·ªßa b·∫°n, t√≠nh b·∫±ng s·ªë t·ª´ 1 ƒë·∫øn 5)\n",
    "\n",
    "B·∫°n PH·∫¢I cung c·∫•p c√°c gi√° tr·ªã cho 'ƒê√°nh gi√°:' v√† 'ƒêi·ªÉm ƒë√°nh gi√°:' trong c√¢u tr·∫£ l·ªùi c·ªßa m√¨nh.\n",
    "\n",
    "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi.\n",
    "\n",
    "C√¢u h·ªèi: {question}\\n\n",
    "Tr·∫£ l·ªùi::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "B·∫°n s·∫Ω ƒë∆∞·ª£c ƒë∆∞a ra m·ªôt c√¢u h·ªèi.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p 'ƒêi·ªÉm x·∫øp h·∫°ng' th·ªÉ hi·ªán m·ª©c ƒë·ªô ƒë·ªôc l·∫≠p c·ªßa c√¢u h·ªèi n√†y v·ªõi b·ªëi c·∫£nh.\n",
    "ƒê∆∞a ra c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n theo thang ƒëi·ªÉm t·ª´ 1 ƒë·∫øn 5, trong ƒë√≥ 1 c√≥ nghƒ©a l√† c√¢u h·ªèi ph·ª• thu·ªôc v√†o th√¥ng tin b·ªï sung ƒë·ªÉ hi·ªÉu. 5 c√≥ nghƒ©a l√† b·∫£n th√¢n c√¢u h·ªèi c√≥ √Ω nghƒ©a v√† c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c.\n",
    "V√≠ d·ª•: n·∫øu c√¢u h·ªèi ƒë·ªÅ c·∫≠p ƒë·∫øn m·ªôt c√†i ƒë·∫∑t c·ª• th·ªÉ, nh∆∞ 'trong b·ªëi c·∫£nh' ho·∫∑c 'trong t√†i li·ªáu' th√¨ x·∫øp h·∫°ng ph·∫£i l√† 1.\n",
    "C√°c c√¢u h·ªèi c√≥ th·ªÉ ch·ª©a c√°c danh t·ª´ ho·∫∑c t·ª´ vi·∫øt t·∫Øt k·ªπ thu·∫≠t kh√≥ hi·ªÉu nh∆∞ Gradio, Hub, Hugging Face v√† v·∫´n ·ªü m·ª©c 5: n√≥ ch·ªâ ƒë∆°n gi·∫£n l√† ph·∫£i r√µ r√†ng ƒë·ªëi v·ªõi ng∆∞·ªùi v·∫≠n h√†nh c√≥ quy·ªÅn truy c·∫≠p v√†o t√†i li·ªáu v·ªÅ n·ªôi dung c√¢u h·ªèi.\n",
    "\n",
    "V√≠ d·ª•: \"T·ªï ch·ª©c Hugging Face nƒÉm 2023 c√≥ bao nhi√™u ng∆∞·ªùi d√πng?\" s·∫Ω nh·∫≠n ƒë∆∞·ª£c ƒëi·ªÉm 1, v√¨ c√≥ s·ª± ƒë·ªÅ c·∫≠p ng·∫ßm ƒë·∫øn m·ªôt ng·ªØ c·∫£nh, do ƒë√≥ c√¢u h·ªèi kh√¥ng ƒë·ªôc l·∫≠p v·ªõi ng·ªØ c·∫£nh.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "Tr·∫£ l·ªùi:::\n",
    "ƒê√°nh gi√°: (l√Ω do ƒë√°nh gi√° c·ªßa b·∫°n, d∆∞·ªõi d·∫°ng vƒÉn b·∫£n)\n",
    "ƒêi·ªÉm ƒë√°nh gi√°: (ƒêi·ªÉm ƒë√°nh gi√° c·ªßa b·∫°n, t√≠nh b·∫±ng s·ªë t·ª´ 1 ƒë·∫øn 5)\n",
    "\n",
    "B·∫°n PH·∫¢I cung c·∫•p c√°c gi√° tr·ªã cho 'ƒê√°nh gi√°:' v√† 'ƒêi·ªÉm ƒë√°nh gi√°:' trong c√¢u tr·∫£ l·ªùi c·ªßa m√¨nh.\n",
    "\n",
    "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi.\n",
    "\n",
    "C√¢u h·ªèi: {question}\\n\n",
    "Tr·∫£ l·ªùi::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "# outputs_2 = outputs[1:4]\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                evaluation.split(\"ƒêi·ªÉm ƒë√°nh gi√°: \")[-1].strip(),\n",
    "                evaluation.split(\"ƒêi·ªÉm ƒë√°nh gi√°: \")[-2].split(\"ƒê√°nh gi√°: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # print(\"______________________________________________________\")\n",
    "        continue\n",
    "    # print(output)\n",
    "    # print(\"______________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 20)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "generated_questions_process = generated_questions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_score(score_string):\n",
    "    if score_string is np.nan:\n",
    "        return np.nan\n",
    "    return int(str(score_string)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process.iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process['groundedness_score'] = generated_questions_process['groundedness_score'].apply(process_score)\n",
    "generated_questions_process['relevance_score'] = generated_questions_process['relevance_score'].apply(process_score)\n",
    "generated_questions_process['standalone_score'] = generated_questions_process['standalone_score'].apply(process_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions_process[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions_process_2 = generated_questions_process.loc[\n",
    "    (generated_questions_process[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions_process[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions_process[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions_process_2[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions_process_2, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.to_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG PIPELINE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../app/api/')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import *\n",
    "from helpers import *\n",
    "from models import *\n",
    "from config import *\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "import logging\n",
    "\n",
    "openai.util.logging.getLogger().setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER=\"postgres\"\n",
    "POSTGRES_PASSWORD=\"postgres\"\n",
    "POSTGRES_DB=\"postgres\"\n",
    "PGVECTOR_ADD_INDEX=True\n",
    "\n",
    "# DB_HOST=localhost\n",
    "DB_HOST=\"localhost\"\n",
    "DB_PORT=5432\n",
    "# DB_PORT=5132\n",
    "DB_USER=\"api\"\n",
    "DB_NAME=\"api\"\n",
    "DB_PASSWORD=123\n",
    "VECTOR_EMBEDDINGS_COUNT = 1024\n",
    "OPENAI_API_KEY='sk-proj-SP7z7Y29wCjNbtRnoRoRT3BlbkFJM4oUr3l8Mv0X6vBdKqF7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SU_DSN = (\n",
    "    f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engine_test(dsn: str = SU_DSN):\n",
    "    return create_engine(dsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_random_agent():\n",
    "    return random.choice(AGENT_NAMES)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Get the count of tokens used\n",
    "# ----------------------------\n",
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def get_token_count(text: str):\n",
    "    if not text:\n",
    "        return 0\n",
    "\n",
    "    return OpenAI().get_num_tokens(text=text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(VECTOR_EMBEDDINGS_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Query embedding search for similar documents\n",
    "# --------------------------------------------\n",
    "def get_nodes_by_embedding_test(\n",
    "    embeddings: List[float],\n",
    "    k: int = LLM_MIN_NODE_LIMIT,\n",
    "    distance_strategy: Optional[DISTANCE_STRATEGY] = LLM_DEFAULT_DISTANCE_STRATEGY,\n",
    "    distance_threshold: Optional[float] = LLM_DISTANCE_THRESHOLD,\n",
    "    session: Optional[Session] = None,\n",
    ") -> List[Node]:\n",
    "    # Convert embeddings array into sql string\n",
    "    embeddings_str = str(embeddings)\n",
    "\n",
    "    if distance_strategy == DISTANCE_STRATEGY.EUCLIDEAN:\n",
    "        distance_fn = \"match_node_euclidean\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.COSINE:\n",
    "        distance_fn = \"match_node_cosine\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.MAX_INNER_PRODUCT:\n",
    "        distance_fn = \"match_node_max_inner_product\"\n",
    "    else:\n",
    "        raise Exception(f\"Invalid distance strategy {distance_strategy}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Lets do a similarity search\n",
    "    # ---------------------------\n",
    "    sql = f\"\"\"SELECT * FROM {distance_fn}(\n",
    "    '{embeddings_str}'::vector({VECTOR_EMBEDDINGS_COUNT}),\n",
    "    {float(distance_threshold)}::double precision,\n",
    "    {int(k)});\"\"\"\n",
    "    print(sql)\n",
    "    # logger.debug(f'üîç Query: {sql}')\n",
    "\n",
    "    # Execute query, convert results to Node objects\n",
    "    if not session:\n",
    "        with Session(get_engine_test()) as session:\n",
    "            nodes = session.exec(text(sql)).all()\n",
    "    else:\n",
    "        nodes = session.exec(text(sql)).all()\n",
    "    return [Node.by_uuid(str(node[0])) for node in nodes] if nodes else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_test(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=LLM_CHUNK_SIZE,\n",
    "                    chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                    add_start_index=True,\n",
    "                    separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                )\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    # https://github.com/hwchase17/langchain/blob/d18b0caf0e00414e066c9903c8df72bb5bcf9998/langchain/embeddings/openai.py#L219\n",
    "    embed_func = OpenAIEmbeddings(openai_api_key='sk-proj-SP7z7Y29wCjNbtRnoRoRT3BlbkFJM4oUr3l8Mv0X6vBdKqF7')\n",
    "    # print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "    \n",
    "    # print(embed_func)\n",
    "    # print(arr_documents)\n",
    "    embeddings = embed_func.embed_documents(\n",
    "        texts=arr_documents, chunk_size=512\n",
    "    )\n",
    "    \n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "def get_embeddings_test_2(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=LLM_CHUNK_SIZE,\n",
    "                    chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                    add_start_index=True,\n",
    "                    separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                )\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    # https://github.com/hwchase17/langchain/blob/d18b0caf0e00414e066c9903c8df72bb5bcf9998/langchain/embeddings/openai.py#L219\n",
    "    # embed_func = OpenAIEmbeddings(openai_api_key='sk-proj-7Ueh52MTS1sPLmpN32UMT3BlbkFJBS2DTMyHkkJaNU44zAsl')\n",
    "    # print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "    \n",
    "    # print(embed_func)\n",
    "    # print(arr_documents)\n",
    "    \n",
    "    # embeddings = embed_func.embed_documents(\n",
    "        # texts=arr_documents, chunk_size=512\n",
    "    # )\n",
    "\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/text-embedding-ada-002')\n",
    "    embeddings = [tokenizer.encode(text=doc) for doc in arr_documents]\n",
    "    \n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_llm_response_test(\n",
    "    query_str: str,\n",
    "    model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO,\n",
    "    temperature: Optional[float] = LLM_DEFAULT_TEMPERATURE,\n",
    "    max_output_tokens: Optional[int] = LLM_MAX_OUTPUT_TOKENS,\n",
    "    prefix_messages: Optional[List[dict]] = None,\n",
    "):\n",
    "    # print(f\"111111111111---{query_str}\\n2222222222---{prefix_messages}\")\n",
    "\n",
    "    llm = OpenAI(\n",
    "        temperature=temperature,\n",
    "        model_name=model.model_name\n",
    "        if isinstance(model, LLM_MODELS)\n",
    "        else LLM_MODELS.GPT_35_TURBO.model_name,\n",
    "        max_tokens=max_output_tokens,\n",
    "        prefix_messages=prefix_messages,\n",
    "        request_timeout=10,\n",
    "    )\n",
    "    try:\n",
    "        result = llm(prompt=query_str)\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        logger.error(f\"üö® LLM error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"LLM error: {e}\")\n",
    "    # logger.debug(f\"üí¨ LLM result: {str(result)}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Retrieve prompt template\n",
    "# ------------------------\n",
    "def get_prompt_template_2(\n",
    "    user_query: str = None,\n",
    "    context_str: str = None,\n",
    "    project: Optional[Project] = None,\n",
    "    organization: Optional[Organization] = None,\n",
    "    agent: str = None,\n",
    ") -> str:\n",
    "    agent = f\"{agent}, \" if agent else \"\"\n",
    "    user_query = user_query if user_query else \"\"\n",
    "    context_str = context_str if context_str else \"\"\n",
    "    organization = (\n",
    "        project.organization.display_name\n",
    "        if project\n",
    "        else organization.display_name\n",
    "        if organization\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    if not context_str or not user_query:\n",
    "        raise ValueError(\n",
    "            \"Missing required arguments context_str, user_query, organization, agent\"\n",
    "        )\n",
    "    system_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"[AGENT]:\n",
    "    T√¥i s·∫Ω tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa [USER] ch·ªâ b·∫±ng c√°ch s·ª≠ d·ª•ng  [DOCUMENT] v√† tu√¢n theo [Quy t·∫Øc].\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context_str}\n",
    "\n",
    "    [QUY T·∫ÆC]:\n",
    "    T√¥i s·∫Ω ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng b·∫±ng  [DOCUMENT] ƒë∆∞·ª£c cung c·∫•p. T√¥i s·∫Ω tu√¢n th·ªß c√°c quy t·∫Øc sau:\n",
    "    - T√¥i l√† nh√¢n vi√™n h·ªó tr·ª£ kh√°ch h√†ng t·ªët nh·∫•t hi·ªán nay\n",
    "    - T√¥i s·∫Ω tr·∫£ l·ªùi to√†n b·ªô n·ªôi dung trong [DOCUMENT]\n",
    "    - T√¥i kh√¥ng bao gi·ªù n√≥i d·ªëi hay b·ªãa ra nh·ªØng c√¢u tr·∫£ l·ªùi kh√¥ng ƒë∆∞·ª£c n√™u r√µ r√†ng trong [DOCUMENT]\n",
    "    - N·∫øu t√¥i kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi ho·∫∑c c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ r√µ r√†ng trong [DOCUMENT], t√¥i s·∫Ω n√≥i: \"T√¥i xin l·ªói, t√¥i kh√¥ng bi·∫øt ph·∫£i tr·ª£ gi√∫p ƒëi·ªÅu ƒë√≥ nh∆∞ th·∫ø n√†o\".\n",
    "    - T√¥i lu√¥n gi·ªØ c√¢u tr·∫£ l·ªùi d√†i, ph√π h·ª£p v√† s√∫c t√≠ch.\n",
    "    - T√¥i s·∫Ω lu√¥n ph·∫£n h·ªìi ·ªü ƒë·ªãnh d·∫°ng JSON b·∫±ng c√°c kh√≥a sau: \"message\" ph·∫£n h·ªìi c·ªßa t√¥i cho ng∆∞·ªùi d√πng.\n",
    "    \"\"\",\n",
    "            }\n",
    "        ]\n",
    "#     system_prompt = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": f\"\"\"[AGENT]:\n",
    "#  I will answer the [USER] questions using only the [DOCUMENT] and following the [RULES].\n",
    "\n",
    "# [DOCUMENT]:\n",
    "# {context_str}\n",
    "\n",
    "# [RULES]:\n",
    "# I will answer the user's questions using only the [DOCUMENT] provided. I will abide by the following rules:\n",
    "# - I am a kind and helpful human, the best customer support agent in existence\n",
    "# - I will answer all content  in [DOCUMENT]\n",
    "# - I never lie or invent answers not explicitly provided in [DOCUMENT]\n",
    "# - If I am unsure of the answer response or the answer is not explicitly contained in [DOCUMENT], I will say: \"I apologize, I'm not sure how to help with that\".\n",
    "# - I always keep my answers long, relevant and concise.\n",
    "# - I will always respond in JSON format with the following keys: \"message\" my response to the user, \"tags\" an array of short labels categorizing user input, \"is_escalate\" a boolean, returning false if I am unsure and true if I do have a relevant answer\n",
    "# \"\"\",\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "    return (system_prompt, f\"[USER]:\\n{user_query}\")\n",
    "# f\"[USER]:\\n{user_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "\n",
    "tokenizer_bartpho = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bartbert(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "\n",
    "    logger.debug(documents)\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=LLM_CHUNK_SIZE,\n",
    "                        chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                        add_start_index=True,\n",
    "                        separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                    )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    # https://github.com/hwchase17/langchain/blob/d18b0caf0e00414e066c9903c8df72bb5bcf9998/langchain/embeddings/openai.py#L219\n",
    "\n",
    "    \n",
    "    # output_segment = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    # output_segment = [each[0] for each in output_segment]\n",
    "\n",
    "    \n",
    "    token_output = torch.tensor(tokenizer_bartpho(arr_documents,padding='max_length', truncation=True, max_length=1024)['input_ids'])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = bartpho(token_output)\n",
    "\n",
    "    last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "    embeddings = torch.mean(last_hidden_state, dim=1)\n",
    "    \n",
    "    # embed_func = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    # # print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "    \n",
    "    # # print(embed_func)\n",
    "    # # print(arr_documents)\n",
    "    # embeddings = embed_func.embed_documents(\n",
    "    #     texts=arr_documents, chunk_size=512\n",
    "    # )\n",
    "\n",
    "\n",
    "    # tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/text-embedding-ada-002')\n",
    "    # embeddings = [tokenizer.encode(text=doc) for doc in arr_documents]\n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'L√†m th·∫ø n√†o ƒë·ªÉ t·∫°o ƒëi·ªÅu ki·ªán ƒë·∫£m b·∫£o tho√°t n∆∞·ªõc t·ªët v√† h·∫°n ch·∫ø ng·∫≠p √∫ng cho c√¢y s·∫ßu ri√™ng?\\n'\n",
    "arr_documents, embeddings = get_embeddings_bartbert(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance(question, embedding_question = None, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'L√†m th·∫ø n√†o ƒë·ªÉ t·∫°o ƒëi·ªÅu ki·ªán ƒë·∫£m b·∫£o tho√°t n∆∞·ªõc t·ªët v√† h·∫°n ch·∫ø ng·∫≠p √∫ng cho c√¢y s·∫ßu ri√™ng?\\n'\n",
    "    # if embedding_question is None:\n",
    "    #     try:\n",
    "    #         # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    #         arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    #     except Exception as e: \n",
    "    #         print(e)\n",
    "    #         return None, None ,None\n",
    "    #     query_embeddings = embeddings[0].tolist()\n",
    "    # else:\n",
    "    #     query_embeddings = embedding_question\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    \n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        document = session.get(Node, nodes[0].id).document\n",
    "        if document not in list_doc:\n",
    "            list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        system_prompt, user_prompt = get_prompt_template_2(\n",
    "            user_query=question,\n",
    "            context_str=context_str,\n",
    "            project=project,\n",
    "            organization=organization,\n",
    "            # agent=agent_name,\n",
    "        )\n",
    "        try:\n",
    "            llm_response = json.loads(\n",
    "                            retrieve_llm_response_test(\n",
    "                            user_prompt,\n",
    "                            model=model,\n",
    "                            # max_output_tokens=256,\n",
    "                            prefix_messages=system_prompt,\n",
    "                            )\n",
    "                            )\n",
    "            return list_relevant_docs, llm_response.get('message'), query_embeddings\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            return None ,None, query_embeddings\n",
    "\n",
    "    return None, None, query_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "# # try:  # load previous generations if they exist\n",
    "# #     with open(output_file, \"r\") as f:\n",
    "# #         outputs = json.load(f)\n",
    "# # except:\n",
    "# #     outputs = []\n",
    "# number_error = 0\n",
    "# while number_error < 1:\n",
    "#     # number_error+=1 \n",
    "#     outputs = []\n",
    "#     # ds_continue = generated_questions.iloc[45:]\n",
    "\n",
    "#     for index in tqdm(range(len(ds))):\n",
    "#         if number_error >10:\n",
    "#             break\n",
    "#         example = ds.iloc[index]\n",
    "#         question = example[\"question\"]\n",
    "#         embedding_question = example[\"embedding_question\"]\n",
    "#         print(example['generated_answer'],111111111111111111)\n",
    "#         if example['generated_answer'] is not None:\n",
    "#             continue\n",
    "#         relevant_docs, answer,embedding_question  = test_rag_performance(question,embedding_question)\n",
    "#         if answer is None:\n",
    "#             number_error+=1\n",
    "\n",
    "#             time.sleep(1)\n",
    "#             continue\n",
    "#             break\n",
    "#         print(\"=======================================================\")\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Answer: {answer}\")\n",
    "#         print(f'True answer: {example[\"answer\"]}')\n",
    "#         ds.at[index,'generated_answer'] = answer\n",
    "#         ds.at[index,'retrieved_docs'] = relevant_docs\n",
    "#         ds.at[index,'embedding_question'] = embedding_question\n",
    "\n",
    "#         # result = {\n",
    "#         #     \"question\": question,\n",
    "#         #     \"true_answer\": example[\"answer\"],\n",
    "#         #     \"source_doc\": example[\"source_doc\"],\n",
    "#         #     \"generated_answer\": answer,\n",
    "#         #     \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "#         # }\n",
    "#         # if result not in outputs:\n",
    "#         #     outputs.append(result)\n",
    "\n",
    "#         # result = {\n",
    "#         #     \"question\": question,\n",
    "#         #     \"true_answer\": example[\"answer\"],\n",
    "#         #     \"source_doc\": example[\"source_doc\"],\n",
    "#         #     \"generated_answer\": None,\n",
    "#         #     \"retrieved_docs\": None,\n",
    "#         # }\n",
    "        \n",
    "#         # if result not in outputs:\n",
    "#         #     outputs.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\" Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt m·ªôt c√¢u tr·∫£ l·ªùi ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng d·ª±a tr√™n b·ªëi c·∫£nh.\n",
    "C√¢u tr·∫£ l·ªùi to√†n b·ªô n·ªôi dung trong b·ªëi  c·∫£nh\n",
    "B·∫°n kh√¥ng bao gi·ªù n√≥i d·ªëi hay b·ªãa ra nh·ªØng c√¢u tr·∫£ l·ªùi kh√¥ng ƒë∆∞·ª£c n√™u r√µ r√†ng trong b·ªëi c·∫£nh\n",
    "N·∫øu b·∫°n kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi ho·∫∑c c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ r√µ r√†ng trong b·ªëi c·∫£nh, b·∫°n s·∫Ω n√≥i: \"T√¥i xin l·ªói, t√¥i kh√¥ng bi·∫øt ph·∫£i tr·ª£ gi√∫p ƒëi·ªÅu ƒë√≥ nh∆∞ th·∫ø n√†o\".\n",
    "B·∫°n lu√¥n gi·ªØ c√¢u tr·∫£ l·ªùi d√†i, ph√π h·ª£p v√† ch√≠nh xa\n",
    "C√¢u tr·∫£ l·ªùi b·∫±ng m·ªôt ƒëo·∫°n th√¥ng tin th·ª±c t·∫ø c·ª• th·ªÉ, ng·∫Øn g·ªçn t·ª´ ng·ªØ c·∫£nh.\n",
    "To√†n b·ªô c√¢u tr·∫£ l·ªùi v√† c√¢u h·ªèi ph·∫£i ƒë∆∞·ª£c vi·∫øt b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "K·∫øt qu·∫£:::\n",
    "Tr·∫£ l·ªùi: (c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n cho c√¢u h·ªèi th·ª±c t·∫ø)\n",
    "\n",
    "ƒê√¢y l√† b·ªëi c·∫£nh.\n",
    "B·ªëi c·∫£nh: {context}\\n\n",
    "ƒê√¢y l√† c√¢u h·ªèi.\n",
    "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}\\n\n",
    "K·∫øt qu·∫£:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\"\n",
    "   [INST]\n",
    "    B·∫°n s·∫Ω tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa [USER] ch·ªâ b·∫±ng c√°ch s·ª≠ d·ª•ng  [DOCUMENT] v√† tu√¢n theo [Quy t·∫Øc].\n",
    "    [USER]:\\n{question}\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context}\n",
    "\n",
    "    [QUY T·∫ÆC]:\n",
    "    T√¥i s·∫Ω ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng b·∫±ng  [DOCUMENT] ƒë∆∞·ª£c cung c·∫•p. T√¥i s·∫Ω tu√¢n th·ªß c√°c quy t·∫Øc sau:\n",
    "    - B·∫°n l√† nh√¢n vi√™n h·ªó tr·ª£ kh√°ch h√†ng t·ªët nh·∫•t hi·ªán nay\n",
    "    - B·∫°n s·∫Ω tr·∫£ l·ªùi to√†n b·ªô n·ªôi dung trong [DOCUMENT]\n",
    "    - B·∫°n kh√¥ng bao gi·ªù n√≥i d·ªëi hay b·ªãa ra nh·ªØng c√¢u tr·∫£ l·ªùi kh√¥ng ƒë∆∞·ª£c n√™u r√µ r√†ng trong [DOCUMENT]\n",
    "    - N·∫øu B·∫°n kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi ho·∫∑c c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ r√µ r√†ng trong [DOCUMENT], B·∫°n s·∫Ω n√≥i: \"T√¥i xin l·ªói, t√¥i kh√¥ng bi·∫øt ph·∫£i tr·ª£ gi√∫p ƒëi·ªÅu ƒë√≥ nh∆∞ th·∫ø n√†o\".\n",
    "    - B·∫°n lu√¥n gi·ªØ c√¢u tr·∫£ l·ªùi d√†i, ph√π h·ª£p v√† s√∫c t√≠ch.\n",
    "    - B·∫°n s·∫Ω lu√¥n ph·∫£n h·ªìi k·∫øt qu·∫£ c·ªßa b·∫°n ·ªü ƒë·ªãnh d·∫°ng JSON \n",
    "    - Ch·ªâ c·∫ßn t·∫°o ƒë·ªëi t∆∞·ª£ng JSON b·∫±ng kh√≥a sau: \"message\" l√† ph·∫£n h·ªìi c·ªßa b·∫°n  cho ng∆∞·ªùi d√πng. Kh√¥ng c·∫ßn gi·∫£i th√≠ch:\n",
    "    [/INST]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance_2(question, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'L√†m th·∫ø n√†o ƒë·ªÉ t·∫°o ƒëi·ªÅu ki·ªán ƒë·∫£m b·∫£o tho√°t n∆∞·ªõc t·ªët v√† h·∫°n ch·∫ø ng·∫≠p √∫ng cho c√¢y s·∫ßu ri√™ng?\\n'\n",
    "    # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "\n",
    "    session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    print(nodes)\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        for node in nodes:\n",
    "            document = session.get(Node, node.id).document\n",
    "            if document not in list_doc:\n",
    "                list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        # system_prompt, user_prompt = get_prompt_template_2(\n",
    "        #     user_query=question,\n",
    "        #     context_str=context_str,\n",
    "        #     project=project,\n",
    "        #     organization=organization,\n",
    "        #     # agent=agent_name,\n",
    "        # )\n",
    "        # try:\n",
    "        # repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "        # llm_client = InferenceClient(\n",
    "        #     model=repo_id,\n",
    "        #     timeout=120,\n",
    "        #     token='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',\n",
    "        # )\n",
    "\n",
    "        \n",
    "        # llm_response = call_llm(llm_client,QA_generation_answer_prompt.format(context=context_str, question=question))\n",
    "        llm_response = None\n",
    "        # llm_response = json.loads(\n",
    "        #                 retrieve_llm_response_test(\n",
    "        #                 user_prompt,\n",
    "        #                 model=model,\n",
    "        #                 # max_output_tokens=256,\n",
    "        #                 prefix_messages=system_prompt,\n",
    "        #                 )\n",
    "                        # )\n",
    "                \n",
    "        return list_relevant_docs, llm_response,list_doc\n",
    "        # except:\n",
    "        #     return None ,None\n",
    "\n",
    "    return None, None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L√†m th·∫ø n√†o ƒë·ªÉ c√† ph√™ ra hoa ƒë·ªìng lo·∫°t?\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index =3\n",
    "ds.iloc[index].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ƒê·ªÉ c√† ph√™ ra hoa ƒë·ªìng lo·∫°t, b·∫°n c·∫ßn x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi n∆∞·ªõc cho ph√π h·ª£p. Sau khi th·∫•y n·ª• hoa c√≥ d·∫°ng m·ªè s·∫ª xu·∫•t hi·ªán ƒë·∫ßy ƒë·ªß ·ªü ƒë·ªët ngo√†i c√πng c·ªßa c√°c c√†nh, ti·∫øn h√†nh t∆∞·ªõi n∆∞·ªõc cho c√† ph√™. Vi·ªác t∆∞·ªõi n∆∞·ªõc ƒë√∫ng th·ªùi ƒëi·ªÉm l·∫ßn ƒë·∫ßu v√† ƒë·ªß l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi s·∫Ω quy·∫øt ƒë·ªãnh ƒë·∫øn vi·ªác ra hoa ƒë·ªìng lo·∫°t.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.iloc[index].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM match_node_euclidean(\n",
      "    '[0.4996092617511749, -1.9378719329833984, -0.3662485182285309, -1.4759762287139893, -0.930750846862793, 0.42005330324172974, -0.079752117395401, -0.36336812376976013, 1.7296572923660278, -1.2460492849349976, 2.705437183380127, -1.0081355571746826, 1.0538535118103027, -0.6345371007919312, -0.12886667251586914, 1.211115837097168, -1.6051325798034668, -1.097388505935669, 0.5939808487892151, 0.6448752284049988, -2.9748642444610596, -3.337594509124756, 0.5572725534439087, -2.1943411827087402, -0.246882826089859, 1.950541377067566, 0.2505793571472168, -1.987774133682251, 0.38381248712539673, 0.6951450705528259, -0.13084068894386292, 2.559694290161133, -1.2074189186096191, 1.6059107780456543, -2.4690303802490234, 0.12016299366950989, 2.383877754211426, -2.7418227195739746, 4.1265435218811035, 1.726280927658081, -0.7463431358337402, 1.3819544315338135, -0.9485738277435303, -2.869506597518921, 0.40796852111816406, -0.7626692652702332, -0.8183176517486572, 0.07036105543375015, -1.0016918182373047, 2.5831427574157715, 3.2148985862731934, -1.825901746749878, -2.905302047729492, -0.11071792989969254, -0.9374761581420898, 1.5839970111846924, 0.6860500574111938, 0.8499265909194946, -0.3030458688735962, -0.019524961709976196, 0.09683758020401001, 1.3160983324050903, 2.3431544303894043, 0.12499648332595825, 2.0941600799560547, -0.9819726347923279, 0.26888155937194824, -0.4067118167877197, 1.410790205001831, -0.6618671417236328, -0.8170261383056641, 1.222944974899292, 0.3384796380996704, 0.43000227212905884, -0.7985965013504028, -1.3906103372573853, 0.5794163346290588, 1.9326015710830688, 1.8277523517608643, -1.218960165977478, -2.568845510482788, -2.072209119796753, 0.7977203130722046, 0.12459411472082138, 0.6151224374771118, -2.276566505432129, -2.6273882389068604, 1.1387172937393188, 1.3571112155914307, 0.3432163596153259, 0.9460930824279785, 1.8738055229187012, 3.089146614074707, -0.7169908881187439, 1.8897690773010254, 1.952214002609253, 0.9415782690048218, 2.6831767559051514, -0.6652147769927979, -0.025233522057533264, 0.8314871191978455, -0.8108299374580383, -0.07216483354568481, 3.5206427574157715, -2.2465953826904297, 0.051339417695999146, -0.5501886606216431, 1.2991567850112915, -0.16400644183158875, 1.4207528829574585, 3.5738227367401123, 0.6982492208480835, -1.3494675159454346, 0.4683758020401001, 1.7212586402893066, 1.1649028062820435, 0.34903186559677124, -1.30790376663208, -1.681915044784546, 0.748380184173584, 0.9543331861495972, 0.11688393354415894, -2.6520256996154785, 0.005046078935265541, -0.8708807826042175, -1.8501300811767578, 0.6296926140785217, 0.4124194383621216, -1.4198899269104004, -0.15669667720794678, -1.0323611497879028, 0.4025740623474121, 0.23118051886558533, 0.5687471628189087, -0.7199106216430664, -0.20229007303714752, 0.49413490295410156, -2.465481996536255, 0.15242284536361694, -2.575972080230713, -0.42106345295906067, -0.8788847923278809, 0.47046053409576416, 2.4897918701171875, 1.639289140701294, 0.14595407247543335, -0.7048893570899963, -1.810807704925537, 1.873110055923462, -3.0064175128936768, -3.4493775367736816, -0.6666325926780701, 0.18222366273403168, 0.7801258563995361, -0.6889358758926392, 1.336021900177002, -1.114638090133667, -0.6865462064743042, -0.08570218086242676, 1.0238600969314575, -0.06105698645114899, -0.030967190861701965, -1.0137968063354492, -1.4780213832855225, -0.267973929643631, 1.9237391948699951, 1.9536679983139038, 0.7415444850921631, -0.3803013563156128, 0.1382637917995453, 0.1389382779598236, -1.0104115009307861, -0.8098639249801636, 2.5957138538360596, 0.2509995102882385, -1.4323391914367676, -0.32374662160873413, 1.0648268461227417, 0.9892578125, -0.3625142574310303, 1.969801902770996, 2.4486234188079834, -1.2050509452819824, 0.4012472927570343, 0.6892302632331848, 0.4913631081581116, 2.7220234870910645, -0.3239961862564087, -2.7165684700012207, -2.9876856803894043, -2.951472759246826, 0.6615762710571289, -0.4026206135749817, -0.21141129732131958, 0.4551425576210022, -0.29432567954063416, -2.3610310554504395, -0.7624770998954773, -0.4807065427303314, 1.6423276662826538, 0.8125874400138855, 2.366424798965454, -2.3769001960754395, 0.5982350707054138, 2.083979606628418, 3.6760621070861816, 0.5808674097061157, -0.29261189699172974, -3.127568483352661, 1.0581600666046143, 1.2768163681030273, 2.819854497909546, -0.2860199213027954, -1.0020285844802856, 2.36556339263916, -0.5896151661872864, 0.046833381056785583, 1.2336021661758423, -2.1828224658966064, -0.33659279346466064, -0.8081132173538208, 0.5925910472869873, 1.7339380979537964, -1.9528300762176514, -0.021310359239578247, -1.6533856391906738, -0.510499894618988, 0.14266517758369446, -1.9498395919799805, -1.0316619873046875, -0.9200255870819092, 0.05424875020980835, -0.33836132287979126, -0.12964186072349548, 0.014276497066020966, 3.496410369873047, -0.5803319215774536, -0.8761020302772522, -0.4950878620147705, 0.37991875410079956, -0.40159523487091064, -0.2773209810256958, -2.2472832202911377, 0.8286755681037903, -0.16462618112564087, -1.4625005722045898, -2.7596359252929688, -0.5986781120300293, -2.044553756713867, 0.4336796998977661, -2.6652073860168457, -1.7230473756790161, 0.6887195706367493, -0.8847858309745789, 0.5620155334472656, -0.20317453145980835, -1.7595142126083374, 1.7614625692367554, 0.0035857558250427246, -2.161348819732666, 0.8645832538604736, 0.8364192247390747, -3.301318883895874, -0.032614052295684814, -2.5816218852996826, 2.9724795818328857, -0.7119124531745911, 1.3585002422332764, -0.5939086675643921, 2.07450008392334, -0.1508193165063858, -1.7134816646575928, -2.0343737602233887, 1.0874419212341309, 1.3269422054290771, 0.8438382744789124, -3.2144365310668945, -2.807286262512207, -0.44879770278930664, -1.1236205101013184, -1.9601384401321411, -1.6823700666427612, -1.4246764183044434, 0.8018877506256104, 1.0819820165634155, 0.548912525177002, -0.3450847268104553, -0.4166955053806305, -1.2564493417739868, 5.497670650482178, 0.9738276600837708, -0.09584533423185349, -0.6981956958770752, 0.5801475048065186, -2.714240789413452, 1.2046492099761963, 1.6916744709014893, 1.0360610485076904, 2.4672694206237793, 0.3805285692214966, -1.9222021102905273, -1.4561119079589844, 1.4035110473632812, -0.22141174972057343, -1.0533100366592407, 0.6387022733688354, -1.976518988609314, -0.7541394233703613, 0.1376773715019226, 1.5566325187683105, -0.3515404462814331, 1.6519408226013184, -1.4270111322402954, -2.1964640617370605, 0.2667677402496338, -1.0872546434402466, -2.037067413330078, -0.7640840411186218, 0.7628176212310791, -0.1612720936536789, -0.9603989720344543, -1.1467101573944092, 1.446634292602539, 0.204560324549675, 0.16437435150146484, 0.6846444606781006, 0.2101382315158844, 1.284989356994629, 1.0708508491516113, -0.968475341796875, 1.419224500656128, 2.4127187728881836, -2.044278860092163, -2.141749382019043, -1.6097605228424072, 2.581207513809204, -1.8360551595687866, -0.9605150818824768, 1.7955139875411987, 2.309875249862671, 0.6302812099456787, -0.8425744771957397, 0.29572880268096924, 1.0609155893325806, 1.0770188570022583, -0.37868934869766235, -0.9092849493026733, 1.6191303730010986, -1.8041576147079468, -1.6017494201660156, -2.2830810546875, -1.4783313274383545, -0.22341281175613403, -1.5997869968414307, 0.572392463684082, 0.5108468532562256, -0.39508622884750366, -1.4003112316131592, 2.259549856185913, 0.5521412491798401, 1.1092517375946045, -2.271087884902954, -0.07913489639759064, -1.7218027114868164, -0.060638152062892914, 0.006688714027404785, 7.786961555480957, 1.5945947170257568, -0.08527393639087677, -2.2386631965637207, 0.3166002333164215, -2.2868025302886963, -0.28786155581474304, 0.15774375200271606, 0.547148585319519, 0.2505944073200226, -3.536583423614502, -2.7908706665039062, -2.5386464595794678, -3.171812057495117, 0.3639541566371918, 0.06424415111541748, -0.7797027826309204, 0.5696341395378113, 1.9849984645843506, 1.9252736568450928, 0.9690226912498474, -0.3778378963470459, 0.02935837209224701, -0.7550874352455139, -2.320455312728882, -0.419511079788208, -2.225386142730713, 0.4410722255706787, 0.6509045958518982, 0.05604705214500427, 1.863168716430664, 0.6306540369987488, 0.208499014377594, -0.9000926613807678, -0.8377560377120972, 0.4073214530944824, -0.5721784234046936, 4.654333114624023, 0.4961538314819336, -2.2195074558258057, -0.9733672738075256, 1.4471514225006104, -1.0081844329833984, -0.20278826355934143, -1.5738786458969116, -0.2667962312698364, -0.817821204662323, 0.5182196497917175, -0.882258415222168, 0.43614447116851807, 1.023116111755371, -1.4044817686080933, 1.3185029029846191, 1.1543421745300293, -0.38049253821372986, 0.5958936810493469, 0.3012394607067108, 0.1471872627735138, 2.650919198989868, 0.8836819529533386, -0.6644574403762817, -3.9879612922668457, -2.2182915210723877, -1.0416455268859863, 1.6322540044784546, 0.23896722495555878, -0.18624255061149597, -1.2744662761688232, -0.23077058792114258, 1.6476298570632935, 0.6662904024124146, 0.6456606388092041, -1.7087122201919556, -0.06139105558395386, 0.51426100730896, 0.9540575742721558, 0.501847505569458, -0.3391173481941223, -1.6503502130508423, -1.0810853242874146, -1.201101303100586, 0.29832351207733154, 3.7196006774902344, 1.6852033138275146, -3.497354507446289, 2.5125298500061035, 0.7827538847923279, 2.136509895324707, 2.9913368225097656, 1.8137115240097046, 0.5036309957504272, 0.6190868616104126, -1.4529483318328857, -0.7243532538414001, -0.2378385365009308, -0.25660932064056396, 2.3626811504364014, -0.27021145820617676, 0.08956030011177063, -0.9938331246376038, 0.09139549732208252, -1.2063791751861572, -0.1749355047941208, 1.3446340560913086, -0.09358978271484375, -2.618769645690918, -1.7764965295791626, -2.305657386779785, -0.7916860580444336, -0.5481195449829102, 0.32148584723472595, -0.1578797996044159, 1.9189192056655884, 1.5775160789489746, 1.714719533920288, 0.21248158812522888, -1.5161683559417725, -1.5663896799087524, -0.26044073700904846, 0.04978370666503906, 3.6031510829925537, -0.48546773195266724, 0.4587443768978119, -1.4849092960357666, 1.8745108842849731, -2.5368247032165527, 0.8206006288528442, -2.7138030529022217, -0.25228169560432434, 2.0130224227905273, -0.5018998384475708, -0.09785439074039459, 0.35776466131210327, 2.225097179412842, 0.11111384630203247, -1.7781941890716553, 1.330155849456787, 0.6286634802818298, 0.884967565536499, 0.267628937959671, -1.0239958763122559, 0.34162482619285583, 2.1497275829315186, 1.6195764541625977, 2.6609814167022705, -1.0721094608306885, -1.7063652276992798, 0.06592491269111633, 1.2114763259887695, 1.570307970046997, 1.3784897327423096, -3.3742847442626953, -0.008897334337234497, -1.9553825855255127, -0.9916145205497742, -2.69174861907959, 1.661900281906128, 1.2641479969024658, -0.29284751415252686, 0.23957429826259613, 2.080204725265503, 0.45811623334884644, -1.751845121383667, 1.9766912460327148, -2.3264927864074707, 0.9603379964828491, -2.078335762023926, -3.7219605445861816, -1.9044495820999146, -0.2544691860675812, -1.2366704940795898, 1.5760488510131836, -0.3888205587863922, -2.450977087020874, -1.7426109313964844, 0.7113677263259888, -1.532036542892456, -0.6184749007225037, 1.4743967056274414, 1.1057469844818115, 0.945091187953949, -0.0355679988861084, -0.43614453077316284, 0.5742219090461731, 0.15757787227630615, 0.9768886566162109, -1.3154196739196777, -2.6843507289886475, 1.7315218448638916, 2.3973135948181152, 0.0977163314819336, -2.410386085510254, -0.6202300786972046, -3.219656467437744, -1.9255951642990112, 1.5513057708740234, -2.107381820678711, -0.6196467280387878, -0.23372483253479004, 0.01404573768377304, -0.11667415499687195, 1.4531465768814087, 0.2774815261363983, 1.8229167461395264, 0.45384061336517334, 4.068092346191406, -0.6983704566955566, -1.9102091789245605, -0.07832276821136475, 0.46790969371795654, -2.4532222747802734, 0.9644925594329834, -0.41278165578842163, -3.0570285320281982, 1.1264076232910156, -0.42130064964294434, -0.4408082067966461, 0.0705813467502594, -3.6566848754882812, -2.1806445121765137, -0.04893304407596588, 0.8667667508125305, 0.01830947957932949, 1.1385339498519897, 2.252769947052002, 1.3623108863830566, 0.663774847984314, -3.570833206176758, 0.4191667139530182, -1.338860034942627, -0.11524398624897003, -0.45062384009361267, 0.8498812913894653, 1.0715113878250122, -0.747296929359436, -2.2983741760253906, -1.299461007118225, 1.061746597290039, -1.5905717611312866, -0.7217793464660645, 0.3372487425804138, -0.24819082021713257, 0.5358131527900696, -0.5887041091918945, 0.9532344341278076, -2.703981637954712, 0.6529525518417358, -0.7472695112228394, 2.8721797466278076, 1.306524395942688, 0.34763821959495544, 1.5362476110458374, -0.014080476015806198, 1.317368507385254, 2.220752716064453, 0.46556517481803894, -0.6651474237442017, 2.1314969062805176, 1.974992036819458, 1.0386911630630493, -0.18063676357269287, -1.3248436450958252, -3.9924182891845703, 1.711763858795166, 0.081606924533844, 3.3293590545654297, 0.1120448112487793, -1.8617459535598755, -0.34564533829689026, -0.3530328869819641, 0.3522892892360687, 1.3494369983673096, -0.5554215908050537, -0.17971867322921753, -0.6917291283607483, 0.16431522369384766, 0.8906238079071045, -1.8120110034942627, 0.01705251634120941, -0.5690996646881104, 0.4575696289539337, -1.8531289100646973, -0.2019757628440857, -0.12480044364929199, 1.596318006515503, 1.1830788850784302, 0.6875076293945312, -0.42730093002319336, 0.729981005191803, 0.591178834438324, -0.6020563840866089, -0.6788214445114136, 2.0788815021514893, -0.13848932087421417, 1.3815932273864746, 0.0142916738986969, 1.0421912670135498, -0.9330877661705017, -0.03544791042804718, 0.4121643304824829, -0.6489253044128418, -1.6332128047943115, -2.839940071105957, -0.32002654671669006, 0.8717694282531738, 0.5398716330528259, 0.1040116548538208, -0.3443649411201477, -3.0537092685699463, -0.4470117390155792, 0.16813978552818298, 1.0905187129974365, -0.5787950754165649, 2.4199423789978027, -1.378591537475586, -0.6060616374015808, -3.054288387298584, -2.9805612564086914, -1.760377049446106, 1.1421799659729004, -4.385231018066406, -2.26050066947937, 1.751394271850586, -1.4484186172485352, -3.613893985748291, -1.5063621997833252, -0.7649528980255127, -1.6489734649658203, -1.7479288578033447, 1.311100959777832, 0.9313958883285522, -0.8818585276603699, 0.47879233956336975, 2.6386523246765137, 0.9963072538375854, 0.2980709373950958, 0.5599012970924377, -0.8217987418174744, -0.3349427878856659, 0.4955357611179352, -0.07521656155586243, -0.6698015928268433, -2.206226110458374, -1.226829171180725, -1.0706045627593994, -1.7680482864379883, -2.5246477127075195, -1.2979187965393066, 1.4813125133514404, -0.41756173968315125, -1.2317359447479248, 0.5584030151367188, -0.8263911008834839, 0.6618524789810181, -0.39423200488090515, 1.253064751625061, -0.5987750887870789, -1.5033905506134033, -1.2102961540222168, -0.07383854687213898, -2.23244047164917, -0.6416722536087036, 0.8055505752563477, -1.6019837856292725, -0.11111763119697571, 1.951481819152832, -1.4358584880828857, 0.2376784086227417, -2.1410179138183594, 2.2347888946533203, -0.9361410737037659, -0.2937948703765869, -2.4433650970458984, 0.3223184049129486, 3.197299003601074, 0.018078237771987915, -0.12288790941238403, -2.666867733001709, -1.4656071662902832, -0.24985691905021667, -1.427527904510498, 1.0314158201217651, -1.9406263828277588, -1.5245994329452515, 1.9537136554718018, -1.1878397464752197, 1.2462376356124878, 1.769618272781372, -0.5975441932678223, -0.023874834179878235, -0.10892325639724731, -2.379742383956909, 0.6514783501625061, 0.04451480507850647, -1.7963614463806152, -0.5071077942848206, 1.3946185111999512, -0.08116622269153595, -1.5183537006378174, -2.1613376140594482, 1.4915709495544434, -0.3008282780647278, -0.15286368131637573, -0.34060990810394287, 3.265631675720215, -2.4130067825317383, -0.002161860466003418, -0.4216127395629883, 2.150665283203125, -0.4496535062789917, 0.06570582836866379, -1.0380239486694336, -1.5594415664672852, 1.5156807899475098, 0.4067175090312958, 0.871914267539978, -0.407113254070282, 1.6282130479812622, -2.382370948791504, 1.4613609313964844, 0.17153701186180115, 1.2147802114486694, 0.6308507323265076, -0.2889246940612793, -1.9793701171875, 0.2509955167770386, 0.3523859679698944, 0.6582516431808472, -1.7645800113677979, -0.003849431872367859, -0.8770895004272461, 0.602353572845459, 1.7918403148651123, -3.3898117542266846, -0.7390509247779846, -0.7597935199737549, -0.7615084648132324, 0.542452335357666, 0.37521234154701233, -0.05060136318206787, -0.6616356372833252, -0.8276628255844116, -1.387223482131958, -1.5842370986938477, -0.4105936884880066, -1.5834641456604004, 0.9270840883255005, 1.5277161598205566, 1.6942052841186523, 0.9861268997192383, 0.3176189363002777, 1.162693738937378, -0.6852779388427734, -0.7075988054275513, -1.4462683200836182, 1.8817791938781738, 2.0281600952148438, 1.5843533277511597, -0.587136447429657, -0.5756190419197083, 0.07235527038574219, -1.3125559091567993, -0.5270045399665833, 0.5987035036087036, -0.4782489836215973, -0.29262033104896545, 1.7877278327941895, -2.2802135944366455, 0.7364464998245239, -1.0304982662200928, -0.8118249177932739, -1.1932055950164795, 1.032246470451355, -0.8709115982055664, 2.5548458099365234, 1.44737708568573, -1.6488683223724365, -2.7035582065582275, -3.3879849910736084, -0.5201979875564575, -2.0274784564971924, -0.5890852808952332, 1.5769528150558472, 0.28011882305145264, 0.5949459075927734, -1.5419032573699951, -0.5942310690879822, 0.06137843430042267, 3.0323095321655273, 1.4739158153533936, -0.5416781902313232, 0.8664578795433044, 0.28872519731521606, 0.7095029354095459, 0.8374093174934387, 0.6245885491371155, 0.3448449969291687, 2.1577610969543457, -4.051453113555908, -1.2309507131576538, 0.8020186424255371, 1.4782969951629639, 2.346820592880249, 0.437297523021698, -3.3498103618621826, 2.2122366428375244, -0.6304749250411987, 2.090489387512207, 0.11402688920497894, -1.1299386024475098, -0.2667185366153717, 0.7696870565414429, -0.4173504412174225, -0.6837857365608215, -3.852814197540283, 0.1691608428955078, 0.17933154106140137, 2.670492649078369, -1.0712279081344604, -0.3288027346134186, 1.5544626712799072, 1.7276338338851929, -0.5656968355178833, -1.7524046897888184, -1.275222897529602, -0.957273542881012, 0.6406216621398926, 1.9216632843017578, 1.146910309791565, -1.4067962169647217, 1.3333888053894043, -0.35058677196502686, 2.731135606765747, -1.2784209251403809, 0.45968249440193176, 2.145756483078003, 0.2854057252407074, 0.12243013083934784, -4.293540000915527, 2.0804848670959473, -0.08590331673622131, -3.5444648265838623, -2.8097915649414062, 0.4813747704029083, -2.27164363861084, 1.897317886352539, 0.102988600730896, 1.9007980823516846, 1.9947376251220703, -0.9158291816711426, -3.182114362716675, 0.5062896013259888, 0.04936642944812775, 2.6615374088287354, 0.6362144351005554, -1.4855692386627197, 1.0660669803619385, -0.5661356449127197, -0.02914014458656311, -0.4000702202320099, 0.18088814616203308, 1.0402891635894775, 1.4646538496017456, -2.0284366607666016, 0.4305945038795471, 0.08348950743675232, 1.291939616203308, -2.259162425994873, -0.50214684009552, 1.4945451021194458, -1.2561131715774536, 0.2799728512763977, 0.9815015196800232, 0.4171880781650543, 1.3442084789276123, -0.8985893130302429, -2.49800968170166, -2.8310508728027344, -0.696010410785675, -2.057899236679077, -2.7591137886047363, 1.1851533651351929, -1.620607852935791, -0.3867129385471344, 1.2247130870819092, -1.3492579460144043, 1.966176986694336, 2.9844508171081543, -0.015006963163614273, 2.071852684020996, 0.5221186280250549, -1.5915896892547607, -0.4586191773414612, 0.027885735034942627, 1.5384904146194458, 0.2658360004425049, -0.7370353937149048, 1.4504541158676147, -0.4064733684062958, 0.5371043682098389, -1.3343061208724976, -0.21989589929580688, -1.9214448928833008, 1.4587030410766602, 1.009344458580017, 1.1079641580581665, -0.07145035266876221, -0.5709485411643982, 0.19030320644378662, -1.5846318006515503, -2.793588399887085, 1.8356014490127563, 0.7101314067840576, 4.079638957977295, 0.4830610156059265, -0.9142231941223145, -0.9011071920394897, 0.5501949787139893, 1.3746764659881592, -1.234757661819458, 2.045149564743042, -0.45008474588394165, -1.0252034664154053, -0.8436344265937805, -0.988187313079834, -0.7411688566207886, -2.1975607872009277, 0.5204917788505554, 1.5533894300460815, 0.6207700967788696, 0.05783924460411072, 2.492870569229126, -3.5984888076782227, -0.9431455135345459, 2.143258571624756, 0.5006019473075867, 0.4068228006362915, 1.4527792930603027, 0.27924609184265137, -0.5548146963119507, 0.6608067154884338, 2.798401355743408, -0.5844500064849854, -0.013840436935424805, -0.537621021270752, 1.2562510967254639, -1.8572319746017456, -0.9373587965965271, 1.067662239074707, -1.3879530429840088, -0.41859352588653564, 2.066883087158203, -0.7941335439682007, -0.21350419521331787, -2.245789051055908, -1.142451286315918, 0.5684605836868286, -0.7719943523406982]'::vector(1024),\n",
      "    0.2::double precision,\n",
      "    3);\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "list_relevant_docs, llm_response,list_doc = test_rag_performance_2(ds.iloc[index].question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Document id=31 name=category_trees_S·∫ßu_ri√™ng.md uuid=f1a381d8-8c06-4c4c-a317-bf633cb94634>,\n",
       " <Document id=52 name=category_trees_Tre_m·∫°nh_t√¥ng.md uuid=88ac50f5-7248-4bf9-8c57-009285c34e9e>,\n",
       " <Document id=55 name=2nong_introduction.md uuid=e8328004-813d-4858-9e6a-7d21d26044c6>,\n",
       " <Document id=50 name=category_trees_Th·ªßy_tr√∫c__l√°c_d√π.md uuid=3421b192-8574-47a3-a628-2feeb9ac30c6>,\n",
       " <Document id=51 name=category_trees_Th·ªßy_t√πng.md uuid=3116c346-f248-4e23-9e9d-f4b7e1ca7640>,\n",
       " <Document id=43 name=category_trees_Lay_∆°n.md uuid=1685a8e9-1ca6-467e-bcb6-06146b78b336>,\n",
       " <Document id=54 name=category_trees_ƒê·ªßng_ƒë·ªânh.md uuid=ea03ee7e-61e5-4da6-a17a-f55b87130f80>,\n",
       " <Document id=28 name=category_trees_D·ª´a.md uuid=1ccf30d7-49e4-41f6-8f79-d855049774d8>,\n",
       " <Document id=46 name=category_trees_L·ª•c_biÃÄnh.md uuid=09a780cc-182a-4322-b78b-68960d4c2a21>,\n",
       " <Document id=33 name=category_trees_C√†_ph√™.md uuid=c3a92b09-9ec5-44f9-825a-0be9c5fc00c3>,\n",
       " <Document id=5 name=knowledge_handbooks_C√°ch_b√≥n_l√≥t_cho_c√†_ph√™_tr·ªìng_m·ªõi.md uuid=942c4399-3714-4dac-b316-189fb444dff7>,\n",
       " <Document id=45 name=category_trees_Sen_ƒë√°.md uuid=248e7a31-246e-4870-bcd7-db2d150aa674>,\n",
       " <Document id=49 name=category_trees_Th·ªët_n·ªët.md uuid=1381ce97-6736-4be4-ac30-3df0886f5d64>,\n",
       " <Document id=44 name=category_trees_L·∫ª_b·∫°n__soÃÄ_huy√™ÃÅt__bang_hoa.md uuid=f63176bc-c04d-4bf3-bb63-b12e350e3bd2>,\n",
       " <Document id=29 name=category_trees_D·ª´a_n∆∞·ªõc__d·ª´a_l√°.md uuid=0d9387d4-435b-43cc-9411-a315d0768b53>,\n",
       " <Document id=38 name=category_trees_Lan_Cattleya.md uuid=b81f2971-cf9a-4439-9ce8-7205102a695d>,\n",
       " <Document id=30 name=category_trees_Chu·ªëi_r·∫Ω_qu·∫°t.md uuid=9a52dd3c-46c6-44b4-aaa0-17623cf6365c>,\n",
       " <Document id=7 name=knowledge_handbooks_C√≥_c·∫ßn_b√≥n_ph√¢n_h·ªØu_c∆°_cho_ƒë·∫•t_ph√®n_kh√¥ng.md uuid=8847f0b2-fa25-42e7-9537-bbf9a8e0cdfd>,\n",
       " <Document id=48 name=category_trees_Thi√™n_tu√™ÃÅ.md uuid=cf7fce02-3679-49ad-8654-291911f3269e>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S·∫ßu ri√™ng\\r\\nDurio zibethinus (ƒë∆∞·ª£c g·ªçi l√† S·∫ßu ri√™ng ƒê√¥ng Nam √Å) l√† lo√†i th·ª±c v·∫≠t thu·ªôc chi S·∫ßu ri√™ng ph·ªï bi·∫øn nh·∫•t.',\n",
       " 'Tre m·∫°nh t√¥ng\\r\\nTre M·∫°nh T√¥ng, t√™n khoa h·ªçc Dendrocalamus asper, l√† m·ªôt lo√†i th·ª±c v·∫≠t c√≥ hoa trong h·ªç H√≤a th·∫£o. Lo√†i n√†y ƒë∆∞·ª£c (Schult.) Backer m√¥ t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n nƒÉm 1927.',\n",
       " 'Th√¥ng tin ·ª©ng d·ª•ng :\\r\\n\\r\\nWebsite : https://2nong.vn/\\r\\n\\r\\nLink adroi : https://play.google.com/store/apps/details?id=com.pvcfc.inong&hl=vi&gl=US\\r\\n\\r\\nLinhk ios : https://apple.co/3kzb6us\\r\\n\\r\\nLink fanpage : https://www.facebook.com/2nong.vn',\n",
       " 'Th·ªßy tr√∫c, l√°c d√π\\r\\nCyperus alternifolius l√† m·ªôt lo√†i th·ª±c v·∫≠t c√≥ hoa trong h·ªç C√≥i. Lo√†i n√†y ƒë∆∞·ª£c L. m√¥ t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n nƒÉm 1767.',\n",
       " 'Th·ªßy t√πng\\r\\nMƒÉng leo hay mƒÉng b√†n tay, th·ªßy t√πng (danh ph√°p: Asparagus setaceus) l√† lo√†i th·ª±c v·∫≠t c√≥ hoa trong h·ªç MƒÉng t√¢y. Lo√†i n√†y ƒë∆∞·ª£c (Kunth) Jessop m√¥ t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n nƒÉm 1966.',\n",
       " 'Lay ∆°n\\r\\nƒêang c·∫≠p nh·∫≠t data v·ªÅ c√¢y tr·ªìng n√†y. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng ƒë√≥ng g√≥p d·ªØ li·ªáu ƒë·ªÉ gi√∫p ch√∫ng t√¥i c·∫£i thi·ªán m√¥ h√¨nh nh·∫≠n d·∫°ng ..',\n",
       " 'ƒê·ªßng ƒë·ªânh\\r\\nC√¢y ƒë·ªßng ƒë·ªânh, c√≤n g·ªçi l√† c√¢y ƒë√πng ƒë√¨nh, m√≥c, t√™n khoa h·ªçc l√† Caryota mitis; l√† lo√†i th·ª±c v·∫≠t c√≥ hoa thu·ªôc h·ªç Cau (Arecaceae). Lo√†i n√†y ƒë∆∞·ª£c Lour. mi√™u t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n nƒÉm 1790.',\n",
       " 'D·ª´a\\r\\nƒêang c·∫≠p nh·∫≠t data v·ªÅ c√¢y tr·ªìng n√†y. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng ƒë√≥ng g√≥p d·ªØ li·ªáu ƒë·ªÉ gi√∫p ch√∫ng t√¥i c·∫£i thi·ªán m√¥ h√¨nh nh·∫≠n d·∫°ng ..',\n",
       " 'L·ª•c biÃÄnh\\r\\nƒêang c·∫≠p nh·∫≠t data v·ªÅ c√¢y tr·ªìng n√†y. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng ƒë√≥ng g√≥p d·ªØ li·ªáu ƒë·ªÉ gi√∫p ch√∫ng t√¥i c·∫£i thi·ªán m√¥ h√¨nh nh·∫≠n d·∫°ng ..',\n",
       " '.000 ha/410.000 ha). Hi·ªán c√† ph√™ ch√® ƒë∆∞·ª£c tr·ªìng ·ªü c√°c t·ªânh L√¢m ƒê·ªìng ·ªü T√¢y Nguy√™n, v√πng th√†nh ph·ªë S∆°n La, huy·ªán Mai S∆°n, huy·ªán Thu·∫≠n Ch√¢u (S∆°n La) v√† M∆∞·ªùng ·∫¢ng (ƒêi·ªán Bi√™n) ·ªü T√¢y B·∫Øc.',\n",
       " 'C√°ch b√≥n l√≥t cho c√† ph√™ tr·ªìng m·ªõi?\\r\\nCho v√†o h·ªë 15 ‚Äì 20 kg ph√¢n chu·ªìng hoai + 1 kg v√¥i + 0,5 kg l√¢n nung ch·∫£y, tr·ªôn v·ªõi ƒë·∫•t m·∫∑t, ƒë·∫£o ƒë·ªÅu (tr∆∞·ªõc khi tr·ªìng 1 th√°ng).',\n",
       " '## **Li√™n k·∫øt kh√°c**\\r\\n\\r\\n<https://vi.wikipedia.org/wiki/Echeveria>\\r\\n\\r\\n<https://caycanhbancong.com/san-pham/cay-sen-da/>\\r\\n\\r\\n<https://noth.garden/echeveria-hoa-da-sen-da/>\\r\\n\\r\\n<https://joygarden.vn/cach-trong-sen-da-chuan-chia-se-cua-nguoi-7-nam-trong-sen-da/>',\n",
       " 'Th·ªët n·ªët\\r\\nTh·ªët n·ªët hay c·ªç ƒë∆∞·ªùng (danh ph√°p hai ph·∫ßn: Borassus flabellifer) l√† lo√†i th·ª±c v·∫≠t thu·ªôc h·ªç Cau, b·∫£n ƒë·ªãa c·ªßa Nam √Å v√† ƒê√¥ng Nam √Å, ph√¢n b·ªë t·ª´ Indonesia ƒë·∫øn Pakistan. Tuy nhi√™n, v√πng T√¢y Song B·∫£n N·∫°p v√† C·∫£nh H·ªìng, V√¢n Nam, Trung Qu·ªëc c≈©ng tr·ªìng lo·∫°i c√¢y n√†y.',\n",
       " 'L·∫ª b·∫°n, soÃÄ huy√™ÃÅt, bang hoa\\r\\nC√¢y s√≤ huy·∫øt (hay s√≤ t√≠m, s·∫Øc m√†u, l·∫ª b·∫°n, b·∫°ng hoa) l√† m·ªôt lo√†i th·ª±c v·∫≠t c√≥ hoa trong h·ªç Th√†i l√†i, danh ph√°p khoa h·ªçc l√† Tradescantia spathacea ho·∫∑c Tradescantia discolor. Lo√†i n√†y ƒë∆∞·ª£c Sw. mi√™u t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n nƒÉm 1788. C√¢y c√≥ t√°c d·ª•ng l√†m c·∫£nh v√† thu·ªëc trong y h·ªçc.',\n",
       " 'D·ª´a n∆∞·ªõc, d·ª´a l√°',\n",
       " '2. <https://vi.wikipedia.org/wiki/Chi_CAt_lan>\\r\\n\\r\\n3. <https://nuoitrong.vn/lan-cattleya.html>',\n",
       " 'C√† ph√™',\n",
       " 'Chu·ªëi r·∫Ω qu·∫°t\\r\\nChu·ªëi r·∫ª qu·∫°t c√≤n ƒë∆∞·ª£c g·ªçi l√† c√¢y chu·ªëi qu·∫°t hay chu·ªëi c·ªç (danh ph√°p hai ph·∫ßn: Ravenala madagascariensis) l√† m·ªôt lo√†i th·ª±c v·∫≠t thu·ªôc h·ªç Thi√™n ƒëi·ªÉu (Strelitziaceae), ƒë·∫∑c h·ªØu c·ªßa Madagascar. Tuy v·∫≠y n√≥ kh√¥ng ph·∫£i l√† chu·ªëi (h·ªç Musaceae) v√† c≈©ng kh√¥ng ph·∫£i l√† c·ªç (h·ªç Arecaceae), Chu·ªëi r·∫ª qu·∫°t l√† lo√†i duy nh·∫•t c·ªßa chi Ravenala.',\n",
       " 'C√≥ c·∫ßn b√≥n ph√¢n h·ªØu c∆° cho ƒë·∫•t ph√®n kh√¥ng?\\r\\nN√≥i chung ƒë·∫•t ph√®n tr·ªìng l√∫a c√≥ ch·ª©a h√†m l∆∞·ª£ng ch·∫•t h·ªØu c∆° cao h∆°n nhi·ªÅu lo·∫°i ƒë·∫•t kh√°c. Nh∆∞ng v·∫´n r·∫•t c·∫ßn b√≥n ph√¢n h·ªØu c∆°. V√¨ ch·∫•t h·ªØu c∆° c√≥ t√°c d·ª•ng nh∆∞ ch·∫•t ƒë·ªám ·ªü trong ƒë·∫•t, l√†m cho ƒë·∫•t kh√¥ng chua th√™m nhi·ªÅu, ngay c·∫£ khi kh√¥ n∆∞·ªõc. Nh∆∞ng ch√∫ √Ω kh√¥ng b√≥n c√°c lo·∫°i ph√¢n h·ªØu c∆° c√≤n t∆∞∆°i.',\n",
       " 'Thi√™n tu√™ÃÅ\\r\\nThi√™n tu·∫ø hay tu·∫ø l∆∞·ª£c (danh ph√°p hai ph·∫ßn: Cycas pectinata l√† lo√†i th·ª±c v·∫≠t ph√¢n b·ªë ·ªü ƒë√¥ng b·∫Øc ·∫§n ƒê·ªô, Nepal, Bhutan, b·∫Øc Myanma, Hoa Nam, b·∫Øc Th√°i Lan, L√†o v√† Vi·ªát Nam. ƒê√¢y l√† lo√†i th·ª© t∆∞ c·ªßa chi Cycasƒë∆∞·ª£c ƒë·∫∑t t√™n khoa h·ªçc v·ªõi m√¥ t·∫£ l·∫ßn ƒë·∫ßu nƒÉm 1826. Thi√™n tu·∫ø cao ƒë·∫øn 40 foot (12 m) v·ªõi t√°n r·ªông.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response.split(\"Tr·∫£ l·ªùi: \")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SPECIFIC EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = \"E:/TrungPhanADVN/Code/LangChain_RAG/app/api/data/training_data/knowledge_handbooks_C√°ch_l√†m_cho_hoa_c√†_ph√™_ra_ƒë·ªìng_lo·∫°t.md\"\n",
    "with open(source_file, \"r\") as f:\n",
    "    test_document = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C√°ch l√†m cho hoa c√† ph√™ ra ƒë·ªìng lo·∫°t?\\nMu·ªën c√† ph√™ ra hoa ƒë·ªìng lo·∫°t c·∫ßn x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi n∆∞·ªõc cho ph√π h·ª£p. Vi·ªác x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi, l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi, ph∆∞∆°ng ph√°p t∆∞·ªõi t√πy thu·ªôc v√†o ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt, lo·∫°i ƒë·∫•t, t√¨nh tr·∫°ng sinh tr∆∞·ªüng c·ªßa c√¢y. Sau th·ªùi gian kh√¥ h·∫°n, khi th·∫•y n·ª• hoa c√≥ d·∫°ng m·ªè s·∫ª xu·∫•t hi·ªán ƒë·∫ßy ƒë·ªß ·ªü ƒë·ªët ngo√†i c√πng c·ªßa c√°c c√†nh th√¨ ti·∫øn h√†nh t∆∞·ªõi n∆∞·ªõc cho c√† ph√™. Vi·ªác t∆∞·ªõi n∆∞·ªõc ƒë√∫ng th·ªùi ƒëi·ªÉm l·∫ßn ƒë·∫ßu (ƒë·ª£t 1) v√† ƒë·ªß l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi s·∫Ω quy·∫øt ƒë·ªãnh ƒë·∫øn vi·ªác ra hoa ƒë·ªìng lo·∫°t.\\n\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"C√°ch l√†m cho hoa c√† ph√™ ra ƒë·ªìng lo·∫°t?\\nMu·ªën c√† ph√™ ra hoa ƒë·ªìng lo·∫°t c·∫ßn x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi n∆∞·ªõc cho ph√π h·ª£p. Vi·ªác x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi, l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi, ph∆∞∆°ng ph√°p t∆∞·ªõi t√πy thu·ªôc v√†o ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt, lo·∫°i ƒë·∫•t, t√¨nh tr·∫°ng sinh tr∆∞·ªüng c·ªßa c√¢y. Sau th·ªùi gian kh√¥ h·∫°n, khi th·∫•y n·ª• hoa c√≥ d·∫°ng m·ªè s·∫ª xu·∫•t hi·ªán ƒë·∫ßy ƒë·ªß ·ªü ƒë·ªët ngo√†i c√πng c·ªßa c√°c c√†nh th√¨ ti·∫øn h√†nh t∆∞·ªõi n∆∞·ªõc cho c√† ph√™. Vi·ªác t∆∞·ªõi n∆∞·ªõc ƒë√∫ng th·ªùi ƒëi·ªÉm l·∫ßn ƒë·∫ßu (ƒë·ª£t 1) v√† ƒë·ªß l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi s·∫Ω quy·∫øt ƒë·ªãnh ƒë·∫øn vi·ªác ra hoa ƒë·ªìng lo·∫°t.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_output = torch.tensor(tokenizer_bartpho([test_document],padding='max_length', truncation=True, max_length=1024)['input_ids'])\n",
    "with torch.no_grad():\n",
    "    features = bartpho(token_output)\n",
    "\n",
    "last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "embeddings_docs = torch.mean(last_hidden_state, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2422, -3.0425, -0.7843,  ..., -1.6288,  0.7525, -0.6217]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_ebed.txt','w') as f:\n",
    "    f.write(str(embeddings_docs.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tokenizer_bartpho([test_document] ,truncation=True)['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_docs, embed_docs = get_embeddings_bartbert(test_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_docs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L√†m th·∫ø n√†o ƒë·ªÉ c√† ph√™ ra hoa ƒë·ªìng lo·∫°t?\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.iloc[index].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_test = '2Nong l√† app g√¨\\n'\n",
    "question_test = ds.iloc[index].question\n",
    "token_output = torch.tensor(tokenizer_bartpho([question_test],padding='max_length', truncation=True, max_length=1024)['input_ids'])\n",
    "with torch.no_grad():\n",
    "    features = bartpho(token_output)\n",
    "\n",
    "last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "embeddings_question = torch.mean(last_hidden_state, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('question_test_ebed.txt','w') as f:\n",
    "    f.write(str(embeddings_question.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_question, embed_question = get_embeddings_bartbert(question_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine(embeddings_docs[0],embeddings_question[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos(embeddings_docs[0],embeddings_question[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(embed_docs.shape[0]):\n",
    "    print(cosine(embed_docs[index],embed_question[0]))\n",
    "    # print(cos(embed_docs,embed_question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST GENERATED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-663c7bbb-562499331e279d775afda808;8901399b-4987-4edd-af05-d90f1c23d985)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1683\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    392\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 393\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    318\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m     )\n\u001b[1;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GatedRepoError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-663c7bbb-562499331e279d775afda808;8901399b-4987-4edd-af05-d90f1c23d985)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m tokenizer_generate \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model_generate \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:819\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 819\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    926\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    929\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    930\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\hub.py:416\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-663c7bbb-562499331e279d775afda808;8901399b-4987-4edd-af05-d90f1c23d985)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer_generate = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model_generate = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer_generate.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model_generate.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORE WITH GENERATED ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT =\"\"\"###M√¥ t·∫£ c√¥ng vi·ªác:\n",
    "M·ªôt h∆∞·ªõng d·∫´n (c√≥ th·ªÉ bao g·ªìm ƒê·∫ßu v√†o b√™n trong n√≥), m·ªôt ph·∫£n h·ªìi ƒë·ªÉ ƒë√°nh gi√°, m·ªôt c√¢u tr·∫£ l·ªùi tham kh·∫£o ƒë·∫°t ƒëi·ªÉm 5 v√† thang ƒëi·ªÉm ƒë·∫°i di·ªán cho ti√™u ch√≠ ƒë√°nh gi√° s·∫Ω ƒë∆∞·ª£c ƒë∆∞a ra.\n",
    "1. Vi·∫øt ph·∫£n h·ªìi chi ti·∫øt ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi d·ª±a tr√™n thang ƒëi·ªÉm cho s·∫µn, kh√¥ng ƒë√°nh gi√° chung chung.\n",
    "2. Sau khi vi·∫øt ph·∫£n h·ªìi, h√£y vi·∫øt ƒëi·ªÉm l√† s·ªë nguy√™n t·ª´ 1 ƒë·∫øn 5. B·∫°n n√™n tham kh·∫£o b·∫£ng ƒë√°nh gi√°.\n",
    "3. ƒê·ªãnh d·∫°ng ƒë·∫ßu ra s·∫Ω nh∆∞ sau: \\\"Ph·∫£n h·ªìi: {{vi·∫øt ph·∫£n h·ªìi cho ti√™u ch√≠}} [K·∫æT QU·∫¢] {{m·ªôt s·ªë nguy√™n t·ª´ 1 ƒë·∫øn 5}}\\\"\n",
    "4. Vui l√≤ng kh√¥ng ƒë∆∞a ra b·∫•t k·ª≥ l·ªùi m·ªü ƒë·∫ßu, k·∫øt th√∫c v√† gi·∫£i th√≠ch n√†o kh√°c. H√£y ch·∫Øc ch·∫Øn bao g·ªìm [K·∫æT QU·∫¢] trong ƒë·∫ßu ra c·ªßa b·∫°n.\n",
    "\n",
    "###H∆∞·ªõng d·∫´n ƒë√°nh gi√°:\n",
    "{instruction}\n",
    "\n",
    "###Ph·∫£n h·ªìi ƒë·ªÉ ƒë√°nh gi√°:\n",
    "{response}\n",
    "\n",
    "###C√¢u tr·∫£ l·ªùi tham kh·∫£o (ƒêi·ªÉm 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Ti√™u ch√≠ ch·∫•m ƒëi·ªÉm:\n",
    "[C√¢u tr·∫£ l·ªùi c√≥ ƒë√∫ng, ch√≠nh x√°c v√† th·ª±c t·∫ø d·ª±a tr√™n c√¢u tr·∫£ l·ªùi tham kh·∫£o kh√¥ng?]\n",
    "ƒêi·ªÉm 1: C√¢u tr·∫£ l·ªùi ho√†n to√†n kh√¥ng ch√≠nh x√°c, kh√¥ng ch√≠nh x√°c v√†/ho·∫∑c kh√¥ng th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 2: C√¢u tr·∫£ l·ªùi ph·∫ßn l·ªõn l√† kh√¥ng ch√≠nh x√°c, kh√¥ng ch√≠nh x√°c v√†/ho·∫∑c kh√¥ng th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 3: C√¢u tr·∫£ l·ªùi c√≥ ph·∫ßn ƒë√∫ng, ch√≠nh x√°c v√†/ho·∫∑c th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 4: C√¢u tr·∫£ l·ªùi h·∫ßu h·∫øt ƒë·ªÅu ƒë√∫ng, ch√≠nh x√°c v√† th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 5: C√¢u tr·∫£ l·ªùi ho√†n to√†n ƒë√∫ng, ch√≠nh x√°c v√† th·ª±c t·∫ø.\n",
    "\n",
    "###Nh·∫≠n x√©t:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "# 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "# 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "# 3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "# 4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Is the response correct, accurate, and factual based on the reference answer?]\n",
    "# Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "# Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "# Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "# Score 4: The response is mostly correct, accurate, and factual.\n",
    "# Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"B·∫°n l√† m·ªôt m√¥ h√¨nh ƒë√°nh gi√° ng√¥n ng·ªØ c√¥ng b·∫±ng\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "# def evaluate_answers(\n",
    "#     answer_path: str,\n",
    "#     eval_chat_model: BaseChatModel,\n",
    "#     evaluator_name: str,\n",
    "#     evaluation_prompt_template: ChatPromptTemplate,\n",
    "# ) -> None:\n",
    "#     \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "#     answers = []\n",
    "# if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "#     answers = json.load(open(answer_path, \"r\"))\n",
    "answers= outputs\n",
    "evaluate_output = []\n",
    "for experiment in tqdm(answers):\n",
    "    if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "        continue\n",
    "\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        instruction=experiment[\"question\"],\n",
    "        response=experiment[\"generated_answer\"],\n",
    "        reference_answer=experiment[\"true_answer\"],\n",
    "    )\n",
    "    try:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    except:\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = None\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = None\n",
    "        continue\n",
    "    feedback, score = [item.strip() for item in eval_result.content.split(\"[K·∫æT QU·∫¢]\")]\n",
    "    experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "    experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "    # evaluate_output.append()\n",
    "    # with open(answer_path, \"w\") as f:\n",
    "    #     json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(answers,open('result_evaluate_1.json','w',encoding='utf-8'), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(json.load(open('result_evaluate_1.json', \"r\")))\n",
    "result = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] ) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result[\"eval_score_GPT4\"].mean()\n",
    "# average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import datasets\n",
    "import pandas as pd\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "# fig.update_layout(w\n",
    "#     width=1000,\n",
    "#     height=600,\n",
    "#     barmode=\"group\",\n",
    "#     yaxis_range=[0, 100],\n",
    "#     title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "#     xaxis_title=\"RAG settings\",\n",
    "#     font=dict(size=15),\n",
    "# )\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 2: https://freedium.cfd/https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-rag-applications-with-ragas-81d67b0ee31a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
