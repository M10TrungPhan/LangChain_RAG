{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\"E:\\TrungPhanADVN\\Code\\LangChain_RAG\\scripts\\evaluat_dataset_2.xlsx\", index_col='Unnamed: 0').to_csv(\"E:\\TrungPhanADVN\\Code\\LangChain_RAG\\scripts\\evaluat_dataset_2.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 1: https://huggingface.co/learn/cookbook/rag_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test = [\n",
    "\"2nong_introduction.md\",\n",
    "\"category_trees_C√†_ph√™.md\",\n",
    "\"category_trees_Chu·ªëi_r·∫Ω_qu·∫°t.md\",\n",
    "\"category_trees_D·ª´a.md\",\n",
    "\"category_trees_D·ª´a_n∆∞·ªõc__d·ª´a_l√°.md\",\n",
    "\"category_trees_ƒê·ªßng_ƒë·ªânh.md\",\n",
    "\"category_trees_Hoa_ly.md\",\n",
    "\"category_trees_H·ªì_ti√™u.md\",\n",
    "\"category_trees_Lan_Cattleya.md\",\n",
    "\"category_trees_Lan_Chi.md\",\n",
    "\"category_trees_Lan_chu_ƒë√≠nh.md\",\n",
    "\"category_trees_Lan_d·∫°_h∆∞∆°ng.md\",\n",
    "\"category_trees_Lan_Vanda.md\",\n",
    "\"category_trees_Lay_∆°n.md\",\n",
    "\"category_trees_L·∫ª_b·∫°n__soÃÄ_huy√™ÃÅt__bang_hoa.md\",\n",
    "\"category_trees_L√¥_h√¥Ã£i__nha_ƒëam.md\",\n",
    "\"category_trees_L√∫a.md\",\n",
    "\"category_trees_L·ª•c_biÃÄnh.md\",\n",
    "\"category_trees_M√≠a.md\",\n",
    "\"category_trees_Phong_L·ªôc_Hoa.md\",\n",
    "\"category_trees_S·∫ßu_ri√™ng.md\",\n",
    "\"category_trees_Sen_ƒë√°.md\",\n",
    "\"category_trees_Thi√™n_tu√™ÃÅ.md\",\n",
    "\"category_trees_Th·ªët_n·ªët.md\",\n",
    "\"category_trees_Th·ªßy_tr√∫c__l√°c_d√π.md\",\n",
    "\"category_trees_Th·ªßy_t√πng.md\",\n",
    "\"category_trees_Tr·∫Øc_b√°_di·ªáp.md\",\n",
    "\"category_trees_Tre_m·∫°nh_t√¥ng.md\",\n",
    "\"knowledge_handbooks_3_b∆∞·ªõc_c·∫£i_t·∫°o_ƒë·∫•t_sau_thu_ho·∫°ch_ƒë·ªëi_v·ªõi_v∆∞·ªùn_c√¢y_ƒÉn_tr√°i.md\",\n",
    "\"knowledge_handbooks_B√≥n_ƒë·∫°m_cho_l√∫a_v√†o_th·ªùi_k·ª≥_n√†o_l√†_t·ªët_nh·∫•t.md\",\n",
    "\"knowledge_handbooks_B√≥n_v√¥i_ƒë√∫ng_quy_tr√¨nh_cho_v∆∞·ªùn_c√¢y_ƒÉn_tr√°i.md\",\n",
    "\"knowledge_handbooks_C√°ch_b√≥n_l√≥t_cho_c√†_ph√™_tr·ªìng_m·ªõi.md\",\n",
    "\"knowledge_handbooks_C√°ch_b√≥n_ph√¢n_chu·ªìng_cho_rau_m√†u_hi·ªáu_qu·∫£_nh·∫•t.md\",\n",
    "\"knowledge_handbooks_C√°ch_chƒÉm_s√≥c_l√∫a_giai_ƒëo·∫°n_ƒë√≤ng_tr·ªï_gi√∫p_tƒÉng_nƒÉng_su·∫•t_hi·ªáu_qu·∫£.md\",\n",
    "\"knowledge_handbooks_C√°ch_kh·∫Øc_ph·ª•c_b∆∞·ªüi_da_xanh_b·ªã_v√†ng_ƒë·ªçt.md\",\n",
    "\"knowledge_handbooks_C√°ch_kh·∫Øc_ph·ª•c_hi·ªán_t∆∞·ª£ng_n·ª©t_tr√°i_tr√™n_c√¢y_tr·ªìng.md\",\n",
    "\"knowledge_handbooks_C√°ch_kh·∫Øc_ph·ª•c_m√≠t_x∆°_ƒëen.md\",\n",
    "\"knowledge_handbooks_C√°ch_l√†m_cho_hoa_c√†_ph√™_ra_ƒë·ªìng_lo·∫°t.md\",\n",
    "\"knowledge_handbooks_C√°ch_ph√≤ng_tr·ªã_s√¢u_v·∫Ω_b√πa_tr√™n_c√¢y_cam.md\",\n",
    "\"knowledge_handbooks_C√°ch_ph√≤ng_tr·ª´_b·ªánh_kh√¥_c√†nh_kh√¥_qu·∫£_g√¢y_h·∫°i_c√¢y_c√†_ph√™.md\",\n",
    "\"knowledge_handbooks_C√°ch_tr·ªã_s√¢u_v·∫Ω_b√πa_tr√™n_b∆∞·ªüi.md\",\n",
    "\"knowledge_handbooks_C√°ch_t∆∞·ªõi_n∆∞·ªõc_cho_m√¥_h√¨nh_tr·ªìng_h·ªì_ti√™u_xen_c√†_ph√™.md\",\n",
    "\"knowledge_handbooks_Cam_s√†nh_ra_b√¥ng__b·ªã_m∆∞a_nhi·ªÅu_c·∫ßn_l√†m_g√¨.md\",\n",
    "\"knowledge_handbooks_C·∫ßn_l√†m_g√¨_sau_khi_thu_ho·∫°ch_s·∫ßu_ri√™ng.md\",\n",
    "\"knowledge_handbooks_C√¢y_c√†_ph√™_gi√†_c·ªói_th√¨_ph·∫£i_l√†m_th·∫ø_n√†o.md\",\n",
    "\"knowledge_handbooks_C√≥_c·∫ßn_b√≥n_ph√¢n_h·ªØu_c∆°_cho_ƒë·∫•t_ph√®n_kh√¥ng.md\",\n",
    "\"knowledge_handbooks_D·ª©t_ƒëi·ªÉm_r·ªáp_s√°p__r·∫ßy_tr·∫Øng_·ªü_ph·∫ßn_r·ªÖ.md\",\n",
    "\"knowledge_handbooks_Gi√¥ÃÅng_caÃÄ_ph√™_v·ªëi_naÃÄo_ch√¢ÃÅt_l∆∞∆°Ã£ng_t·ªët_nh√¢ÃÅt_hi·ªán_nay.md\",\n",
    "\"knowledge_handbooks_Gi·ªØ_·∫©m_cho_ƒë·∫•t_trong_m√πa_kh√¥_nh∆∞_th·∫ø_n√†o.md\",\n",
    "\"knowledge_handbooks_K√≠ch_th∆∞·ªõc_ph·∫≥ng_c·ªßa_b·∫ßu_∆∞∆°m_c√†_ph√™_ra_sao.md\",\n",
    "\"knowledge_handbooks_Kinh_nghi·ªám_chƒÉm_s√≥c_s·∫ßu_ri√™ng_giai_ƒëo·∫°n_nu√¥i_tr√°i_non_hi·ªáu_qu·∫£.md\",\n",
    "\"knowledge_handbooks_K·ªπ_thu·∫≠t_c·∫Øt_t·ªâa_c√†nh_v√†_t·∫°o_t√°n_cho_c√†_ph√™.md\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "list_total_doc = os.listdir(folder_doc)\n",
    "for each_doc in list_total_doc:\n",
    "    if each_doc not in documents_test:\n",
    "        os.remove(folder_doc + each_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "documents_content = []\n",
    "for name_doc in documents_test:\n",
    "    with open(folder_doc + name_doc, \"r\") as f:\n",
    "        documents_content.append((name_doc,f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs = [LangchainDocument(page_content=doc[1], metadata={\"source\": doc[0]}) for doc in tqdm(documents_content)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# repo_id = \"noah-ai/mt5-base-question-generation-vi\"\n",
    "# repo_id= \"NlpHUST/gpt2-vietnamese\"\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token='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',\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={ \n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( docs_processed[5].page_content, \"\\n________________\", call_llm(llm_client, docs_processed[5].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_generation_prompt = \"\"\"\n",
    "# Your task is to write a factoid question and an answer given a context.\n",
    "# Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "# Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "# This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Output:::\n",
    "# Factoid question: (your factoid question)\n",
    "# Answer: (your answer to the factoid question)\n",
    "\n",
    "# Now here is the context.\n",
    "\n",
    "# Context: {context}\\n\n",
    "# Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\" Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt m·ªôt c√¢u h·ªèi th·ª±c t·∫ø v√† m·ªôt c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh.\n",
    "C√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n ph·∫£i ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng m·ªôt ƒëo·∫°n th√¥ng tin th·ª±c t·∫ø c·ª• th·ªÉ, ng·∫Øn g·ªçn t·ª´ ng·ªØ c·∫£nh.\n",
    "C√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n ph·∫£i ƒë∆∞·ª£c x√¢y d·ª±ng theo phong c√°ch gi·ªëng nh∆∞ nh·ªØng c√¢u h·ªèi m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi trong c√¥ng c·ª• t√¨m ki·∫øm.\n",
    "ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† c√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n KH√îNG PH·∫¢I ƒë·ªÅ c·∫≠p ƒë·∫øn nh·ªØng th·ª© nh∆∞ \"theo ƒëo·∫°n vƒÉn\" ho·∫∑c \"ng·ªØ c·∫£nh\".\n",
    "To√†n b·ªô c√¢u tr·∫£ l·ªùi v√† c√¢u h·ªèi ph·∫£i ƒë∆∞·ª£c vi·∫øt b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "K·∫øt qu·∫£:::\n",
    "C√¢u h·ªèi th·ª±c t·∫ø: (c√¢u h·ªèi th·ª±c t·∫ø c·ªßa b·∫°n)\n",
    "Tr·∫£ l·ªùi: (c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n cho c√¢u h·ªèi th·ª±c t·∫ø)\n",
    "\n",
    "ƒê√¢y l√† b·ªëi c·∫£nh.\n",
    "B·ªëi c·∫£nh: {context}\\n\n",
    "K·∫øt qu·∫£:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 50  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    # print(output_QA_couple)\n",
    "    # print(\"_______________________________________\")\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"C√¢u h·ªèi th·ª±c t·∫ø: \")[-1].split(\"Tr·∫£ l·ªùi: \")[0]\n",
    "        answer = output_QA_couple.split(\"Tr·∫£ l·ªùi: \")[-1]\n",
    "        print(question,answer )\n",
    "        # assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pd.DataFrame(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_groundedness_critique_prompt = \"\"\"\n",
    "# You will be given a context and a question.\n",
    "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here are the question and context.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Context: {context}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_relevance_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_standalone_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt b·ªëi c·∫£nh v√† m·ªôt c√¢u h·ªèi.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë∆∞a ra 'ƒêi·ªÉm ƒë√°nh gi√°' cho ƒëi·ªÉm m·ª©c ƒë·ªô m·ªôt ng∆∞·ªùi c√≥ th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi ƒë√£ cho m·ªôt c√°ch r√µ r√†ng v·ªõi b·ªëi c·∫£nh nh·∫•t ƒë·ªãnh.\n",
    "ƒê∆∞a ra c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n theo thang ƒëi·ªÉm t·ª´ 1 ƒë·∫øn 5, trong ƒë√≥ 1 c√≥ nghƒ©a l√† c√¢u h·ªèi ho√†n to√†n kh√¥ng th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c trong b·ªëi c·∫£nh v√† 5 c√≥ nghƒ©a l√† c√¢u h·ªèi c√≥ th·ªÉ tr·∫£ l·ªùi r√µ r√†ng v√† r√µ r√†ng v·ªõi b·ªëi c·∫£nh.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "Tr·∫£ l·ªùi:::\n",
    "ƒê√°nh gi√°: (l√Ω do ƒë√°nh gi√° c·ªßa b·∫°n, d∆∞·ªõi d·∫°ng vƒÉn b·∫£n)\n",
    "ƒêi·ªÉm ƒë√°nh gi√°: (ƒêi·ªÉm ƒë√°nh gi√° c·ªßa b·∫°n, t√≠nh b·∫±ng s·ªë t·ª´ 1 ƒë·∫øn 5)\n",
    "\n",
    "B·∫°n PH·∫¢I cung c·∫•p c√°c gi√° tr·ªã cho 'ƒê√°nh gi√°:' v√† 'ƒêi·ªÉm ƒë√°nh gi√°:' trong c√¢u tr·∫£ l·ªùi c·ªßa m√¨nh.\n",
    "\n",
    "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi v√† b·ªëi c·∫£nh.\n",
    "\n",
    "C√¢u h·ªèi: {question}\\n\n",
    "B·ªëi c·∫£nh: {context}\\n\n",
    "Tr·∫£ l·ªùi::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "B·∫°n s·∫Ω ƒë∆∞·ª£c ƒë∆∞a ra m·ªôt c√¢u h·ªèi.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p 'ƒêi·ªÉm x·∫øp h·∫°ng' th·ªÉ hi·ªán m·ª©c ƒë·ªô h·ªØu √≠ch c·ªßa c√¢u h·ªèi n√†y ƒë·ªëi v·ªõi c√°c nh√† ph√°t tri·ªÉn m√°y h·ªçc ƒëang x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng NLP v·ªõi h·ªá sinh th√°i Hugging Face.\n",
    "ƒê∆∞a ra c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n theo thang ƒëi·ªÉm t·ª´ 1 ƒë·∫øn 5, trong ƒë√≥ 1 c√≥ nghƒ©a l√† c√¢u h·ªèi kh√¥ng h·ªØu √≠ch ch√∫t n√†o v√† 5 c√≥ nghƒ©a l√† c√¢u h·ªèi c·ª±c k·ª≥ h·ªØu √≠ch.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "Tr·∫£ l·ªùi:::\n",
    "ƒê√°nh gi√°: (l√Ω do ƒë√°nh gi√° c·ªßa b·∫°n, d∆∞·ªõi d·∫°ng vƒÉn b·∫£n)\n",
    "ƒêi·ªÉm ƒë√°nh gi√°: (ƒêi·ªÉm ƒë√°nh gi√° c·ªßa b·∫°n, t√≠nh b·∫±ng s·ªë t·ª´ 1 ƒë·∫øn 5)\n",
    "\n",
    "B·∫°n PH·∫¢I cung c·∫•p c√°c gi√° tr·ªã cho 'ƒê√°nh gi√°:' v√† 'ƒêi·ªÉm ƒë√°nh gi√°:' trong c√¢u tr·∫£ l·ªùi c·ªßa m√¨nh.\n",
    "\n",
    "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi.\n",
    "\n",
    "C√¢u h·ªèi: {question}\\n\n",
    "Tr·∫£ l·ªùi::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "B·∫°n s·∫Ω ƒë∆∞·ª£c ƒë∆∞a ra m·ªôt c√¢u h·ªèi.\n",
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p 'ƒêi·ªÉm x·∫øp h·∫°ng' th·ªÉ hi·ªán m·ª©c ƒë·ªô ƒë·ªôc l·∫≠p c·ªßa c√¢u h·ªèi n√†y v·ªõi b·ªëi c·∫£nh.\n",
    "ƒê∆∞a ra c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n theo thang ƒëi·ªÉm t·ª´ 1 ƒë·∫øn 5, trong ƒë√≥ 1 c√≥ nghƒ©a l√† c√¢u h·ªèi ph·ª• thu·ªôc v√†o th√¥ng tin b·ªï sung ƒë·ªÉ hi·ªÉu. 5 c√≥ nghƒ©a l√† b·∫£n th√¢n c√¢u h·ªèi c√≥ √Ω nghƒ©a v√† c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c.\n",
    "V√≠ d·ª•: n·∫øu c√¢u h·ªèi ƒë·ªÅ c·∫≠p ƒë·∫øn m·ªôt c√†i ƒë·∫∑t c·ª• th·ªÉ, nh∆∞ 'trong b·ªëi c·∫£nh' ho·∫∑c 'trong t√†i li·ªáu' th√¨ x·∫øp h·∫°ng ph·∫£i l√† 1.\n",
    "C√°c c√¢u h·ªèi c√≥ th·ªÉ ch·ª©a c√°c danh t·ª´ ho·∫∑c t·ª´ vi·∫øt t·∫Øt k·ªπ thu·∫≠t kh√≥ hi·ªÉu nh∆∞ Gradio, Hub, Hugging Face v√† v·∫´n ·ªü m·ª©c 5: n√≥ ch·ªâ ƒë∆°n gi·∫£n l√† ph·∫£i r√µ r√†ng ƒë·ªëi v·ªõi ng∆∞·ªùi v·∫≠n h√†nh c√≥ quy·ªÅn truy c·∫≠p v√†o t√†i li·ªáu v·ªÅ n·ªôi dung c√¢u h·ªèi.\n",
    "\n",
    "V√≠ d·ª•: \"T·ªï ch·ª©c Hugging Face nƒÉm 2023 c√≥ bao nhi√™u ng∆∞·ªùi d√πng?\" s·∫Ω nh·∫≠n ƒë∆∞·ª£c ƒëi·ªÉm 1, v√¨ c√≥ s·ª± ƒë·ªÅ c·∫≠p ng·∫ßm ƒë·∫øn m·ªôt ng·ªØ c·∫£nh, do ƒë√≥ c√¢u h·ªèi kh√¥ng ƒë·ªôc l·∫≠p v·ªõi ng·ªØ c·∫£nh.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "Tr·∫£ l·ªùi:::\n",
    "ƒê√°nh gi√°: (l√Ω do ƒë√°nh gi√° c·ªßa b·∫°n, d∆∞·ªõi d·∫°ng vƒÉn b·∫£n)\n",
    "ƒêi·ªÉm ƒë√°nh gi√°: (ƒêi·ªÉm ƒë√°nh gi√° c·ªßa b·∫°n, t√≠nh b·∫±ng s·ªë t·ª´ 1 ƒë·∫øn 5)\n",
    "\n",
    "B·∫°n PH·∫¢I cung c·∫•p c√°c gi√° tr·ªã cho 'ƒê√°nh gi√°:' v√† 'ƒêi·ªÉm ƒë√°nh gi√°:' trong c√¢u tr·∫£ l·ªùi c·ªßa m√¨nh.\n",
    "\n",
    "B√¢y gi·ªù ƒë√¢y l√† c√¢u h·ªèi.\n",
    "\n",
    "C√¢u h·ªèi: {question}\\n\n",
    "Tr·∫£ l·ªùi::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "# outputs_2 = outputs[1:4]\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                evaluation.split(\"ƒêi·ªÉm ƒë√°nh gi√°: \")[-1].strip(),\n",
    "                evaluation.split(\"ƒêi·ªÉm ƒë√°nh gi√°: \")[-2].split(\"ƒê√°nh gi√°: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # print(\"______________________________________________________\")\n",
    "        continue\n",
    "    # print(output)\n",
    "    # print(\"______________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 20)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "generated_questions_process = generated_questions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_score(score_string):\n",
    "    if score_string is np.nan:\n",
    "        return np.nan\n",
    "    return int(str(score_string)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process.iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process['groundedness_score'] = generated_questions_process['groundedness_score'].apply(process_score)\n",
    "generated_questions_process['relevance_score'] = generated_questions_process['relevance_score'].apply(process_score)\n",
    "generated_questions_process['standalone_score'] = generated_questions_process['standalone_score'].apply(process_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions_process[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions_process_2 = generated_questions_process.loc[\n",
    "    (generated_questions_process[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions_process[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions_process[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions_process_2[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions_process_2, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.to_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG PIPELINE TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../app/api/')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm import *\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "from helpers import *\n",
    "from models import *\n",
    "from config import *\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "import logging\n",
    "from typing import (\n",
    "    List,\n",
    "    Union,\n",
    "    Optional,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Any\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangChainDocument\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from fastapi import HTTPException\n",
    "from uuid import UUID, uuid4\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    MarkdownTextSplitter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER=\"postgres\"\n",
    "POSTGRES_PASSWORD=\"postgres\"\n",
    "POSTGRES_DB=\"postgres\"\n",
    "PGVECTOR_ADD_INDEX=True\n",
    "\n",
    "# DB_HOST=localhost\n",
    "DB_HOST=\"localhost\"\n",
    "DB_PORT=5432\n",
    "# DB_PORT=5132\n",
    "DB_USER=\"api\"\n",
    "DB_NAME=\"api\"\n",
    "DB_PASSWORD=123\n",
    "VECTOR_EMBEDDINGS_DIM = 768\n",
    "OPENAI_API_KEY='sk-proj-SP7z7Y29wCjNbtRnoRoRT3BlbkFJM4oUr3l8Mv0X6vBdKqF7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SU_DSN = (\n",
    "    f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engine_test(dsn: str = SU_DSN):\n",
    "    return create_engine(dsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_engine = Session(get_engine_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_random_agent():\n",
    "    return random.choice(AGENT_NAMES)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Get the count of tokens used\n",
    "# ----------------------------\n",
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def get_token_count(text: str):\n",
    "    if not text:\n",
    "        return 0\n",
    "\n",
    "    return OpenAI().get_num_tokens(text=text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VECTOR_EMBEDDINGS_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Query embedding search for similar documents\n",
    "# --------------------------------------------\n",
    "def get_nodes_by_embedding_test(\n",
    "    embeddings: List[float],\n",
    "    k: int = LLM_MIN_NODE_LIMIT,\n",
    "    distance_strategy: Optional[DISTANCE_STRATEGY] = LLM_DEFAULT_DISTANCE_STRATEGY,\n",
    "    distance_threshold: Optional[float] = LLM_DISTANCE_THRESHOLD,\n",
    "    session: Optional[Session] = None,\n",
    ") -> List[Node]:\n",
    "    # Convert embeddings array into sql string\n",
    "    embeddings_str = str(embeddings)\n",
    "\n",
    "    if distance_strategy == DISTANCE_STRATEGY.EUCLIDEAN:\n",
    "        distance_fn = \"match_node_euclidean\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.COSINE:\n",
    "        distance_fn = \"match_node_cosine\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.MAX_INNER_PRODUCT:\n",
    "        distance_fn = \"match_node_max_inner_product\"\n",
    "    else:\n",
    "        raise Exception(f\"Invalid distance strategy {distance_strategy}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Lets do a similarity search\n",
    "    # ---------------------------\n",
    "    sql = f\"\"\"SELECT * FROM {distance_fn}(\n",
    "    '{embeddings_str}'::vector({VECTOR_EMBEDDINGS_DIM}),\n",
    "    {float(distance_threshold)}::double precision,\n",
    "    {int(k)});\"\"\"\n",
    "    # print(sql)\n",
    "    # logger.debug(f'üîç Query: {sql}')\n",
    "\n",
    "    # Execute query, convert results to Node objects\n",
    "    if not session:\n",
    "        with Session(get_engine_test()) as session:\n",
    "            nodes = session.exec(text(sql)).all()\n",
    "    else:\n",
    "        nodes = session.exec(text(sql)).all()\n",
    "    return [Node.by_uuid(str(node[0])) for node in nodes] if nodes else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Retrieve prompt template\n",
    "# ------------------------\n",
    "def get_prompt_template_2(\n",
    "    user_query: str = None,\n",
    "    context_str: str = None,\n",
    "    project: Optional[Project] = None,\n",
    "    organization: Optional[Organization] = None,\n",
    "    agent: str = None,\n",
    ") -> str:\n",
    "    agent = f\"{agent}, \" if agent else \"\"\n",
    "    user_query = user_query if user_query else \"\"\n",
    "    context_str = context_str if context_str else \"\"\n",
    "    organization = (\n",
    "        project.organization.display_name\n",
    "        if project\n",
    "        else organization.display_name\n",
    "        if organization\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    if not context_str or not user_query:\n",
    "        raise ValueError(\n",
    "            \"Missing required arguments context_str, user_query, organization, agent\"\n",
    "        )\n",
    "    system_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"[AGENT]:\n",
    "    T√¥i s·∫Ω tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa [USER] ch·ªâ b·∫±ng c√°ch s·ª≠ d·ª•ng  [DOCUMENT] v√† tu√¢n theo [Quy t·∫Øc].\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context_str}\n",
    "\n",
    "    [QUY T·∫ÆC]:\n",
    "    T√¥i s·∫Ω ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng b·∫±ng  [DOCUMENT] ƒë∆∞·ª£c cung c·∫•p. T√¥i s·∫Ω tu√¢n th·ªß c√°c quy t·∫Øc sau:\n",
    "    - T√¥i l√† nh√¢n vi√™n h·ªó tr·ª£ kh√°ch h√†ng t·ªët nh·∫•t hi·ªán nay\n",
    "    - T√¥i s·∫Ω tr·∫£ l·ªùi to√†n b·ªô n·ªôi dung trong [DOCUMENT]\n",
    "    - T√¥i kh√¥ng bao gi·ªù n√≥i d·ªëi hay b·ªãa ra nh·ªØng c√¢u tr·∫£ l·ªùi kh√¥ng ƒë∆∞·ª£c n√™u r√µ r√†ng trong [DOCUMENT]\n",
    "    - N·∫øu t√¥i kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi ho·∫∑c c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ r√µ r√†ng trong [DOCUMENT], t√¥i s·∫Ω n√≥i: \"T√¥i xin l·ªói, t√¥i kh√¥ng bi·∫øt ph·∫£i tr·ª£ gi√∫p ƒëi·ªÅu ƒë√≥ nh∆∞ th·∫ø n√†o\".\n",
    "    - T√¥i lu√¥n gi·ªØ c√¢u tr·∫£ l·ªùi d√†i, ph√π h·ª£p v√† s√∫c t√≠ch.\n",
    "    - T√¥i s·∫Ω lu√¥n ph·∫£n h·ªìi ·ªü ƒë·ªãnh d·∫°ng JSON b·∫±ng c√°c kh√≥a sau: \"message\" ph·∫£n h·ªìi c·ªßa t√¥i cho ng∆∞·ªùi d√πng.\n",
    "    \"\"\",\n",
    "            }\n",
    "        ]\n",
    "#     system_prompt = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": f\"\"\"[AGENT]:\n",
    "#  I will answer the [USER] questions using only the [DOCUMENT] and following the [RULES].\n",
    "\n",
    "# [DOCUMENT]:\n",
    "# {context_str}\n",
    "\n",
    "# [RULES]:\n",
    "# I will answer the user's questions using only the [DOCUMENT] provided. I will abide by the following rules:\n",
    "# - I am a kind and helpful human, the best customer support agent in existence\n",
    "# - I will answer all content  in [DOCUMENT]\n",
    "# - I never lie or invent answers not explicitly provided in [DOCUMENT]\n",
    "# - If I am unsure of the answer response or the answer is not explicitly contained in [DOCUMENT], I will say: \"I apologize, I'm not sure how to help with that\".\n",
    "# - I always keep my answers long, relevant and concise.\n",
    "# - I will always respond in JSON format with the following keys: \"message\" my response to the user, \"tags\" an array of short labels categorizing user input, \"is_escalate\" a boolean, returning false if I am unsure and true if I do have a relevant answer\n",
    "# \"\"\",\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "    return (system_prompt, f\"[USER]:\\n{user_query}\")\n",
    "# f\"[USER]:\\n{user_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bartbert(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    LLM_CHUNK_SIZE = 1024\n",
    "    LLM_CHUNK_OVERLAP = 100\n",
    "    logger.debug(documents)\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=LLM_CHUNK_SIZE,\n",
    "                        chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                        add_start_index=True,\n",
    "                        separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                    )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # encoding = tokenizer_bartpho(arr_documents,padding='max_length',return_tensors='pt', truncation=True, max_length=1024)\n",
    "    # input_ids = encoding['input_ids']\n",
    "    # attention_mask = encoding['attention_mask'] \n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     features = bartpho(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # last_hidden_state, _ = features[0], features[1]\n",
    "    # embeddings = last_hidden_state.mean(dim=1)\n",
    "\n",
    "\n",
    "    # output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    # # output_segment = [each[0] for each in output_segment]\n",
    "    output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    # output_segment = [each[0] for each in output_segment]\n",
    "\n",
    "    total_segment_doc = []\n",
    "    for list_segment_in_doc in output_segment_doc:\n",
    "        segment_doc = \"\"\n",
    "        for each in list_segment_in_doc:\n",
    "            segment_doc += each + \" \"\n",
    "        total_segment_doc.append(segment_doc.strip())\n",
    "\n",
    "    encoding_doc = tokenizer_bartpho(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=int(MAX_TOKEN_EMBEDDINGS))\n",
    "    input_ids_doc = encoding_doc['input_ids']\n",
    "    attention_mask_doc = encoding_doc['attention_mask'] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = bartpho(input_ids_doc,attention_mask=attention_mask_doc)\n",
    "\n",
    "    last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "    embeddings = last_hidden_state.mean(dim=1)\n",
    "    print(arr_documents) \n",
    "    print(len(arr_documents))\n",
    "    print(total_segment_doc)\n",
    "    print(len(arr_documents))\n",
    "    print(embeddings)\n",
    "    print(embeddings.shape)\n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "import torch\n",
    "import numpy as nps\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/model.safetensors HTTP/1.1\" 404 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/vocab.txt HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer_phobert = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')\n",
    "\n",
    "def get_embeddings_photbert(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    LLM_CHUNK_SIZE = 256\n",
    "    LLM_CHUNK_OVERLAP = 20\n",
    "    logger.debug(documents)\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=LLM_CHUNK_SIZE,\n",
    "                        chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                        add_start_index=True,\n",
    "                        separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                    )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "    output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    total_segment_doc = []\n",
    "    for list_segment_in_doc in output_segment_doc:\n",
    "        segment_doc = \"\"\n",
    "        for each in list_segment_in_doc:\n",
    "            segment_doc += each + \" \"\n",
    "        total_segment_doc.append(segment_doc.strip())\n",
    "\n",
    "    encoding_doc = tokenizer_phobert(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=int(256))\n",
    "    input_ids_doc = encoding_doc['input_ids']\n",
    "    attention_mask_doc = encoding_doc['attention_mask'] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids_doc,attention_mask=attention_mask_doc)\n",
    "\n",
    "    last_hidden_state, _ = features[0], features[1]\n",
    "    embeddings = last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # print(arr_documents) \n",
    "    # print(len(arr_documents))\n",
    "    # print(total_segment_doc)\n",
    "    # print(len(arr_documents))\n",
    "    # print(embeddings)\n",
    "    # print(embeddings.shape)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'L√†m th·∫ø n√†o ƒë·ªÉ t·∫°o ƒëi·ªÅu ki·ªán ƒë·∫£m b·∫£o tho√°t n∆∞·ªõc t·ªët v√† h·∫°n ch·∫ø ng·∫≠p √∫ng cho c√¢y s·∫ßu ri√™ng?\\n'\n",
    "# arr_documents, embeddings = get_embeddings_bartbert(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_documents, embeddings = get_embeddings_photbert(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance(question, embedding_question = None, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'L√†m th·∫ø n√†o ƒë·ªÉ t·∫°o ƒëi·ªÅu ki·ªán ƒë·∫£m b·∫£o tho√°t n∆∞·ªõc t·ªët v√† h·∫°n ch·∫ø ng·∫≠p √∫ng cho c√¢y s·∫ßu ri√™ng?\\n'\n",
    "    # if embedding_question is None:\n",
    "    #     try:\n",
    "    #         # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    #         arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    #     except Exception as e: \n",
    "    #         print(e)\n",
    "    #         return None, None ,None\n",
    "    #     query_embeddings = embeddings[0].tolist()\n",
    "    # else:\n",
    "    #     query_embeddings = embedding_question\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    print(question)\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    \n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        document = session.get(Node, nodes[0].id).document\n",
    "        if document not in list_doc:\n",
    "            list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        system_prompt, user_prompt = get_prompt_template_2(\n",
    "            user_query=question,\n",
    "            context_str=context_str,\n",
    "            project=project,\n",
    "            organization=organization,\n",
    "            # agent=agent_name,\n",
    "        )\n",
    "        try:\n",
    "            llm_response = json.loads(\n",
    "                            retrieve_llm_response_test(\n",
    "                            user_prompt,\n",
    "                            model=model,\n",
    "                            # max_output_tokens=256,\n",
    "                            prefix_messages=system_prompt,\n",
    "                            )\n",
    "                            )\n",
    "            return list_relevant_docs, llm_response.get('message'), query_embeddings\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            return None ,None, query_embeddings\n",
    "\n",
    "    return None, None, query_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "# try:  # load previous generations if they exist\n",
    "#     with open(output_file, \"r\") as f:\n",
    "#         outputs = json.load(f)\n",
    "# except:\n",
    "#     outputs = []\n",
    "number_error = 0\n",
    "while number_error < 1:\n",
    "    # number_error+=1 \n",
    "    outputs = []\n",
    "    # ds_continue = generated_questions.iloc[45:]\n",
    "\n",
    "    for index in tqdm(range(len(ds))):\n",
    "        if number_error >10:\n",
    "            break\n",
    "        example = ds.iloc[index]\n",
    "        question = example[\"question\"]\n",
    "        embedding_question = example[\"embedding_question\"]\n",
    "        print(example['generated_answer'],111111111111111111)\n",
    "        if example['generated_answer'] is not None:\n",
    "            continue\n",
    "        relevant_docs, answer,embedding_question  = test_rag_performance(question,embedding_question)\n",
    "        if answer is None:\n",
    "            number_error+=1\n",
    "\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "            break\n",
    "        print(\"=======================================================\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f'True answer: {example[\"answer\"]}')\n",
    "        ds.at[index,'generated_answer'] = answer\n",
    "        ds.at[index,'retrieved_docs'] = relevant_docs\n",
    "        ds.at[index,'embedding_question'] = embedding_question\n",
    "\n",
    "        # result = {\n",
    "        #     \"question\": question,\n",
    "        #     \"true_answer\": example[\"answer\"],\n",
    "        #     \"source_doc\": example[\"source_doc\"],\n",
    "        #     \"generated_answer\": answer,\n",
    "        #     \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        # }\n",
    "        # if result not in outputs:\n",
    "        #     outputs.append(result)\n",
    "\n",
    "        # result = {\n",
    "        #     \"question\": question,\n",
    "        #     \"true_answer\": example[\"answer\"],\n",
    "        #     \"source_doc\": example[\"source_doc\"],\n",
    "        #     \"generated_answer\": None,\n",
    "        #     \"retrieved_docs\": None,\n",
    "        # }\n",
    "        \n",
    "        # if result not in outputs:\n",
    "        #     outputs.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\" Nhi·ªám v·ª• c·ªßa b·∫°n l√† vi·∫øt m·ªôt c√¢u tr·∫£ l·ªùi ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng d·ª±a tr√™n b·ªëi c·∫£nh.\n",
    "C√¢u tr·∫£ l·ªùi to√†n b·ªô n·ªôi dung trong b·ªëi  c·∫£nh\n",
    "B·∫°n kh√¥ng bao gi·ªù n√≥i d·ªëi hay b·ªãa ra nh·ªØng c√¢u tr·∫£ l·ªùi kh√¥ng ƒë∆∞·ª£c n√™u r√µ r√†ng trong b·ªëi c·∫£nh\n",
    "N·∫øu b·∫°n kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi ho·∫∑c c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ r√µ r√†ng trong b·ªëi c·∫£nh, b·∫°n s·∫Ω n√≥i: \"T√¥i xin l·ªói, t√¥i kh√¥ng bi·∫øt ph·∫£i tr·ª£ gi√∫p ƒëi·ªÅu ƒë√≥ nh∆∞ th·∫ø n√†o\".\n",
    "B·∫°n lu√¥n gi·ªØ c√¢u tr·∫£ l·ªùi d√†i, ph√π h·ª£p v√† ch√≠nh xa\n",
    "C√¢u tr·∫£ l·ªùi b·∫±ng m·ªôt ƒëo·∫°n th√¥ng tin th·ª±c t·∫ø c·ª• th·ªÉ, ng·∫Øn g·ªçn t·ª´ ng·ªØ c·∫£nh.\n",
    "To√†n b·ªô c√¢u tr·∫£ l·ªùi v√† c√¢u h·ªèi ph·∫£i ƒë∆∞·ª£c vi·∫øt b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "Cung c·∫•p c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n nh∆∞ sau:\n",
    "\n",
    "K·∫øt qu·∫£:::\n",
    "Tr·∫£ l·ªùi: (c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n cho c√¢u h·ªèi th·ª±c t·∫ø)\n",
    "\n",
    "ƒê√¢y l√† b·ªëi c·∫£nh.\n",
    "B·ªëi c·∫£nh: {context}\\n\n",
    "ƒê√¢y l√† c√¢u h·ªèi.\n",
    "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}\\n\n",
    "K·∫øt qu·∫£:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\"\n",
    "   [INST]\n",
    "    B·∫°n s·∫Ω tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa [USER] ch·ªâ b·∫±ng c√°ch s·ª≠ d·ª•ng  [DOCUMENT] v√† tu√¢n theo [Quy t·∫Øc].\n",
    "    [USER]:\\n{question}\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context}\n",
    "\n",
    "    [QUY T·∫ÆC]:\n",
    "    T√¥i s·∫Ω ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng b·∫±ng  [DOCUMENT] ƒë∆∞·ª£c cung c·∫•p. T√¥i s·∫Ω tu√¢n th·ªß c√°c quy t·∫Øc sau:\n",
    "    - B·∫°n l√† nh√¢n vi√™n h·ªó tr·ª£ kh√°ch h√†ng t·ªët nh·∫•t hi·ªán nay\n",
    "    - B·∫°n s·∫Ω tr·∫£ l·ªùi to√†n b·ªô n·ªôi dung trong [DOCUMENT]\n",
    "    - B·∫°n kh√¥ng bao gi·ªù n√≥i d·ªëi hay b·ªãa ra nh·ªØng c√¢u tr·∫£ l·ªùi kh√¥ng ƒë∆∞·ª£c n√™u r√µ r√†ng trong [DOCUMENT]\n",
    "    - N·∫øu B·∫°n kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi ho·∫∑c c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ r√µ r√†ng trong [DOCUMENT], B·∫°n s·∫Ω n√≥i: \"T√¥i xin l·ªói, t√¥i kh√¥ng bi·∫øt ph·∫£i tr·ª£ gi√∫p ƒëi·ªÅu ƒë√≥ nh∆∞ th·∫ø n√†o\".\n",
    "    - B·∫°n lu√¥n gi·ªØ c√¢u tr·∫£ l·ªùi d√†i, ph√π h·ª£p v√† s√∫c t√≠ch.\n",
    "    - B·∫°n s·∫Ω lu√¥n ph·∫£n h·ªìi k·∫øt qu·∫£ c·ªßa b·∫°n ·ªü ƒë·ªãnh d·∫°ng JSON \n",
    "    - Ch·ªâ c·∫ßn t·∫°o ƒë·ªëi t∆∞·ª£ng JSON b·∫±ng kh√≥a sau: \"message\" l√† ph·∫£n h·ªìi c·ªßa b·∫°n  cho ng∆∞·ªùi d√πng. Kh√¥ng c·∫ßn gi·∫£i th√≠ch:\n",
    "    [/INST]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance_2(question, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'L√†m th·∫ø n√†o ƒë·ªÉ t·∫°o ƒëi·ªÅu ki·ªán ƒë·∫£m b·∫£o tho√°t n∆∞·ªõc t·ªët v√† h·∫°n ch·∫ø ng·∫≠p √∫ng cho c√¢y s·∫ßu ri√™ng?\\n'\n",
    "    # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    print(question)\n",
    "    session= main_engine\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "\n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    print(nodes)\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        for node in nodes:\n",
    "            document = session.get(Node, node.id).document\n",
    "            if document not in list_doc:\n",
    "                list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        # system_prompt, user_prompt = get_prompt_template_2(\n",
    "        #     user_query=question,\n",
    "        #     context_str=context_str,\n",
    "        #     project=project,\n",
    "        #     organization=organization,\n",
    "        #     # agent=agent_name,\n",
    "        # )\n",
    "        # try:\n",
    "        # repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "        # llm_client = InferenceClient(\n",
    "        #     model=repo_id,\n",
    "        #     timeout=120,\n",
    "        #     token='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',\n",
    "        # )\n",
    "\n",
    "        \n",
    "        # llm_response = call_llm(llm_client,QA_generation_answer_prompt.format(context=context_str, question=question))\n",
    "        llm_response = None\n",
    "        # llm_response = json.loads(\n",
    "        #                 retrieve_llm_response_test(\n",
    "        #                 user_prompt,\n",
    "        #                 model=model,\n",
    "        #                 # max_output_tokens=256,\n",
    "        #                 prefix_messages=system_prompt,\n",
    "        #                 )\n",
    "                        # )\n",
    "                \n",
    "        return list_relevant_docs, llm_response,list_doc\n",
    "        # except:\n",
    "        #     return None ,None\n",
    "\n",
    "    return None, None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index =3\n",
    "ds.iloc[index].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_relevant_docs, llm_response,list_doc = test_rag_performance_2(ds.iloc[index].question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SPECIFIC EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as nps\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'E:/TrungPhanADVN/Code/LangChain_RAG/scripts/bartpho-word/'\n",
    "bartpho = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_bartpho = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Query embedding search for similar documents\n",
    "# --------------------------------------------\n",
    "def get_nodes_by_embedding_custom(\n",
    "    embeddings: List[float],\n",
    "    k: int = LLM_MIN_NODE_LIMIT,\n",
    "    distance_strategy: Optional[DISTANCE_STRATEGY] = LLM_DEFAULT_DISTANCE_STRATEGY,\n",
    "    distance_threshold: Optional[float] = LLM_DISTANCE_THRESHOLD,\n",
    "    session: Optional[Session] = main_engine,\n",
    ") -> List[Node]:\n",
    "    # Convert embeddings array into sql string\n",
    "    embeddings_str = str(embeddings)\n",
    "\n",
    "    if distance_strategy == DISTANCE_STRATEGY.EUCLIDEAN:\n",
    "        distance_fn = \"match_node_euclidean_2\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.COSINE:\n",
    "        distance_fn = \"match_node_cosine_2\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.MAX_INNER_PRODUCT:\n",
    "        distance_fn = \"match_node_max_inner_product_2\"\n",
    "    else:\n",
    "        raise Exception(f\"Invalid distance strategy {distance_strategy}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Lets do a similarity search\n",
    "    # ---------------------------\n",
    "    sql = f\"\"\"SELECT * FROM {distance_fn}(\n",
    "    '{embeddings_str}'::vector({VECTOR_EMBEDDINGS_DIM}),\n",
    "    {float(distance_threshold)}::double precision,\n",
    "    {int(k)});\"\"\"\n",
    "    # print(sql)\n",
    "    # logger.debug(f'üîç Query: {sql}')\n",
    "\n",
    "    # Execute query, convert results to Node objects\n",
    "    if not session:\n",
    "        with Session(get_engine_test()) as session:\n",
    "            nodes = session.exec(text(sql)).all()\n",
    "    else:\n",
    "        nodes = session.exec(text(sql)).all()\n",
    "    return [Node.by_uuid(str(node[0])) for node in nodes] if nodes else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding_performance(question, strategy = DISTANCE_STRATEGY.EUCLIDEAN, threshold = 0.3,model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "\n",
    "\n",
    "    arr_documents, embeddings = get_embeddings_photbert(question)\n",
    "    # arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    # session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT = 5\n",
    "    session = main_engine\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    # print(query_embeddings)\n",
    "\n",
    "    nodes = get_nodes_by_embedding_custom(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=strategy,\n",
    "                                    distance_threshold=threshold,\n",
    "                                    session=main_engine\n",
    "                                )\n",
    "\n",
    "    # nodes = get_nodes(query_embeddings,\n",
    "    #                             LLM_MIN_NODE_LIMIT,\n",
    "    #                             distance_strategy=strategy,\n",
    "    #                             distance_threshold=threshold,\n",
    "    #                             session=session\n",
    "    #                         )\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        for node in nodes:\n",
    "            document = session.get(Node, node.id).document\n",
    "            if document not in list_doc:\n",
    "                list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "    return list_doc, list_relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√¥ng d·ª•ng c·ªßa c√¢y nha ƒëam ƒë·ªëi v·ªõi s·ª©c kh·ªèe?\\n')]\n",
      "[Document(page_content='C√¥ng d·ª•ng c·ªßa c√¢y nha ƒëam ƒë·ªëi v·ªõi s·ª©c kh·ªèe?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 1/29 [00:01<00:49,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ th·ª±c hi·ªán k·ªπ thu·∫≠t c·∫Øt t·ªâa v√† t·∫°o t√°n cho c√¢y c√† ph√™?\\n')]\n",
      "[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ th·ª±c hi·ªán k·ªπ thu·∫≠t c·∫Øt t·ªâa v√† t·∫°o t√°n cho c√¢y c√† ph√™?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 2/29 [00:02<00:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ c√† ph√™ ra hoa ƒë·ªìng lo·∫°t?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ c√† ph√™ ra hoa ƒë·ªìng lo·∫°t?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 3/29 [00:04<00:33,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Chu·ªëi r·∫Ω qu·∫°t c√≥ ph·∫£i l√† lo·∫°i chu·ªëi?\\n')]\n",
      "[Document(page_content='Chu·ªëi r·∫Ω qu·∫°t c√≥ ph·∫£i l√† lo·∫°i chu·ªëi?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 4/29 [00:05<00:32,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°ch phun lo·∫°i thu·ªëc l∆∞u d·∫´n ·ªü giai ƒëo·∫°n m·ªõi nh√∫ ƒë·ªçt non?\\n')]\n",
      "[Document(page_content='C√°ch phun lo·∫°i thu·ªëc l∆∞u d·∫´n ·ªü giai ƒëo·∫°n m·ªõi nh√∫ ƒë·ªçt non?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 5/29 [00:06<00:33,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ kh·∫Øc ph·ª•c n·ª©t tr√°i tr√™n c√¢y tr·ªìng?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ kh·∫Øc ph·ª•c n·ª©t tr√°i tr√™n c√¢y tr·ªìng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 6/29 [00:08<00:32,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='T·∫°i sao 2N√¥ng h·ªó tr·ª£ truy xu·∫•t ngu·ªìn g·ªëc s·∫£n ph·∫©m?\\n')]\n",
      "[Document(page_content='T·∫°i sao 2N√¥ng h·ªó tr·ª£ truy xu·∫•t ngu·ªìn g·ªëc s·∫£n ph·∫©m?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 7/29 [00:09<00:30,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ph√≤ng tr·ª´ c√¥n tr√πng v√† n·∫•m b·ªánh cho c√¢y s·∫ßu ri√™ng?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ph√≤ng tr·ª´ c√¥n tr√πng v√† n·∫•m b·ªánh cho c√¢y s·∫ßu ri√™ng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 8/29 [00:10<00:27,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ b√≥n ƒë·∫°m cho l√∫a?\\n')]\n",
      "[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ b√≥n ƒë·∫°m cho l√∫a?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà       | 9/29 [00:12<00:26,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ tr·ªìng v√† chƒÉm s√≥c c√¢y chu ƒëinh lan?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ tr·ªìng v√† chƒÉm s√≥c c√¢y chu ƒëinh lan?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:13<00:24,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°ch kh·∫Øc ph·ª•c m√≠t x∆° ƒëen?\\n')]\n",
      "[Document(page_content='C√°ch kh·∫Øc ph·ª•c m√≠t x∆° ƒëen?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:14<00:23,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°c bi·ªán ph√°p ph√≤ng tr·ª´ b·ªánh kh√¥ c√†nh kh√¥ qu·∫£ cho c√¢y c√† ph√™?\\n')]\n",
      "[Document(page_content='C√°c bi·ªán ph√°p ph√≤ng tr·ª´ b·ªánh kh√¥ c√†nh kh√¥ qu·∫£ cho c√¢y c√† ph√™?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:16<00:22,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Th·ªßy t√πng l√† lo√†i th·ª±c v·∫≠t n√†o?\\n')]\n",
      "[Document(page_content='Th·ªßy t√πng l√† lo√†i th·ª±c v·∫≠t n√†o?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:17<00:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi n√†o Tre M·∫°nh T√¥ng ƒë∆∞·ª£c m√¥ t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n?\\n')]\n",
      "[Document(page_content='Khi n√†o Tre M·∫°nh T√¥ng ƒë∆∞·ª£c m√¥ t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:18<00:19,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan Vanda?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan Vanda?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:19<00:17,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='S·∫£n l∆∞·ª£ng l√∫a thu ho·∫°ch nƒÉm 2022 c·ªßa Vi·ªát Nam ∆∞·ªõc ƒë·∫°t bao nhi√™u t·∫•n?\\n')]\n",
      "[Document(page_content='S·∫£n l∆∞·ª£ng l√∫a thu ho·∫°ch nƒÉm 2022 c·ªßa Vi·ªát Nam ∆∞·ªõc ƒë·∫°t bao nhi√™u t·∫•n?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:21<00:16,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Lo√†i th·ª±c v·∫≠t thu·ªôc chi S·∫ßu ri√™ng n√†o l√† lo√†i ph·ªï bi·∫øn nh·∫•t?\\n')]\n",
      "[Document(page_content='Lo√†i th·ª±c v·∫≠t thu·ªôc chi S·∫ßu ri√™ng n√†o l√† lo√†i ph·ªï bi·∫øn nh·∫•t?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:22<00:15,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='T·∫°i sao c·∫ßn lo·∫°i b·ªè c√°c c√†nh kh√¥ng mang tr√°i, c√†nh gi√† y·∫øu, c√†nh b·ªã nhi·ªÖm b·ªánh?\\n')]\n",
      "[Document(page_content='T·∫°i sao c·∫ßn lo·∫°i b·ªè c√°c c√†nh kh√¥ng mang tr√°i, c√†nh gi√† y·∫øu, c√†nh b·ªã nhi·ªÖm b·ªánh?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:23<00:13,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?\\n')]\n",
      "[Document(page_content='C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:24<00:12,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Sau khi thu ho·∫°ch s·∫ßu ri√™ng, c·∫ßn l√†m g√¨ ƒë·ªÉ chƒÉm s√≥c c√¢y?\\n')]\n",
      "[Document(page_content='Sau khi thu ho·∫°ch s·∫ßu ri√™ng, c·∫ßn l√†m g√¨ ƒë·ªÉ chƒÉm s√≥c c√¢y?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:26<00:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='ƒê√¢u l√† t√™n g·ªçi ti·∫øng Vi·ªát c·ªßa lo√†i d·ª´a n∆∞·ªõc?\\n')]\n",
      "[Document(page_content='ƒê√¢u l√† t√™n g·ªçi ti·∫øng Vi·ªát c·ªßa lo√†i d·ª´a n∆∞·ªõc?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:27<00:10,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ƒëi·ªÅu tr·ªã s√¢u ƒÉn l√° ho·∫∑c b√°m v√†o c√¢y hoa Lan Cattleya?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ƒëi·ªÅu tr·ªã s√¢u ƒÉn l√° ho·∫∑c b√°m v√†o c√¢y hoa Lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:29<00:09,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ b·ªï sung ch·∫•t h·ªØu c∆° cho ƒë·∫•t sau thu ho·∫°ch ƒë·ªëi v·ªõi v∆∞·ªùn c√¢y ƒÉn tr√°i?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ b·ªï sung ch·∫•t h·ªØu c∆° cho ƒë·∫•t sau thu ho·∫°ch ƒë·ªëi v·ªõi v∆∞·ªùn c√¢y ƒÉn tr√°i?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:30<00:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c hoa lan Cattleya?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c hoa lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:31<00:06,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi cho m·ªôt c√¢y h·ªì ti√™u trong m√¥ h√¨nh tr·ªìng h·ªì ti√™u xen c√† ph√™?\\n')]\n",
      "[Document(page_content='L∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi cho m·ªôt c√¢y h·ªì ti√™u trong m√¥ h√¨nh tr·ªìng h·ªì ti√™u xen c√† ph√™?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:33<00:05,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='ƒê√¢u l√† n∆°i c√≥ ngu·ªìn g·ªëc c·ªßa m√≠a?\\n')]\n",
      "[Document(page_content='ƒê√¢u l√† n∆°i c√≥ ngu·ªìn g·ªëc c·ªßa m√≠a?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:34<00:03,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='T√™n khoa h·ªçc c·ªßa c√¢y ƒë·ªßng ƒë·ªânh l√† g√¨?\\n')]\n",
      "[Document(page_content='T√™n khoa h·ªçc c·ªßa c√¢y ƒë·ªßng ƒë·ªânh l√† g√¨?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:35<00:02,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan chi?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan chi?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:36<00:01,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Th·ªët n·ªët c√≥ ph√¢n b·ªë ·ªü ƒë√¢u?\\n')]\n",
      "[Document(page_content='Th·ªët n·ªët c√≥ ph√¢n b·ªë ·ªü ƒë√¢u?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:38<00:00,  1.32s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√¥ng d·ª•ng c·ªßa c√¢y nha ƒëam ƒë·ªëi v·ªõi s·ª©c kh·ªèe?\\n')]\n",
      "[Document(page_content='C√¥ng d·ª•ng c·ªßa c√¢y nha ƒëam ƒë·ªëi v·ªõi s·ª©c kh·ªèe?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 1/29 [00:01<00:44,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ th·ª±c hi·ªán k·ªπ thu·∫≠t c·∫Øt t·ªâa v√† t·∫°o t√°n cho c√¢y c√† ph√™?\\n')]\n",
      "[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ th·ª±c hi·ªán k·ªπ thu·∫≠t c·∫Øt t·ªâa v√† t·∫°o t√°n cho c√¢y c√† ph√™?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 2/29 [00:02<00:38,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ c√† ph√™ ra hoa ƒë·ªìng lo·∫°t?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ c√† ph√™ ra hoa ƒë·ªìng lo·∫°t?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 3/29 [00:04<00:37,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Chu·ªëi r·∫Ω qu·∫°t c√≥ ph·∫£i l√† lo·∫°i chu·ªëi?\\n')]\n",
      "[Document(page_content='Chu·ªëi r·∫Ω qu·∫°t c√≥ ph·∫£i l√† lo·∫°i chu·ªëi?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 4/29 [00:05<00:35,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°ch phun lo·∫°i thu·ªëc l∆∞u d·∫´n ·ªü giai ƒëo·∫°n m·ªõi nh√∫ ƒë·ªçt non?\\n')]\n",
      "[Document(page_content='C√°ch phun lo·∫°i thu·ªëc l∆∞u d·∫´n ·ªü giai ƒëo·∫°n m·ªõi nh√∫ ƒë·ªçt non?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 5/29 [00:06<00:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ kh·∫Øc ph·ª•c n·ª©t tr√°i tr√™n c√¢y tr·ªìng?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ kh·∫Øc ph·ª•c n·ª©t tr√°i tr√™n c√¢y tr·ªìng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 6/29 [00:08<00:30,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='T·∫°i sao 2N√¥ng h·ªó tr·ª£ truy xu·∫•t ngu·ªìn g·ªëc s·∫£n ph·∫©m?\\n')]\n",
      "[Document(page_content='T·∫°i sao 2N√¥ng h·ªó tr·ª£ truy xu·∫•t ngu·ªìn g·ªëc s·∫£n ph·∫©m?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 7/29 [00:09<00:28,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ph√≤ng tr·ª´ c√¥n tr√πng v√† n·∫•m b·ªánh cho c√¢y s·∫ßu ri√™ng?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ph√≤ng tr·ª´ c√¥n tr√πng v√† n·∫•m b·ªánh cho c√¢y s·∫ßu ri√™ng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 8/29 [00:10<00:25,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ b√≥n ƒë·∫°m cho l√∫a?\\n')]\n",
      "[Document(page_content='Khi n√†o l√† th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ b√≥n ƒë·∫°m cho l√∫a?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà       | 9/29 [00:11<00:24,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ tr·ªìng v√† chƒÉm s√≥c c√¢y chu ƒëinh lan?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ tr·ªìng v√† chƒÉm s√≥c c√¢y chu ƒëinh lan?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 10/29 [00:13<00:24,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°ch kh·∫Øc ph·ª•c m√≠t x∆° ƒëen?\\n')]\n",
      "[Document(page_content='C√°ch kh·∫Øc ph·ª•c m√≠t x∆° ƒëen?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 11/29 [00:14<00:23,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°c bi·ªán ph√°p ph√≤ng tr·ª´ b·ªánh kh√¥ c√†nh kh√¥ qu·∫£ cho c√¢y c√† ph√™?\\n')]\n",
      "[Document(page_content='C√°c bi·ªán ph√°p ph√≤ng tr·ª´ b·ªánh kh√¥ c√†nh kh√¥ qu·∫£ cho c√¢y c√† ph√™?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 12/29 [00:15<00:21,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Th·ªßy t√πng l√† lo√†i th·ª±c v·∫≠t n√†o?\\n')]\n",
      "[Document(page_content='Th·ªßy t√πng l√† lo√†i th·ª±c v·∫≠t n√†o?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 13/29 [00:16<00:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi n√†o Tre M·∫°nh T√¥ng ƒë∆∞·ª£c m√¥ t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n?\\n')]\n",
      "[Document(page_content='Khi n√†o Tre M·∫°nh T√¥ng ƒë∆∞·ª£c m√¥ t·∫£ khoa h·ªçc ƒë·∫ßu ti√™n?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 14/29 [00:18<00:19,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan Vanda?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan Vanda?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 15/29 [00:19<00:19,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='S·∫£n l∆∞·ª£ng l√∫a thu ho·∫°ch nƒÉm 2022 c·ªßa Vi·ªát Nam ∆∞·ªõc ƒë·∫°t bao nhi√™u t·∫•n?\\n')]\n",
      "[Document(page_content='S·∫£n l∆∞·ª£ng l√∫a thu ho·∫°ch nƒÉm 2022 c·ªßa Vi·ªát Nam ∆∞·ªõc ƒë·∫°t bao nhi√™u t·∫•n?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 16/29 [00:21<00:17,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Lo√†i th·ª±c v·∫≠t thu·ªôc chi S·∫ßu ri√™ng n√†o l√† lo√†i ph·ªï bi·∫øn nh·∫•t?\\n')]\n",
      "[Document(page_content='Lo√†i th·ª±c v·∫≠t thu·ªôc chi S·∫ßu ri√™ng n√†o l√† lo√†i ph·ªï bi·∫øn nh·∫•t?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 17/29 [00:22<00:16,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='T·∫°i sao c·∫ßn lo·∫°i b·ªè c√°c c√†nh kh√¥ng mang tr√°i, c√†nh gi√† y·∫øu, c√†nh b·ªã nhi·ªÖm b·ªánh?\\n')]\n",
      "[Document(page_content='T·∫°i sao c·∫ßn lo·∫°i b·ªè c√°c c√†nh kh√¥ng mang tr√°i, c√†nh gi√† y·∫øu, c√†nh b·ªã nhi·ªÖm b·ªánh?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 18/29 [00:23<00:14,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?\\n')]\n",
      "[Document(page_content='C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 19/29 [00:25<00:13,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Sau khi thu ho·∫°ch s·∫ßu ri√™ng, c·∫ßn l√†m g√¨ ƒë·ªÉ chƒÉm s√≥c c√¢y?\\n')]\n",
      "[Document(page_content='Sau khi thu ho·∫°ch s·∫ßu ri√™ng, c·∫ßn l√†m g√¨ ƒë·ªÉ chƒÉm s√≥c c√¢y?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 20/29 [00:26<00:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='ƒê√¢u l√† t√™n g·ªçi ti·∫øng Vi·ªát c·ªßa lo√†i d·ª´a n∆∞·ªõc?\\n')]\n",
      "[Document(page_content='ƒê√¢u l√† t√™n g·ªçi ti·∫øng Vi·ªát c·ªßa lo√†i d·ª´a n∆∞·ªõc?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 21/29 [00:28<00:11,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ƒëi·ªÅu tr·ªã s√¢u ƒÉn l√° ho·∫∑c b√°m v√†o c√¢y hoa Lan Cattleya?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ ƒëi·ªÅu tr·ªã s√¢u ƒÉn l√° ho·∫∑c b√°m v√†o c√¢y hoa Lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 22/29 [00:29<00:09,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ b·ªï sung ch·∫•t h·ªØu c∆° cho ƒë·∫•t sau thu ho·∫°ch ƒë·ªëi v·ªõi v∆∞·ªùn c√¢y ƒÉn tr√°i?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ b·ªï sung ch·∫•t h·ªØu c∆° cho ƒë·∫•t sau thu ho·∫°ch ƒë·ªëi v·ªõi v∆∞·ªùn c√¢y ƒÉn tr√°i?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/29 [00:30<00:08,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c hoa lan Cattleya?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c hoa lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 24/29 [00:32<00:06,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi cho m·ªôt c√¢y h·ªì ti√™u trong m√¥ h√¨nh tr·ªìng h·ªì ti√™u xen c√† ph√™?\\n')]\n",
      "[Document(page_content='L∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi cho m·ªôt c√¢y h·ªì ti√™u trong m√¥ h√¨nh tr·ªìng h·ªì ti√™u xen c√† ph√™?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [00:33<00:05,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='ƒê√¢u l√† n∆°i c√≥ ngu·ªìn g·ªëc c·ªßa m√≠a?\\n')]\n",
      "[Document(page_content='ƒê√¢u l√† n∆°i c√≥ ngu·ªìn g·ªëc c·ªßa m√≠a?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 26/29 [00:35<00:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='T√™n khoa h·ªçc c·ªßa c√¢y ƒë·ªßng ƒë·ªânh l√† g√¨?\\n')]\n",
      "[Document(page_content='T√™n khoa h·ªçc c·ªßa c√¢y ƒë·ªßng ƒë·ªânh l√† g√¨?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [00:36<00:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan chi?\\n')]\n",
      "[Document(page_content='L√†m th·∫ø n√†o ƒë·ªÉ chƒÉm s√≥c ƒë√∫ng c√¢y lan chi?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 28/29 [00:38<00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Th·ªët n·ªët c√≥ ph√¢n b·ªë ·ªü ƒë√¢u?\\n')]\n",
      "[Document(page_content='Th·ªët n·ªët c√≥ ph√¢n b·ªë ·ªü ƒë√¢u?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:39<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "# time.sleep(5*60*60)\n",
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset_2.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions.copy()\n",
    "ds['retrieved_docs_source'] = None\n",
    "ds['relevant_docs_text'] = None\n",
    "main_engine = Session(get_engine_test())\n",
    "VECTOR_EMBEDDINGS_DIM = 768\n",
    "dict_strategy = {1:DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                    2:DISTANCE_STRATEGY.COSINE,\n",
    "                    # 3:DISTANCE_STRATEGY.MAX_INNER_PRODUCT\n",
    "                    }\n",
    "for strategy_index in dict_strategy:\n",
    "    name_strategy = dict_strategy[strategy_index].value\n",
    "    model_name = 'vinai/phobert-base-v2' + \"_vncore_segment\"\n",
    "\n",
    "    # model_name = 'vinai/bartpho-word' + \"_vncore_segment\"\n",
    "    model_name = model_name.replace(\"/\",\"__\")\n",
    "    folder_name = \"e:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluate/embeddings_models/\"  + model_name + \"/\"\n",
    "    os.makedirs(folder_name , exist_ok=True)\n",
    "    chunk_size = 256\n",
    "    chunk_overlap = 20 \n",
    "    ds = generated_questions.copy()\n",
    "    ds['retrieved_docs_source'] = None\n",
    "    ds['relevant_docs_text'] = None\n",
    "    file_name_excel = model_name + \"_\" + str(chunk_size) + \"_\" + str(chunk_overlap) + \"_\" + name_strategy + '.xlsx'\n",
    "    file_name_csv = model_name + \"_\" + str(chunk_size) + \"_\" + str(chunk_overlap) + \"_\" + name_strategy + '.csv'\n",
    "\n",
    "    for index in tqdm(range(len(ds))):\n",
    "        example = ds.iloc[index]\n",
    "        question = example[\"question\"]\n",
    "        list_doc, list_relevant_docs = test_embedding_performance(question, dict_strategy[1])\n",
    "        ds.at[index,'retrieved_docs_source'] = [each.display_name for each in list_doc]\n",
    "        ds.at[index,'relevant_docs_text'] = list_relevant_docs\n",
    "    ds.to_csv(folder_name + file_name_csv,sep=\"|\", header=True, index=False)\n",
    "    ds.to_excel(folder_name + file_name_excel)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BARTBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import py_vncorenlp\n",
    "\n",
    "# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrsegmenter.word_segment(\"ƒê·∫°i h·ªçc b√°ch khoa H·ªì Ch√≠ Minh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'E:/TrungPhanADVN/Code/LangChain_RAG/scripts/bartpho-word/'\n",
    "bartpho = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_bartpho = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document= 'C√°ch chƒÉm s√≥c l√∫a giai ƒëo·∫°n ƒë√≤ng tr·ªï gi√∫p tƒÉng nƒÉng su·∫•t hi·ªáu qu·∫£'\n",
    "question = 'C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_ques = [rdrsegmenter.word_segment(doc) for doc in [question]]\n",
    "\n",
    "total_segment_ques = []\n",
    "for list_segment_ques in output_segment_ques:\n",
    "    segment_doc = \"\"\n",
    "    for each in list_segment_ques:\n",
    "        segment_doc += each + \" \"\n",
    "    total_segment_ques.append(segment_doc.strip()) \n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding__ques = tokenizer_bartpho(total_segment_ques,padding='max_length',return_tensors='pt', truncation=True, max_length=1024)\n",
    "\n",
    "input_ids_ques = encoding_ques['input_ids']\n",
    "attention_mask_ques = encoding_ques['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_ques = bartpho(input_ids_ques, attention_mask=attention_mask_ques)\n",
    "\n",
    "last_hidden_state_ques, _ = features_ques[0], features_ques[1]\n",
    "\n",
    "embeddings_ques = last_hidden_state_ques.mean(dim=1)\n",
    "print(question)\n",
    "print(total_segment_ques)\n",
    "print(last_hidden_state_ques.shape)\n",
    "print(embeddings_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in [test_document]]\n",
    "\n",
    "total_segment_doc = []\n",
    "for list_segment_doc in output_segment_doc:\n",
    "    segment_doc = \"\"\n",
    "    for each in list_segment_doc:\n",
    "        segment_doc += each + \" \"\n",
    "    total_segment_doc.append(segment_doc.strip()) \n",
    "\n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding_doc = tokenizer_bartpho(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=1024)\n",
    "\n",
    "input_ids_doc = encoding_doc['input_ids']\n",
    "attention_mask_doc = encoding_doc['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_doc = bartpho(input_ids_doc, attention_mask=attention_mask_doc)\n",
    "\n",
    "last_hidden_state_doc, _ = features_doc[0], features_doc[1]\n",
    "\n",
    "embeddings_doc = last_hidden_state_doc.mean(dim=1)\n",
    "print(test_document)\n",
    "print(total_segment_doc)\n",
    "print(last_hidden_state_doc.shape)\n",
    "print(embeddings_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_doc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(embeddings_doc[0],embeddings_ques[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(embeddings[0],embeddings_2[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.739711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(embeddings[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_documents = ['Lo√†i th·ª±c v·∫≠t thu·ªôc chi S·∫ßu ri√™ng n√†o l√† lo√†i ph·ªï bi·∫øn nh·∫•t?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHOBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "# tokenizer_phobert = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = 'C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?\\r\\n‚Äì ƒêi·ªÅu khi·ªÉn ƒë·ªçt non ra ƒë·ªìng lo·∫°t\\r\\n\\r\\n‚Äì Phun lo·∫°i thu·ªëc l∆∞u d·∫´n ho·∫∑c qu√©t thu·ªëc l∆∞u d·∫´n ·ªü giai ƒëo·∫°n m·ªõi nh√∫ ƒë·ªçt non (b·∫±ng h·∫°t g·∫°o)'\n",
    "question = 'C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr_doc_ques, embed_ques = get_embeddings_photbert(question)\n",
    "arr_doc, embed_doc = get_embeddings_photbert(test_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_ques[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(embed_ques[0],embed_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document= 'C√°ch l√†m cho hoa c√† ph√™ ra ƒë·ªìng lo·∫°t?\\r\\nMu·ªën c√† ph√™ ra hoa ƒë·ªìng lo·∫°t c·∫ßn x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi n∆∞·ªõc cho ph√π h·ª£p. Vi·ªác x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi, l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi, ph∆∞∆°ng ph√°p t∆∞·ªõi t√πy thu·ªôc v√†o ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt, lo·∫°i ƒë·∫•t, t√¨nh tr·∫°ng sinh tr∆∞·ªüng c·ªßa c√¢y. Sau th·ªùi gian kh√¥ h·∫°n, khi th·∫•y n·ª• hoa c√≥ d·∫°ng m·ªè s·∫ª xu·∫•t hi·ªán ƒë·∫ßy ƒë·ªß ·ªü ƒë·ªët ngo√†i c√πng c·ªßa c√°c c√†nh th√¨ ti·∫øn h√†nh t∆∞·ªõi n∆∞·ªõc cho c√† ph√™. Vi·ªác t∆∞·ªõi n∆∞·ªõc ƒë√∫ng th·ªùi ƒëi·ªÉm l·∫ßn ƒë·∫ßu (ƒë·ª£t 1) v√† ƒë·ªß l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi s·∫Ω quy·∫øt ƒë·ªãnh ƒë·∫øn vi·ªác ra hoa ƒë·ªìng lo·∫°t.'\n",
    "# test_document = \"C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?\\r\\n‚Äì ƒêi·ªÅu khi·ªÉn ƒë·ªçt non ra ƒë·ªìng lo·∫°t\\r\\n\\r\\n‚Äì Phun lo·∫°i thu·ªëc l∆∞u d·∫´n ho·∫∑c qu√©t thu·ªëc l∆∞u d·∫´n ·ªü giai ƒëo·∫°n m·ªõi nh√∫ ƒë·ªçt non (b·∫±ng h·∫°t g·∫°o)\"\n",
    "question = 'C√°ch ph√≤ng tr·ªã s√¢u v·∫Ω b√πa tr√™n c√¢y cam?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_ques = [rdrsegmenter.word_segment(doc) for doc in [question]]\n",
    "\n",
    "\n",
    "\n",
    "encoding_ques = tokenizer_phobert(output_segment_ques[0],padding='max_length',return_tensors='pt', truncation=True, max_length=256)\n",
    "input_ids_ques = encoding_ques['input_ids']\n",
    "attention_mask_ques = encoding_ques['attention_mask'] \n",
    "with torch.no_grad():\n",
    "    features_ques = phobert(input_ids_ques, attention_mask=attention_mask_ques)\n",
    "\n",
    "last_hidden_state_ques, _ = features_ques[0], features_ques[1]\n",
    "\n",
    "embeddings_ques = last_hidden_state_ques.mean(dim=1).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_ques.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in [test_document]]\n",
    "\n",
    "# total_segment_doc = []\n",
    "# for list_segment_doc in output_segment_doc:\n",
    "#     segment_doc = \"\"\n",
    "#     for each in list_segment_doc:\n",
    "#         segment_doc += each + \"\"\n",
    "#     total_segment_doc.append(segment_doc.strip()) \n",
    "output_segment_doc = [each for each in output_segment_doc]\n",
    "# print(total_segment_doc)\n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding_doc = tokenizer_phobert(output_segment_doc[0],padding='max_length',return_tensors='pt', truncation=True, max_length=256)\n",
    "\n",
    "input_ids_doc = encoding_doc['input_ids']\n",
    "attention_mask_doc = encoding_doc['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_doc = phobert(input_ids_doc, attention_mask=attention_mask_doc)\n",
    "\n",
    "last_hidden_state_doc, _ = features_doc[0], features_doc[1]\n",
    "\n",
    "embeddings_doc = last_hidden_state_doc.mean(dim=1).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_doc.mean(dim=1).mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_doc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document_1 = 'C√°ch l√†m cho hoa c√† ph√™ ra ƒë·ªìng lo·∫°t?\\r\\nMu·ªën c√† ph√™ ra hoa ƒë·ªìng lo·∫°t c·∫ßn x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi n∆∞·ªõc cho ph√π h·ª£p. Vi·ªác x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi, l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi, ph∆∞∆°ng ph√°p t∆∞·ªõi t√πy thu·ªôc v√†o ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt, lo·∫°i ƒë·∫•t, t√¨nh tr·∫°ng sinh tr∆∞·ªüng c·ªßa c√¢y. Sau th·ªùi gian kh√¥ h·∫°n, khi th·∫•y n·ª• hoa c√≥ d·∫°ng m·ªè s·∫ª xu·∫•t hi·ªán ƒë·∫ßy ƒë·ªß ·ªü ƒë·ªët ngo√†i c√πng c·ªßa c√°c c√†nh th√¨ ti·∫øn h√†nh t∆∞·ªõi n∆∞·ªõc cho c√† ph√™. Vi·ªác t∆∞·ªõi n∆∞·ªõc ƒë√∫ng th·ªùi ƒëi·ªÉm l·∫ßn ƒë·∫ßu (ƒë·ª£t 1) v√† ƒë·ªß l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi s·∫Ω quy·∫øt ƒë·ªãnh ƒë·∫øn vi·ªác ra hoa ƒë·ªìng lo·∫°t.'\n",
    "\n",
    "test_document_2 = 'C√°ch l√†m cho hoa c√† ph√™ ra ƒë·ªìng lo·∫°t?\\r\\nMu·ªën c√† ph√™ ra hoa ƒë·ªìng lo·∫°t c·∫ßn x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi n∆∞·ªõc cho ph√π h·ª£p. Vi·ªác x√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm t∆∞·ªõi, l∆∞·ª£ng n∆∞·ªõc t∆∞·ªõi, ph∆∞∆°ng ph√°p t∆∞·ªõi t√πy thu·ªôc v√†o ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt, lo·∫°i ƒë·∫•t, t√¨nh tr·∫°ng sinh tr∆∞·ªüng c·ªßa c√¢y.'\n",
    "\n",
    "list_doc = [test_document_1, test_document_2]\n",
    "output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in list_doc]\n",
    "\n",
    "total_segment_doc = []\n",
    "for list_segment_doc in output_segment_doc:\n",
    "    segment_doc = \"\"\n",
    "    for each in list_segment_doc:\n",
    "        segment_doc += each + \" \"\n",
    "    total_segment_doc.append(segment_doc.strip()) \n",
    "output_segment_doc = [each for each in output_segment_doc]\n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding_doc = tokenizer_phobert(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=256)\n",
    "\n",
    "input_ids_doc = encoding_doc['input_ids']\n",
    "attention_mask_doc = encoding_doc['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_doc = phobert(input_ids_doc, attention_mask=attention_mask_doc)\n",
    "\n",
    "last_hidden_state_doc, _ = features_doc[0], features_doc[1]\n",
    "\n",
    "embeddings_doc = last_hidden_state_doc.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = \"\"\n",
    "for segment in rdrsegmenter.word_segment(test_document_1):\n",
    "    total_text += segment + \" \"\n",
    "total_text = total_text.strip()\n",
    "len(tokenizer_phobert.encode(total_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_segment_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_doc.mean(dim=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ques.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(last_hidden_state_doc.mean(dim=1)[0],embeddings_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST GENERATED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import py_vncorenlp\n",
    "\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')\n",
    "\n",
    "\n",
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# tokenizer_generate = AutoTokenizer.from_pretrained(model_id)\n",
    "# model_generate = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer_generate.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model_generate.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORE WITH GENERATED ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT =\"\"\"###M√¥ t·∫£ c√¥ng vi·ªác:\n",
    "M·ªôt h∆∞·ªõng d·∫´n (c√≥ th·ªÉ bao g·ªìm ƒê·∫ßu v√†o b√™n trong n√≥), m·ªôt ph·∫£n h·ªìi ƒë·ªÉ ƒë√°nh gi√°, m·ªôt c√¢u tr·∫£ l·ªùi tham kh·∫£o ƒë·∫°t ƒëi·ªÉm 5 v√† thang ƒëi·ªÉm ƒë·∫°i di·ªán cho ti√™u ch√≠ ƒë√°nh gi√° s·∫Ω ƒë∆∞·ª£c ƒë∆∞a ra.\n",
    "1. Vi·∫øt ph·∫£n h·ªìi chi ti·∫øt ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi d·ª±a tr√™n thang ƒëi·ªÉm cho s·∫µn, kh√¥ng ƒë√°nh gi√° chung chung.\n",
    "2. Sau khi vi·∫øt ph·∫£n h·ªìi, h√£y vi·∫øt ƒëi·ªÉm l√† s·ªë nguy√™n t·ª´ 1 ƒë·∫øn 5. B·∫°n n√™n tham kh·∫£o b·∫£ng ƒë√°nh gi√°.\n",
    "3. ƒê·ªãnh d·∫°ng ƒë·∫ßu ra s·∫Ω nh∆∞ sau: \\\"Ph·∫£n h·ªìi: {{vi·∫øt ph·∫£n h·ªìi cho ti√™u ch√≠}} [K·∫æT QU·∫¢] {{m·ªôt s·ªë nguy√™n t·ª´ 1 ƒë·∫øn 5}}\\\"\n",
    "4. Vui l√≤ng kh√¥ng ƒë∆∞a ra b·∫•t k·ª≥ l·ªùi m·ªü ƒë·∫ßu, k·∫øt th√∫c v√† gi·∫£i th√≠ch n√†o kh√°c. H√£y ch·∫Øc ch·∫Øn bao g·ªìm [K·∫æT QU·∫¢] trong ƒë·∫ßu ra c·ªßa b·∫°n.\n",
    "\n",
    "###H∆∞·ªõng d·∫´n ƒë√°nh gi√°:\n",
    "{instruction}\n",
    "\n",
    "###Ph·∫£n h·ªìi ƒë·ªÉ ƒë√°nh gi√°:\n",
    "{response}\n",
    "\n",
    "###C√¢u tr·∫£ l·ªùi tham kh·∫£o (ƒêi·ªÉm 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Ti√™u ch√≠ ch·∫•m ƒëi·ªÉm:\n",
    "[C√¢u tr·∫£ l·ªùi c√≥ ƒë√∫ng, ch√≠nh x√°c v√† th·ª±c t·∫ø d·ª±a tr√™n c√¢u tr·∫£ l·ªùi tham kh·∫£o kh√¥ng?]\n",
    "ƒêi·ªÉm 1: C√¢u tr·∫£ l·ªùi ho√†n to√†n kh√¥ng ch√≠nh x√°c, kh√¥ng ch√≠nh x√°c v√†/ho·∫∑c kh√¥ng th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 2: C√¢u tr·∫£ l·ªùi ph·∫ßn l·ªõn l√† kh√¥ng ch√≠nh x√°c, kh√¥ng ch√≠nh x√°c v√†/ho·∫∑c kh√¥ng th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 3: C√¢u tr·∫£ l·ªùi c√≥ ph·∫ßn ƒë√∫ng, ch√≠nh x√°c v√†/ho·∫∑c th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 4: C√¢u tr·∫£ l·ªùi h·∫ßu h·∫øt ƒë·ªÅu ƒë√∫ng, ch√≠nh x√°c v√† th·ª±c t·∫ø.\n",
    "ƒêi·ªÉm 5: C√¢u tr·∫£ l·ªùi ho√†n to√†n ƒë√∫ng, ch√≠nh x√°c v√† th·ª±c t·∫ø.\n",
    "\n",
    "###Nh·∫≠n x√©t:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "# 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "# 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "# 3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "# 4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Is the response correct, accurate, and factual based on the reference answer?]\n",
    "# Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "# Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "# Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "# Score 4: The response is mostly correct, accurate, and factual.\n",
    "# Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"B·∫°n l√† m·ªôt m√¥ h√¨nh ƒë√°nh gi√° ng√¥n ng·ªØ c√¥ng b·∫±ng\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "# def evaluate_answers(\n",
    "#     answer_path: str,\n",
    "#     eval_chat_model: BaseChatModel,\n",
    "#     evaluator_name: str,\n",
    "#     evaluation_prompt_template: ChatPromptTemplate,\n",
    "# ) -> None:\n",
    "#     \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "#     answers = []\n",
    "# if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "#     answers = json.load(open(answer_path, \"r\"))\n",
    "answers= outputs\n",
    "evaluate_output = []\n",
    "for experiment in tqdm(answers):\n",
    "    if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "        continue\n",
    "\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        instruction=experiment[\"question\"],\n",
    "        response=experiment[\"generated_answer\"],\n",
    "        reference_answer=experiment[\"true_answer\"],\n",
    "    )\n",
    "    try:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    except:\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = None\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = None\n",
    "        continue\n",
    "    feedback, score = [item.strip() for item in eval_result.content.split(\"[K·∫æT QU·∫¢]\")]\n",
    "    experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "    experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "    # evaluate_output.append()\n",
    "    # with open(answer_path, \"w\") as f:\n",
    "    #     json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(answers,open('result_evaluate_1.json','w',encoding='utf-8'), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(json.load(open('result_evaluate_1.json', \"r\")))\n",
    "result = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] ) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result[\"eval_score_GPT4\"].mean()\n",
    "# average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import datasets\n",
    "import pandas as pd\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "# fig.update_layout(w\n",
    "#     width=1000,\n",
    "#     height=600,\n",
    "#     barmode=\"group\",\n",
    "#     yaxis_range=[0, 100],\n",
    "#     title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "#     xaxis_title=\"RAG settings\",\n",
    "#     font=dict(size=15),\n",
    "# )\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 2: https://freedium.cfd/https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-rag-applications-with-ragas-81d67b0ee31a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
