{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE EVALUATE DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 1: https://huggingface.co/learn/cookbook/rag_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test = [\n",
    "\"2nong_introduction.md\",\n",
    "\"category_trees_Cà_phê.md\",\n",
    "\"category_trees_Chuối_rẽ_quạt.md\",\n",
    "\"category_trees_Dừa.md\",\n",
    "\"category_trees_Dừa_nước__dừa_lá.md\",\n",
    "\"category_trees_Đủng_đỉnh.md\",\n",
    "\"category_trees_Hoa_ly.md\",\n",
    "\"category_trees_Hồ_tiêu.md\",\n",
    "\"category_trees_Lan_Cattleya.md\",\n",
    "\"category_trees_Lan_Chi.md\",\n",
    "\"category_trees_Lan_chu_đính.md\",\n",
    "\"category_trees_Lan_dạ_hương.md\",\n",
    "\"category_trees_Lan_Vanda.md\",\n",
    "\"category_trees_Lay_ơn.md\",\n",
    "\"category_trees_Lẻ_bạn__sò_huyết__bang_hoa.md\",\n",
    "\"category_trees_Lô_hội__nha_đam.md\",\n",
    "\"category_trees_Lúa.md\",\n",
    "\"category_trees_Lục_bình.md\",\n",
    "\"category_trees_Mía.md\",\n",
    "\"category_trees_Phong_Lộc_Hoa.md\",\n",
    "\"category_trees_Sầu_riêng.md\",\n",
    "\"category_trees_Sen_đá.md\",\n",
    "\"category_trees_Thiên_tuế.md\",\n",
    "\"category_trees_Thốt_nốt.md\",\n",
    "\"category_trees_Thủy_trúc__lác_dù.md\",\n",
    "\"category_trees_Thủy_tùng.md\",\n",
    "\"category_trees_Trắc_bá_diệp.md\",\n",
    "\"category_trees_Tre_mạnh_tông.md\",\n",
    "\"knowledge_handbooks_3_bước_cải_tạo_đất_sau_thu_hoạch_đối_với_vườn_cây_ăn_trái.md\",\n",
    "\"knowledge_handbooks_Bón_đạm_cho_lúa_vào_thời_kỳ_nào_là_tốt_nhất.md\",\n",
    "\"knowledge_handbooks_Bón_vôi_đúng_quy_trình_cho_vườn_cây_ăn_trái.md\",\n",
    "\"knowledge_handbooks_Cách_bón_lót_cho_cà_phê_trồng_mới.md\",\n",
    "\"knowledge_handbooks_Cách_bón_phân_chuồng_cho_rau_màu_hiệu_quả_nhất.md\",\n",
    "\"knowledge_handbooks_Cách_chăm_sóc_lúa_giai_đoạn_đòng_trổ_giúp_tăng_năng_suất_hiệu_quả.md\",\n",
    "\"knowledge_handbooks_Cách_khắc_phục_bưởi_da_xanh_bị_vàng_đọt.md\",\n",
    "\"knowledge_handbooks_Cách_khắc_phục_hiện_tượng_nứt_trái_trên_cây_trồng.md\",\n",
    "\"knowledge_handbooks_Cách_khắc_phục_mít_xơ_đen.md\",\n",
    "\"knowledge_handbooks_Cách_làm_cho_hoa_cà_phê_ra_đồng_loạt.md\",\n",
    "\"knowledge_handbooks_Cách_phòng_trị_sâu_vẽ_bùa_trên_cây_cam.md\",\n",
    "\"knowledge_handbooks_Cách_phòng_trừ_bệnh_khô_cành_khô_quả_gây_hại_cây_cà_phê.md\",\n",
    "\"knowledge_handbooks_Cách_trị_sâu_vẽ_bùa_trên_bưởi.md\",\n",
    "\"knowledge_handbooks_Cách_tưới_nước_cho_mô_hình_trồng_hồ_tiêu_xen_cà_phê.md\",\n",
    "\"knowledge_handbooks_Cam_sành_ra_bông__bị_mưa_nhiều_cần_làm_gì.md\",\n",
    "\"knowledge_handbooks_Cần_làm_gì_sau_khi_thu_hoạch_sầu_riêng.md\",\n",
    "\"knowledge_handbooks_Cây_cà_phê_già_cỗi_thì_phải_làm_thế_nào.md\",\n",
    "\"knowledge_handbooks_Có_cần_bón_phân_hữu_cơ_cho_đất_phèn_không.md\",\n",
    "\"knowledge_handbooks_Dứt_điểm_rệp_sáp__rầy_trắng_ở_phần_rễ.md\",\n",
    "\"knowledge_handbooks_Giống_cà_phê_vối_nào_chất_lượng_tốt_nhất_hiện_nay.md\",\n",
    "\"knowledge_handbooks_Giữ_ẩm_cho_đất_trong_mùa_khô_như_thế_nào.md\",\n",
    "\"knowledge_handbooks_Kích_thước_phẳng_của_bầu_ươm_cà_phê_ra_sao.md\",\n",
    "\"knowledge_handbooks_Kinh_nghiệm_chăm_sóc_sầu_riêng_giai_đoạn_nuôi_trái_non_hiệu_quả.md\",\n",
    "\"knowledge_handbooks_Kỹ_thuật_cắt_tỉa_cành_và_tạo_tán_cho_cà_phê.md\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "list_total_doc = os.listdir(folder_doc)\n",
    "for each_doc in list_total_doc:\n",
    "    if each_doc not in documents_test:\n",
    "        os.remove(folder_doc + each_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "documents_content = []\n",
    "for name_doc in documents_test:\n",
    "    with open(folder_doc + name_doc, \"r\") as f:\n",
    "        documents_content.append((name_doc,f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs = [LangchainDocument(page_content=doc[1], metadata={\"source\": doc[0]}) for doc in tqdm(documents_content)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# repo_id = \"noah-ai/mt5-base-question-generation-vi\"\n",
    "# repo_id= \"NlpHUST/gpt2-vietnamese\"\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token='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',\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={ \n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( docs_processed[5].page_content, \"\\n________________\", call_llm(llm_client, docs_processed[5].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_generation_prompt = \"\"\"\n",
    "# Your task is to write a factoid question and an answer given a context.\n",
    "# Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "# Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "# This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Output:::\n",
    "# Factoid question: (your factoid question)\n",
    "# Answer: (your answer to the factoid question)\n",
    "\n",
    "# Now here is the context.\n",
    "\n",
    "# Context: {context}\\n\n",
    "# Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\" Nhiệm vụ của bạn là viết một câu hỏi thực tế và một câu trả lời dựa trên ngữ cảnh.\n",
    "Câu hỏi thực tế của bạn phải được trả lời bằng một đoạn thông tin thực tế cụ thể, ngắn gọn từ ngữ cảnh.\n",
    "Câu hỏi thực tế của bạn phải được xây dựng theo phong cách giống như những câu hỏi mà người dùng có thể hỏi trong công cụ tìm kiếm.\n",
    "Điều này có nghĩa là câu hỏi thực tế của bạn KHÔNG PHẢI đề cập đến những thứ như \"theo đoạn văn\" hoặc \"ngữ cảnh\".\n",
    "Toàn bộ câu trả lời và câu hỏi phải được viết bằng tiếng Việt.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Kết quả:::\n",
    "Câu hỏi thực tế: (câu hỏi thực tế của bạn)\n",
    "Trả lời: (câu trả lời của bạn cho câu hỏi thực tế)\n",
    "\n",
    "Đây là bối cảnh.\n",
    "Bối cảnh: {context}\\n\n",
    "Kết quả:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 50  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    # print(output_QA_couple)\n",
    "    # print(\"_______________________________________\")\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Câu hỏi thực tế: \")[-1].split(\"Trả lời: \")[0]\n",
    "        answer = output_QA_couple.split(\"Trả lời: \")[-1]\n",
    "        print(question,answer )\n",
    "        # assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pd.DataFrame(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_groundedness_critique_prompt = \"\"\"\n",
    "# You will be given a context and a question.\n",
    "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here are the question and context.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Context: {context}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_relevance_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_standalone_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "Bạn sẽ được cung cấp một bối cảnh và một câu hỏi.\n",
    "Nhiệm vụ của bạn là đưa ra 'Điểm đánh giá' cho điểm mức độ một người có thể trả lời câu hỏi đã cho một cách rõ ràng với bối cảnh nhất định.\n",
    "Đưa ra câu trả lời của bạn theo thang điểm từ 1 đến 5, trong đó 1 có nghĩa là câu hỏi hoàn toàn không thể trả lời được trong bối cảnh và 5 có nghĩa là câu hỏi có thể trả lời rõ ràng và rõ ràng với bối cảnh.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Trả lời:::\n",
    "Đánh giá: (lý do đánh giá của bạn, dưới dạng văn bản)\n",
    "Điểm đánh giá: (Điểm đánh giá của bạn, tính bằng số từ 1 đến 5)\n",
    "\n",
    "Bạn PHẢI cung cấp các giá trị cho 'Đánh giá:' và 'Điểm đánh giá:' trong câu trả lời của mình.\n",
    "\n",
    "Bây giờ đây là câu hỏi và bối cảnh.\n",
    "\n",
    "Câu hỏi: {question}\\n\n",
    "Bối cảnh: {context}\\n\n",
    "Trả lời::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "Bạn sẽ được đưa ra một câu hỏi.\n",
    "Nhiệm vụ của bạn là cung cấp 'Điểm xếp hạng' thể hiện mức độ hữu ích của câu hỏi này đối với các nhà phát triển máy học đang xây dựng các ứng dụng NLP với hệ sinh thái Hugging Face.\n",
    "Đưa ra câu trả lời của bạn theo thang điểm từ 1 đến 5, trong đó 1 có nghĩa là câu hỏi không hữu ích chút nào và 5 có nghĩa là câu hỏi cực kỳ hữu ích.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Trả lời:::\n",
    "Đánh giá: (lý do đánh giá của bạn, dưới dạng văn bản)\n",
    "Điểm đánh giá: (Điểm đánh giá của bạn, tính bằng số từ 1 đến 5)\n",
    "\n",
    "Bạn PHẢI cung cấp các giá trị cho 'Đánh giá:' và 'Điểm đánh giá:' trong câu trả lời của mình.\n",
    "\n",
    "Bây giờ đây là câu hỏi.\n",
    "\n",
    "Câu hỏi: {question}\\n\n",
    "Trả lời::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "Bạn sẽ được đưa ra một câu hỏi.\n",
    "Nhiệm vụ của bạn là cung cấp 'Điểm xếp hạng' thể hiện mức độ độc lập của câu hỏi này với bối cảnh.\n",
    "Đưa ra câu trả lời của bạn theo thang điểm từ 1 đến 5, trong đó 1 có nghĩa là câu hỏi phụ thuộc vào thông tin bổ sung để hiểu. 5 có nghĩa là bản thân câu hỏi có ý nghĩa và có thể hiểu được.\n",
    "Ví dụ: nếu câu hỏi đề cập đến một cài đặt cụ thể, như 'trong bối cảnh' hoặc 'trong tài liệu' thì xếp hạng phải là 1.\n",
    "Các câu hỏi có thể chứa các danh từ hoặc từ viết tắt kỹ thuật khó hiểu như Gradio, Hub, Hugging Face và vẫn ở mức 5: nó chỉ đơn giản là phải rõ ràng đối với người vận hành có quyền truy cập vào tài liệu về nội dung câu hỏi.\n",
    "\n",
    "Ví dụ: \"Tổ chức Hugging Face năm 2023 có bao nhiêu người dùng?\" sẽ nhận được điểm 1, vì có sự đề cập ngầm đến một ngữ cảnh, do đó câu hỏi không độc lập với ngữ cảnh.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Trả lời:::\n",
    "Đánh giá: (lý do đánh giá của bạn, dưới dạng văn bản)\n",
    "Điểm đánh giá: (Điểm đánh giá của bạn, tính bằng số từ 1 đến 5)\n",
    "\n",
    "Bạn PHẢI cung cấp các giá trị cho 'Đánh giá:' và 'Điểm đánh giá:' trong câu trả lời của mình.\n",
    "\n",
    "Bây giờ đây là câu hỏi.\n",
    "\n",
    "Câu hỏi: {question}\\n\n",
    "Trả lời::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "# outputs_2 = outputs[1:4]\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                evaluation.split(\"Điểm đánh giá: \")[-1].strip(),\n",
    "                evaluation.split(\"Điểm đánh giá: \")[-2].split(\"Đánh giá: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # print(\"______________________________________________________\")\n",
    "        continue\n",
    "    # print(output)\n",
    "    # print(\"______________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 20)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "generated_questions_process = generated_questions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_score(score_string):\n",
    "    if score_string is np.nan:\n",
    "        return np.nan\n",
    "    return int(str(score_string)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process.iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process['groundedness_score'] = generated_questions_process['groundedness_score'].apply(process_score)\n",
    "generated_questions_process['relevance_score'] = generated_questions_process['relevance_score'].apply(process_score)\n",
    "generated_questions_process['standalone_score'] = generated_questions_process['standalone_score'].apply(process_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions_process[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions_process_2 = generated_questions_process.loc[\n",
    "    (generated_questions_process[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions_process[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions_process[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions_process_2[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions_process_2, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.to_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG PIPELINE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../app/api/')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import *\n",
    "from helpers import *\n",
    "from models import *\n",
    "from config import *\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "import logging\n",
    "\n",
    "openai.util.logging.getLogger().setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER=\"postgres\"\n",
    "POSTGRES_PASSWORD=\"postgres\"\n",
    "POSTGRES_DB=\"postgres\"\n",
    "PGVECTOR_ADD_INDEX=True\n",
    "\n",
    "# DB_HOST=localhost\n",
    "DB_HOST=\"localhost\"\n",
    "DB_PORT=5432\n",
    "# DB_PORT=5132\n",
    "DB_USER=\"api\"\n",
    "DB_NAME=\"api\"\n",
    "DB_PASSWORD=123\n",
    "VECTOR_EMBEDDINGS_COUNT = 1024\n",
    "OPENAI_API_KEY='sk-proj-SP7z7Y29wCjNbtRnoRoRT3BlbkFJM4oUr3l8Mv0X6vBdKqF7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SU_DSN = (\n",
    "    f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engine_test(dsn: str = SU_DSN):\n",
    "    return create_engine(dsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_random_agent():\n",
    "    return random.choice(AGENT_NAMES)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Get the count of tokens used\n",
    "# ----------------------------\n",
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def get_token_count(text: str):\n",
    "    if not text:\n",
    "        return 0\n",
    "\n",
    "    return OpenAI().get_num_tokens(text=text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(VECTOR_EMBEDDINGS_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Query embedding search for similar documents\n",
    "# --------------------------------------------\n",
    "def get_nodes_by_embedding_test(\n",
    "    embeddings: List[float],\n",
    "    k: int = LLM_MIN_NODE_LIMIT,\n",
    "    distance_strategy: Optional[DISTANCE_STRATEGY] = LLM_DEFAULT_DISTANCE_STRATEGY,\n",
    "    distance_threshold: Optional[float] = LLM_DISTANCE_THRESHOLD,\n",
    "    session: Optional[Session] = None,\n",
    ") -> List[Node]:\n",
    "    # Convert embeddings array into sql string\n",
    "    embeddings_str = str(embeddings)\n",
    "\n",
    "    if distance_strategy == DISTANCE_STRATEGY.EUCLIDEAN:\n",
    "        distance_fn = \"match_node_euclidean\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.COSINE:\n",
    "        distance_fn = \"match_node_cosine\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.MAX_INNER_PRODUCT:\n",
    "        distance_fn = \"match_node_max_inner_product\"\n",
    "    else:\n",
    "        raise Exception(f\"Invalid distance strategy {distance_strategy}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Lets do a similarity search\n",
    "    # ---------------------------\n",
    "    sql = f\"\"\"SELECT * FROM {distance_fn}(\n",
    "    '{embeddings_str}'::vector({VECTOR_EMBEDDINGS_COUNT}),\n",
    "    {float(distance_threshold)}::double precision,\n",
    "    {int(k)});\"\"\"\n",
    "    print(sql)\n",
    "    # logger.debug(f'🔍 Query: {sql}')\n",
    "\n",
    "    # Execute query, convert results to Node objects\n",
    "    if not session:\n",
    "        with Session(get_engine_test()) as session:\n",
    "            nodes = session.exec(text(sql)).all()\n",
    "    else:\n",
    "        nodes = session.exec(text(sql)).all()\n",
    "    return [Node.by_uuid(str(node[0])) for node in nodes] if nodes else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_test(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=LLM_CHUNK_SIZE,\n",
    "                    chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                    add_start_index=True,\n",
    "                    separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                )\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    # https://github.com/hwchase17/langchain/blob/d18b0caf0e00414e066c9903c8df72bb5bcf9998/langchain/embeddings/openai.py#L219\n",
    "    embed_func = OpenAIEmbeddings(openai_api_key='sk-proj-SP7z7Y29wCjNbtRnoRoRT3BlbkFJM4oUr3l8Mv0X6vBdKqF7')\n",
    "    # print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "    \n",
    "    # print(embed_func)\n",
    "    # print(arr_documents)\n",
    "    embeddings = embed_func.embed_documents(\n",
    "        texts=arr_documents, chunk_size=512\n",
    "    )\n",
    "    \n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "def get_embeddings_test_2(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=LLM_CHUNK_SIZE,\n",
    "                    chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                    add_start_index=True,\n",
    "                    separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                )\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    # https://github.com/hwchase17/langchain/blob/d18b0caf0e00414e066c9903c8df72bb5bcf9998/langchain/embeddings/openai.py#L219\n",
    "    # embed_func = OpenAIEmbeddings(openai_api_key='sk-proj-7Ueh52MTS1sPLmpN32UMT3BlbkFJBS2DTMyHkkJaNU44zAsl')\n",
    "    # print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "    \n",
    "    # print(embed_func)\n",
    "    # print(arr_documents)\n",
    "    \n",
    "    # embeddings = embed_func.embed_documents(\n",
    "        # texts=arr_documents, chunk_size=512\n",
    "    # )\n",
    "\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/text-embedding-ada-002')\n",
    "    embeddings = [tokenizer.encode(text=doc) for doc in arr_documents]\n",
    "    \n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_llm_response_test(\n",
    "    query_str: str,\n",
    "    model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO,\n",
    "    temperature: Optional[float] = LLM_DEFAULT_TEMPERATURE,\n",
    "    max_output_tokens: Optional[int] = LLM_MAX_OUTPUT_TOKENS,\n",
    "    prefix_messages: Optional[List[dict]] = None,\n",
    "):\n",
    "    # print(f\"111111111111---{query_str}\\n2222222222---{prefix_messages}\")\n",
    "\n",
    "    llm = OpenAI(\n",
    "        temperature=temperature,\n",
    "        model_name=model.model_name\n",
    "        if isinstance(model, LLM_MODELS)\n",
    "        else LLM_MODELS.GPT_35_TURBO.model_name,\n",
    "        max_tokens=max_output_tokens,\n",
    "        prefix_messages=prefix_messages,\n",
    "        request_timeout=10,\n",
    "    )\n",
    "    try:\n",
    "        result = llm(prompt=query_str)\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        logger.error(f\"🚨 LLM error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"LLM error: {e}\")\n",
    "    # logger.debug(f\"💬 LLM result: {str(result)}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Retrieve prompt template\n",
    "# ------------------------\n",
    "def get_prompt_template_2(\n",
    "    user_query: str = None,\n",
    "    context_str: str = None,\n",
    "    project: Optional[Project] = None,\n",
    "    organization: Optional[Organization] = None,\n",
    "    agent: str = None,\n",
    ") -> str:\n",
    "    agent = f\"{agent}, \" if agent else \"\"\n",
    "    user_query = user_query if user_query else \"\"\n",
    "    context_str = context_str if context_str else \"\"\n",
    "    organization = (\n",
    "        project.organization.display_name\n",
    "        if project\n",
    "        else organization.display_name\n",
    "        if organization\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    if not context_str or not user_query:\n",
    "        raise ValueError(\n",
    "            \"Missing required arguments context_str, user_query, organization, agent\"\n",
    "        )\n",
    "    system_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"[AGENT]:\n",
    "    Tôi sẽ trả lời các câu hỏi của [USER] chỉ bằng cách sử dụng  [DOCUMENT] và tuân theo [Quy tắc].\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context_str}\n",
    "\n",
    "    [QUY TẮC]:\n",
    "    Tôi sẽ chỉ trả lời các câu hỏi của người dùng bằng  [DOCUMENT] được cung cấp. Tôi sẽ tuân thủ các quy tắc sau:\n",
    "    - Tôi là nhân viên hỗ trợ khách hàng tốt nhất hiện nay\n",
    "    - Tôi sẽ trả lời toàn bộ nội dung trong [DOCUMENT]\n",
    "    - Tôi không bao giờ nói dối hay bịa ra những câu trả lời không được nêu rõ ràng trong [DOCUMENT]\n",
    "    - Nếu tôi không chắc chắn về câu trả lời hoặc câu trả lời không có rõ ràng trong [DOCUMENT], tôi sẽ nói: \"Tôi xin lỗi, tôi không biết phải trợ giúp điều đó như thế nào\".\n",
    "    - Tôi luôn giữ câu trả lời dài, phù hợp và súc tích.\n",
    "    - Tôi sẽ luôn phản hồi ở định dạng JSON bằng các khóa sau: \"message\" phản hồi của tôi cho người dùng.\n",
    "    \"\"\",\n",
    "            }\n",
    "        ]\n",
    "#     system_prompt = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": f\"\"\"[AGENT]:\n",
    "#  I will answer the [USER] questions using only the [DOCUMENT] and following the [RULES].\n",
    "\n",
    "# [DOCUMENT]:\n",
    "# {context_str}\n",
    "\n",
    "# [RULES]:\n",
    "# I will answer the user's questions using only the [DOCUMENT] provided. I will abide by the following rules:\n",
    "# - I am a kind and helpful human, the best customer support agent in existence\n",
    "# - I will answer all content  in [DOCUMENT]\n",
    "# - I never lie or invent answers not explicitly provided in [DOCUMENT]\n",
    "# - If I am unsure of the answer response or the answer is not explicitly contained in [DOCUMENT], I will say: \"I apologize, I'm not sure how to help with that\".\n",
    "# - I always keep my answers long, relevant and concise.\n",
    "# - I will always respond in JSON format with the following keys: \"message\" my response to the user, \"tags\" an array of short labels categorizing user input, \"is_escalate\" a boolean, returning false if I am unsure and true if I do have a relevant answer\n",
    "# \"\"\",\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "    return (system_prompt, f\"[USER]:\\n{user_query}\")\n",
    "# f\"[USER]:\\n{user_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "\n",
    "tokenizer_bartpho = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bartbert(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "\n",
    "    logger.debug(documents)\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=LLM_CHUNK_SIZE,\n",
    "                        chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                        add_start_index=True,\n",
    "                        separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                    )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    # https://github.com/hwchase17/langchain/blob/d18b0caf0e00414e066c9903c8df72bb5bcf9998/langchain/embeddings/openai.py#L219\n",
    "\n",
    "    \n",
    "    # output_segment = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    # output_segment = [each[0] for each in output_segment]\n",
    "\n",
    "    \n",
    "    token_output = torch.tensor(tokenizer_bartpho(arr_documents,padding='max_length', truncation=True, max_length=1024)['input_ids'])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = bartpho(token_output)\n",
    "\n",
    "    last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "    embeddings = torch.mean(last_hidden_state, dim=1)\n",
    "    \n",
    "    # embed_func = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    # # print(\"________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "    \n",
    "    # # print(embed_func)\n",
    "    # # print(arr_documents)\n",
    "    # embeddings = embed_func.embed_documents(\n",
    "    #     texts=arr_documents, chunk_size=512\n",
    "    # )\n",
    "\n",
    "\n",
    "    # tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/text-embedding-ada-002')\n",
    "    # embeddings = [tokenizer.encode(text=doc) for doc in arr_documents]\n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Làm thế nào để tạo điều kiện đảm bảo thoát nước tốt và hạn chế ngập úng cho cây sầu riêng?\\n'\n",
    "arr_documents, embeddings = get_embeddings_bartbert(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance(question, embedding_question = None, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'Làm thế nào để tạo điều kiện đảm bảo thoát nước tốt và hạn chế ngập úng cho cây sầu riêng?\\n'\n",
    "    # if embedding_question is None:\n",
    "    #     try:\n",
    "    #         # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    #         arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    #     except Exception as e: \n",
    "    #         print(e)\n",
    "    #         return None, None ,None\n",
    "    #     query_embeddings = embeddings[0].tolist()\n",
    "    # else:\n",
    "    #     query_embeddings = embedding_question\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    \n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        document = session.get(Node, nodes[0].id).document\n",
    "        if document not in list_doc:\n",
    "            list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        system_prompt, user_prompt = get_prompt_template_2(\n",
    "            user_query=question,\n",
    "            context_str=context_str,\n",
    "            project=project,\n",
    "            organization=organization,\n",
    "            # agent=agent_name,\n",
    "        )\n",
    "        try:\n",
    "            llm_response = json.loads(\n",
    "                            retrieve_llm_response_test(\n",
    "                            user_prompt,\n",
    "                            model=model,\n",
    "                            # max_output_tokens=256,\n",
    "                            prefix_messages=system_prompt,\n",
    "                            )\n",
    "                            )\n",
    "            return list_relevant_docs, llm_response.get('message'), query_embeddings\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            return None ,None, query_embeddings\n",
    "\n",
    "    return None, None, query_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "# # try:  # load previous generations if they exist\n",
    "# #     with open(output_file, \"r\") as f:\n",
    "# #         outputs = json.load(f)\n",
    "# # except:\n",
    "# #     outputs = []\n",
    "# number_error = 0\n",
    "# while number_error < 1:\n",
    "#     # number_error+=1 \n",
    "#     outputs = []\n",
    "#     # ds_continue = generated_questions.iloc[45:]\n",
    "\n",
    "#     for index in tqdm(range(len(ds))):\n",
    "#         if number_error >10:\n",
    "#             break\n",
    "#         example = ds.iloc[index]\n",
    "#         question = example[\"question\"]\n",
    "#         embedding_question = example[\"embedding_question\"]\n",
    "#         print(example['generated_answer'],111111111111111111)\n",
    "#         if example['generated_answer'] is not None:\n",
    "#             continue\n",
    "#         relevant_docs, answer,embedding_question  = test_rag_performance(question,embedding_question)\n",
    "#         if answer is None:\n",
    "#             number_error+=1\n",
    "\n",
    "#             time.sleep(1)\n",
    "#             continue\n",
    "#             break\n",
    "#         print(\"=======================================================\")\n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Answer: {answer}\")\n",
    "#         print(f'True answer: {example[\"answer\"]}')\n",
    "#         ds.at[index,'generated_answer'] = answer\n",
    "#         ds.at[index,'retrieved_docs'] = relevant_docs\n",
    "#         ds.at[index,'embedding_question'] = embedding_question\n",
    "\n",
    "#         # result = {\n",
    "#         #     \"question\": question,\n",
    "#         #     \"true_answer\": example[\"answer\"],\n",
    "#         #     \"source_doc\": example[\"source_doc\"],\n",
    "#         #     \"generated_answer\": answer,\n",
    "#         #     \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "#         # }\n",
    "#         # if result not in outputs:\n",
    "#         #     outputs.append(result)\n",
    "\n",
    "#         # result = {\n",
    "#         #     \"question\": question,\n",
    "#         #     \"true_answer\": example[\"answer\"],\n",
    "#         #     \"source_doc\": example[\"source_doc\"],\n",
    "#         #     \"generated_answer\": None,\n",
    "#         #     \"retrieved_docs\": None,\n",
    "#         # }\n",
    "        \n",
    "#         # if result not in outputs:\n",
    "#         #     outputs.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\" Nhiệm vụ của bạn là viết một câu trả lời để trả lời câu hỏi từ người dùng dựa trên bối cảnh.\n",
    "Câu trả lời toàn bộ nội dung trong bối  cảnh\n",
    "Bạn không bao giờ nói dối hay bịa ra những câu trả lời không được nêu rõ ràng trong bối cảnh\n",
    "Nếu bạn không chắc chắn về câu trả lời hoặc câu trả lời không có rõ ràng trong bối cảnh, bạn sẽ nói: \"Tôi xin lỗi, tôi không biết phải trợ giúp điều đó như thế nào\".\n",
    "Bạn luôn giữ câu trả lời dài, phù hợp và chính xa\n",
    "Câu trả lời bằng một đoạn thông tin thực tế cụ thể, ngắn gọn từ ngữ cảnh.\n",
    "Toàn bộ câu trả lời và câu hỏi phải được viết bằng tiếng Việt.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Kết quả:::\n",
    "Trả lời: (câu trả lời của bạn cho câu hỏi thực tế)\n",
    "\n",
    "Đây là bối cảnh.\n",
    "Bối cảnh: {context}\\n\n",
    "Đây là câu hỏi.\n",
    "Câu hỏi của người dùng: {question}\\n\n",
    "Kết quả:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\"\n",
    "   [INST]\n",
    "    Bạn sẽ trả lời các câu hỏi của [USER] chỉ bằng cách sử dụng  [DOCUMENT] và tuân theo [Quy tắc].\n",
    "    [USER]:\\n{question}\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context}\n",
    "\n",
    "    [QUY TẮC]:\n",
    "    Tôi sẽ chỉ trả lời các câu hỏi của người dùng bằng  [DOCUMENT] được cung cấp. Tôi sẽ tuân thủ các quy tắc sau:\n",
    "    - Bạn là nhân viên hỗ trợ khách hàng tốt nhất hiện nay\n",
    "    - Bạn sẽ trả lời toàn bộ nội dung trong [DOCUMENT]\n",
    "    - Bạn không bao giờ nói dối hay bịa ra những câu trả lời không được nêu rõ ràng trong [DOCUMENT]\n",
    "    - Nếu Bạn không chắc chắn về câu trả lời hoặc câu trả lời không có rõ ràng trong [DOCUMENT], Bạn sẽ nói: \"Tôi xin lỗi, tôi không biết phải trợ giúp điều đó như thế nào\".\n",
    "    - Bạn luôn giữ câu trả lời dài, phù hợp và súc tích.\n",
    "    - Bạn sẽ luôn phản hồi kết quả của bạn ở định dạng JSON \n",
    "    - Chỉ cần tạo đối tượng JSON bằng khóa sau: \"message\" là phản hồi của bạn  cho người dùng. Không cần giải thích:\n",
    "    [/INST]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance_2(question, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'Làm thế nào để tạo điều kiện đảm bảo thoát nước tốt và hạn chế ngập úng cho cây sầu riêng?\\n'\n",
    "    # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "\n",
    "    session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    print(nodes)\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        for node in nodes:\n",
    "            document = session.get(Node, node.id).document\n",
    "            if document not in list_doc:\n",
    "                list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        # system_prompt, user_prompt = get_prompt_template_2(\n",
    "        #     user_query=question,\n",
    "        #     context_str=context_str,\n",
    "        #     project=project,\n",
    "        #     organization=organization,\n",
    "        #     # agent=agent_name,\n",
    "        # )\n",
    "        # try:\n",
    "        # repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "        # llm_client = InferenceClient(\n",
    "        #     model=repo_id,\n",
    "        #     timeout=120,\n",
    "        #     token='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',\n",
    "        # )\n",
    "\n",
    "        \n",
    "        # llm_response = call_llm(llm_client,QA_generation_answer_prompt.format(context=context_str, question=question))\n",
    "        llm_response = None\n",
    "        # llm_response = json.loads(\n",
    "        #                 retrieve_llm_response_test(\n",
    "        #                 user_prompt,\n",
    "        #                 model=model,\n",
    "        #                 # max_output_tokens=256,\n",
    "        #                 prefix_messages=system_prompt,\n",
    "        #                 )\n",
    "                        # )\n",
    "                \n",
    "        return list_relevant_docs, llm_response,list_doc\n",
    "        # except:\n",
    "        #     return None ,None\n",
    "\n",
    "    return None, None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Làm thế nào để cà phê ra hoa đồng loạt?\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index =3\n",
    "ds.iloc[index].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Để cà phê ra hoa đồng loạt, bạn cần xác định thời điểm tưới nước cho phù hợp. Sau khi thấy nụ hoa có dạng mỏ sẻ xuất hiện đầy đủ ở đốt ngoài cùng của các cành, tiến hành tưới nước cho cà phê. Việc tưới nước đúng thời điểm lần đầu và đủ lượng nước tưới sẽ quyết định đến việc ra hoa đồng loạt.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.iloc[index].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM match_node_euclidean(\n",
      "    '[0.4996092617511749, -1.9378719329833984, -0.3662485182285309, -1.4759762287139893, -0.930750846862793, 0.42005330324172974, -0.079752117395401, -0.36336812376976013, 1.7296572923660278, -1.2460492849349976, 2.705437183380127, -1.0081355571746826, 1.0538535118103027, -0.6345371007919312, -0.12886667251586914, 1.211115837097168, -1.6051325798034668, -1.097388505935669, 0.5939808487892151, 0.6448752284049988, -2.9748642444610596, -3.337594509124756, 0.5572725534439087, -2.1943411827087402, -0.246882826089859, 1.950541377067566, 0.2505793571472168, -1.987774133682251, 0.38381248712539673, 0.6951450705528259, -0.13084068894386292, 2.559694290161133, -1.2074189186096191, 1.6059107780456543, -2.4690303802490234, 0.12016299366950989, 2.383877754211426, -2.7418227195739746, 4.1265435218811035, 1.726280927658081, -0.7463431358337402, 1.3819544315338135, -0.9485738277435303, -2.869506597518921, 0.40796852111816406, -0.7626692652702332, -0.8183176517486572, 0.07036105543375015, -1.0016918182373047, 2.5831427574157715, 3.2148985862731934, -1.825901746749878, -2.905302047729492, -0.11071792989969254, -0.9374761581420898, 1.5839970111846924, 0.6860500574111938, 0.8499265909194946, -0.3030458688735962, -0.019524961709976196, 0.09683758020401001, 1.3160983324050903, 2.3431544303894043, 0.12499648332595825, 2.0941600799560547, -0.9819726347923279, 0.26888155937194824, -0.4067118167877197, 1.410790205001831, -0.6618671417236328, -0.8170261383056641, 1.222944974899292, 0.3384796380996704, 0.43000227212905884, -0.7985965013504028, -1.3906103372573853, 0.5794163346290588, 1.9326015710830688, 1.8277523517608643, -1.218960165977478, -2.568845510482788, -2.072209119796753, 0.7977203130722046, 0.12459411472082138, 0.6151224374771118, -2.276566505432129, -2.6273882389068604, 1.1387172937393188, 1.3571112155914307, 0.3432163596153259, 0.9460930824279785, 1.8738055229187012, 3.089146614074707, -0.7169908881187439, 1.8897690773010254, 1.952214002609253, 0.9415782690048218, 2.6831767559051514, -0.6652147769927979, -0.025233522057533264, 0.8314871191978455, -0.8108299374580383, -0.07216483354568481, 3.5206427574157715, -2.2465953826904297, 0.051339417695999146, -0.5501886606216431, 1.2991567850112915, -0.16400644183158875, 1.4207528829574585, 3.5738227367401123, 0.6982492208480835, -1.3494675159454346, 0.4683758020401001, 1.7212586402893066, 1.1649028062820435, 0.34903186559677124, -1.30790376663208, -1.681915044784546, 0.748380184173584, 0.9543331861495972, 0.11688393354415894, -2.6520256996154785, 0.005046078935265541, -0.8708807826042175, -1.8501300811767578, 0.6296926140785217, 0.4124194383621216, -1.4198899269104004, -0.15669667720794678, -1.0323611497879028, 0.4025740623474121, 0.23118051886558533, 0.5687471628189087, -0.7199106216430664, -0.20229007303714752, 0.49413490295410156, -2.465481996536255, 0.15242284536361694, -2.575972080230713, -0.42106345295906067, -0.8788847923278809, 0.47046053409576416, 2.4897918701171875, 1.639289140701294, 0.14595407247543335, -0.7048893570899963, -1.810807704925537, 1.873110055923462, -3.0064175128936768, -3.4493775367736816, -0.6666325926780701, 0.18222366273403168, 0.7801258563995361, -0.6889358758926392, 1.336021900177002, -1.114638090133667, -0.6865462064743042, -0.08570218086242676, 1.0238600969314575, -0.06105698645114899, -0.030967190861701965, -1.0137968063354492, -1.4780213832855225, -0.267973929643631, 1.9237391948699951, 1.9536679983139038, 0.7415444850921631, -0.3803013563156128, 0.1382637917995453, 0.1389382779598236, -1.0104115009307861, -0.8098639249801636, 2.5957138538360596, 0.2509995102882385, -1.4323391914367676, -0.32374662160873413, 1.0648268461227417, 0.9892578125, -0.3625142574310303, 1.969801902770996, 2.4486234188079834, -1.2050509452819824, 0.4012472927570343, 0.6892302632331848, 0.4913631081581116, 2.7220234870910645, -0.3239961862564087, -2.7165684700012207, -2.9876856803894043, -2.951472759246826, 0.6615762710571289, -0.4026206135749817, -0.21141129732131958, 0.4551425576210022, -0.29432567954063416, -2.3610310554504395, -0.7624770998954773, -0.4807065427303314, 1.6423276662826538, 0.8125874400138855, 2.366424798965454, -2.3769001960754395, 0.5982350707054138, 2.083979606628418, 3.6760621070861816, 0.5808674097061157, -0.29261189699172974, -3.127568483352661, 1.0581600666046143, 1.2768163681030273, 2.819854497909546, -0.2860199213027954, -1.0020285844802856, 2.36556339263916, -0.5896151661872864, 0.046833381056785583, 1.2336021661758423, -2.1828224658966064, -0.33659279346466064, -0.8081132173538208, 0.5925910472869873, 1.7339380979537964, -1.9528300762176514, -0.021310359239578247, -1.6533856391906738, -0.510499894618988, 0.14266517758369446, -1.9498395919799805, -1.0316619873046875, -0.9200255870819092, 0.05424875020980835, -0.33836132287979126, -0.12964186072349548, 0.014276497066020966, 3.496410369873047, -0.5803319215774536, -0.8761020302772522, -0.4950878620147705, 0.37991875410079956, -0.40159523487091064, -0.2773209810256958, -2.2472832202911377, 0.8286755681037903, -0.16462618112564087, -1.4625005722045898, -2.7596359252929688, -0.5986781120300293, -2.044553756713867, 0.4336796998977661, -2.6652073860168457, -1.7230473756790161, 0.6887195706367493, -0.8847858309745789, 0.5620155334472656, -0.20317453145980835, -1.7595142126083374, 1.7614625692367554, 0.0035857558250427246, -2.161348819732666, 0.8645832538604736, 0.8364192247390747, -3.301318883895874, -0.032614052295684814, -2.5816218852996826, 2.9724795818328857, -0.7119124531745911, 1.3585002422332764, -0.5939086675643921, 2.07450008392334, -0.1508193165063858, -1.7134816646575928, -2.0343737602233887, 1.0874419212341309, 1.3269422054290771, 0.8438382744789124, -3.2144365310668945, -2.807286262512207, -0.44879770278930664, -1.1236205101013184, -1.9601384401321411, -1.6823700666427612, -1.4246764183044434, 0.8018877506256104, 1.0819820165634155, 0.548912525177002, -0.3450847268104553, -0.4166955053806305, -1.2564493417739868, 5.497670650482178, 0.9738276600837708, -0.09584533423185349, -0.6981956958770752, 0.5801475048065186, -2.714240789413452, 1.2046492099761963, 1.6916744709014893, 1.0360610485076904, 2.4672694206237793, 0.3805285692214966, -1.9222021102905273, -1.4561119079589844, 1.4035110473632812, -0.22141174972057343, -1.0533100366592407, 0.6387022733688354, -1.976518988609314, -0.7541394233703613, 0.1376773715019226, 1.5566325187683105, -0.3515404462814331, 1.6519408226013184, -1.4270111322402954, -2.1964640617370605, 0.2667677402496338, -1.0872546434402466, -2.037067413330078, -0.7640840411186218, 0.7628176212310791, -0.1612720936536789, -0.9603989720344543, -1.1467101573944092, 1.446634292602539, 0.204560324549675, 0.16437435150146484, 0.6846444606781006, 0.2101382315158844, 1.284989356994629, 1.0708508491516113, -0.968475341796875, 1.419224500656128, 2.4127187728881836, -2.044278860092163, -2.141749382019043, -1.6097605228424072, 2.581207513809204, -1.8360551595687866, -0.9605150818824768, 1.7955139875411987, 2.309875249862671, 0.6302812099456787, -0.8425744771957397, 0.29572880268096924, 1.0609155893325806, 1.0770188570022583, -0.37868934869766235, -0.9092849493026733, 1.6191303730010986, -1.8041576147079468, -1.6017494201660156, -2.2830810546875, -1.4783313274383545, -0.22341281175613403, -1.5997869968414307, 0.572392463684082, 0.5108468532562256, -0.39508622884750366, -1.4003112316131592, 2.259549856185913, 0.5521412491798401, 1.1092517375946045, -2.271087884902954, -0.07913489639759064, -1.7218027114868164, -0.060638152062892914, 0.006688714027404785, 7.786961555480957, 1.5945947170257568, -0.08527393639087677, -2.2386631965637207, 0.3166002333164215, -2.2868025302886963, -0.28786155581474304, 0.15774375200271606, 0.547148585319519, 0.2505944073200226, -3.536583423614502, -2.7908706665039062, -2.5386464595794678, -3.171812057495117, 0.3639541566371918, 0.06424415111541748, -0.7797027826309204, 0.5696341395378113, 1.9849984645843506, 1.9252736568450928, 0.9690226912498474, -0.3778378963470459, 0.02935837209224701, -0.7550874352455139, -2.320455312728882, -0.419511079788208, -2.225386142730713, 0.4410722255706787, 0.6509045958518982, 0.05604705214500427, 1.863168716430664, 0.6306540369987488, 0.208499014377594, -0.9000926613807678, -0.8377560377120972, 0.4073214530944824, -0.5721784234046936, 4.654333114624023, 0.4961538314819336, -2.2195074558258057, -0.9733672738075256, 1.4471514225006104, -1.0081844329833984, -0.20278826355934143, -1.5738786458969116, -0.2667962312698364, -0.817821204662323, 0.5182196497917175, -0.882258415222168, 0.43614447116851807, 1.023116111755371, -1.4044817686080933, 1.3185029029846191, 1.1543421745300293, -0.38049253821372986, 0.5958936810493469, 0.3012394607067108, 0.1471872627735138, 2.650919198989868, 0.8836819529533386, -0.6644574403762817, -3.9879612922668457, -2.2182915210723877, -1.0416455268859863, 1.6322540044784546, 0.23896722495555878, -0.18624255061149597, -1.2744662761688232, -0.23077058792114258, 1.6476298570632935, 0.6662904024124146, 0.6456606388092041, -1.7087122201919556, -0.06139105558395386, 0.51426100730896, 0.9540575742721558, 0.501847505569458, -0.3391173481941223, -1.6503502130508423, -1.0810853242874146, -1.201101303100586, 0.29832351207733154, 3.7196006774902344, 1.6852033138275146, -3.497354507446289, 2.5125298500061035, 0.7827538847923279, 2.136509895324707, 2.9913368225097656, 1.8137115240097046, 0.5036309957504272, 0.6190868616104126, -1.4529483318328857, -0.7243532538414001, -0.2378385365009308, -0.25660932064056396, 2.3626811504364014, -0.27021145820617676, 0.08956030011177063, -0.9938331246376038, 0.09139549732208252, -1.2063791751861572, -0.1749355047941208, 1.3446340560913086, -0.09358978271484375, -2.618769645690918, -1.7764965295791626, -2.305657386779785, -0.7916860580444336, -0.5481195449829102, 0.32148584723472595, -0.1578797996044159, 1.9189192056655884, 1.5775160789489746, 1.714719533920288, 0.21248158812522888, -1.5161683559417725, -1.5663896799087524, -0.26044073700904846, 0.04978370666503906, 3.6031510829925537, -0.48546773195266724, 0.4587443768978119, -1.4849092960357666, 1.8745108842849731, -2.5368247032165527, 0.8206006288528442, -2.7138030529022217, -0.25228169560432434, 2.0130224227905273, -0.5018998384475708, -0.09785439074039459, 0.35776466131210327, 2.225097179412842, 0.11111384630203247, -1.7781941890716553, 1.330155849456787, 0.6286634802818298, 0.884967565536499, 0.267628937959671, -1.0239958763122559, 0.34162482619285583, 2.1497275829315186, 1.6195764541625977, 2.6609814167022705, -1.0721094608306885, -1.7063652276992798, 0.06592491269111633, 1.2114763259887695, 1.570307970046997, 1.3784897327423096, -3.3742847442626953, -0.008897334337234497, -1.9553825855255127, -0.9916145205497742, -2.69174861907959, 1.661900281906128, 1.2641479969024658, -0.29284751415252686, 0.23957429826259613, 2.080204725265503, 0.45811623334884644, -1.751845121383667, 1.9766912460327148, -2.3264927864074707, 0.9603379964828491, -2.078335762023926, -3.7219605445861816, -1.9044495820999146, -0.2544691860675812, -1.2366704940795898, 1.5760488510131836, -0.3888205587863922, -2.450977087020874, -1.7426109313964844, 0.7113677263259888, -1.532036542892456, -0.6184749007225037, 1.4743967056274414, 1.1057469844818115, 0.945091187953949, -0.0355679988861084, -0.43614453077316284, 0.5742219090461731, 0.15757787227630615, 0.9768886566162109, -1.3154196739196777, -2.6843507289886475, 1.7315218448638916, 2.3973135948181152, 0.0977163314819336, -2.410386085510254, -0.6202300786972046, -3.219656467437744, -1.9255951642990112, 1.5513057708740234, -2.107381820678711, -0.6196467280387878, -0.23372483253479004, 0.01404573768377304, -0.11667415499687195, 1.4531465768814087, 0.2774815261363983, 1.8229167461395264, 0.45384061336517334, 4.068092346191406, -0.6983704566955566, -1.9102091789245605, -0.07832276821136475, 0.46790969371795654, -2.4532222747802734, 0.9644925594329834, -0.41278165578842163, -3.0570285320281982, 1.1264076232910156, -0.42130064964294434, -0.4408082067966461, 0.0705813467502594, -3.6566848754882812, -2.1806445121765137, -0.04893304407596588, 0.8667667508125305, 0.01830947957932949, 1.1385339498519897, 2.252769947052002, 1.3623108863830566, 0.663774847984314, -3.570833206176758, 0.4191667139530182, -1.338860034942627, -0.11524398624897003, -0.45062384009361267, 0.8498812913894653, 1.0715113878250122, -0.747296929359436, -2.2983741760253906, -1.299461007118225, 1.061746597290039, -1.5905717611312866, -0.7217793464660645, 0.3372487425804138, -0.24819082021713257, 0.5358131527900696, -0.5887041091918945, 0.9532344341278076, -2.703981637954712, 0.6529525518417358, -0.7472695112228394, 2.8721797466278076, 1.306524395942688, 0.34763821959495544, 1.5362476110458374, -0.014080476015806198, 1.317368507385254, 2.220752716064453, 0.46556517481803894, -0.6651474237442017, 2.1314969062805176, 1.974992036819458, 1.0386911630630493, -0.18063676357269287, -1.3248436450958252, -3.9924182891845703, 1.711763858795166, 0.081606924533844, 3.3293590545654297, 0.1120448112487793, -1.8617459535598755, -0.34564533829689026, -0.3530328869819641, 0.3522892892360687, 1.3494369983673096, -0.5554215908050537, -0.17971867322921753, -0.6917291283607483, 0.16431522369384766, 0.8906238079071045, -1.8120110034942627, 0.01705251634120941, -0.5690996646881104, 0.4575696289539337, -1.8531289100646973, -0.2019757628440857, -0.12480044364929199, 1.596318006515503, 1.1830788850784302, 0.6875076293945312, -0.42730093002319336, 0.729981005191803, 0.591178834438324, -0.6020563840866089, -0.6788214445114136, 2.0788815021514893, -0.13848932087421417, 1.3815932273864746, 0.0142916738986969, 1.0421912670135498, -0.9330877661705017, -0.03544791042804718, 0.4121643304824829, -0.6489253044128418, -1.6332128047943115, -2.839940071105957, -0.32002654671669006, 0.8717694282531738, 0.5398716330528259, 0.1040116548538208, -0.3443649411201477, -3.0537092685699463, -0.4470117390155792, 0.16813978552818298, 1.0905187129974365, -0.5787950754165649, 2.4199423789978027, -1.378591537475586, -0.6060616374015808, -3.054288387298584, -2.9805612564086914, -1.760377049446106, 1.1421799659729004, -4.385231018066406, -2.26050066947937, 1.751394271850586, -1.4484186172485352, -3.613893985748291, -1.5063621997833252, -0.7649528980255127, -1.6489734649658203, -1.7479288578033447, 1.311100959777832, 0.9313958883285522, -0.8818585276603699, 0.47879233956336975, 2.6386523246765137, 0.9963072538375854, 0.2980709373950958, 0.5599012970924377, -0.8217987418174744, -0.3349427878856659, 0.4955357611179352, -0.07521656155586243, -0.6698015928268433, -2.206226110458374, -1.226829171180725, -1.0706045627593994, -1.7680482864379883, -2.5246477127075195, -1.2979187965393066, 1.4813125133514404, -0.41756173968315125, -1.2317359447479248, 0.5584030151367188, -0.8263911008834839, 0.6618524789810181, -0.39423200488090515, 1.253064751625061, -0.5987750887870789, -1.5033905506134033, -1.2102961540222168, -0.07383854687213898, -2.23244047164917, -0.6416722536087036, 0.8055505752563477, -1.6019837856292725, -0.11111763119697571, 1.951481819152832, -1.4358584880828857, 0.2376784086227417, -2.1410179138183594, 2.2347888946533203, -0.9361410737037659, -0.2937948703765869, -2.4433650970458984, 0.3223184049129486, 3.197299003601074, 0.018078237771987915, -0.12288790941238403, -2.666867733001709, -1.4656071662902832, -0.24985691905021667, -1.427527904510498, 1.0314158201217651, -1.9406263828277588, -1.5245994329452515, 1.9537136554718018, -1.1878397464752197, 1.2462376356124878, 1.769618272781372, -0.5975441932678223, -0.023874834179878235, -0.10892325639724731, -2.379742383956909, 0.6514783501625061, 0.04451480507850647, -1.7963614463806152, -0.5071077942848206, 1.3946185111999512, -0.08116622269153595, -1.5183537006378174, -2.1613376140594482, 1.4915709495544434, -0.3008282780647278, -0.15286368131637573, -0.34060990810394287, 3.265631675720215, -2.4130067825317383, -0.002161860466003418, -0.4216127395629883, 2.150665283203125, -0.4496535062789917, 0.06570582836866379, -1.0380239486694336, -1.5594415664672852, 1.5156807899475098, 0.4067175090312958, 0.871914267539978, -0.407113254070282, 1.6282130479812622, -2.382370948791504, 1.4613609313964844, 0.17153701186180115, 1.2147802114486694, 0.6308507323265076, -0.2889246940612793, -1.9793701171875, 0.2509955167770386, 0.3523859679698944, 0.6582516431808472, -1.7645800113677979, -0.003849431872367859, -0.8770895004272461, 0.602353572845459, 1.7918403148651123, -3.3898117542266846, -0.7390509247779846, -0.7597935199737549, -0.7615084648132324, 0.542452335357666, 0.37521234154701233, -0.05060136318206787, -0.6616356372833252, -0.8276628255844116, -1.387223482131958, -1.5842370986938477, -0.4105936884880066, -1.5834641456604004, 0.9270840883255005, 1.5277161598205566, 1.6942052841186523, 0.9861268997192383, 0.3176189363002777, 1.162693738937378, -0.6852779388427734, -0.7075988054275513, -1.4462683200836182, 1.8817791938781738, 2.0281600952148438, 1.5843533277511597, -0.587136447429657, -0.5756190419197083, 0.07235527038574219, -1.3125559091567993, -0.5270045399665833, 0.5987035036087036, -0.4782489836215973, -0.29262033104896545, 1.7877278327941895, -2.2802135944366455, 0.7364464998245239, -1.0304982662200928, -0.8118249177932739, -1.1932055950164795, 1.032246470451355, -0.8709115982055664, 2.5548458099365234, 1.44737708568573, -1.6488683223724365, -2.7035582065582275, -3.3879849910736084, -0.5201979875564575, -2.0274784564971924, -0.5890852808952332, 1.5769528150558472, 0.28011882305145264, 0.5949459075927734, -1.5419032573699951, -0.5942310690879822, 0.06137843430042267, 3.0323095321655273, 1.4739158153533936, -0.5416781902313232, 0.8664578795433044, 0.28872519731521606, 0.7095029354095459, 0.8374093174934387, 0.6245885491371155, 0.3448449969291687, 2.1577610969543457, -4.051453113555908, -1.2309507131576538, 0.8020186424255371, 1.4782969951629639, 2.346820592880249, 0.437297523021698, -3.3498103618621826, 2.2122366428375244, -0.6304749250411987, 2.090489387512207, 0.11402688920497894, -1.1299386024475098, -0.2667185366153717, 0.7696870565414429, -0.4173504412174225, -0.6837857365608215, -3.852814197540283, 0.1691608428955078, 0.17933154106140137, 2.670492649078369, -1.0712279081344604, -0.3288027346134186, 1.5544626712799072, 1.7276338338851929, -0.5656968355178833, -1.7524046897888184, -1.275222897529602, -0.957273542881012, 0.6406216621398926, 1.9216632843017578, 1.146910309791565, -1.4067962169647217, 1.3333888053894043, -0.35058677196502686, 2.731135606765747, -1.2784209251403809, 0.45968249440193176, 2.145756483078003, 0.2854057252407074, 0.12243013083934784, -4.293540000915527, 2.0804848670959473, -0.08590331673622131, -3.5444648265838623, -2.8097915649414062, 0.4813747704029083, -2.27164363861084, 1.897317886352539, 0.102988600730896, 1.9007980823516846, 1.9947376251220703, -0.9158291816711426, -3.182114362716675, 0.5062896013259888, 0.04936642944812775, 2.6615374088287354, 0.6362144351005554, -1.4855692386627197, 1.0660669803619385, -0.5661356449127197, -0.02914014458656311, -0.4000702202320099, 0.18088814616203308, 1.0402891635894775, 1.4646538496017456, -2.0284366607666016, 0.4305945038795471, 0.08348950743675232, 1.291939616203308, -2.259162425994873, -0.50214684009552, 1.4945451021194458, -1.2561131715774536, 0.2799728512763977, 0.9815015196800232, 0.4171880781650543, 1.3442084789276123, -0.8985893130302429, -2.49800968170166, -2.8310508728027344, -0.696010410785675, -2.057899236679077, -2.7591137886047363, 1.1851533651351929, -1.620607852935791, -0.3867129385471344, 1.2247130870819092, -1.3492579460144043, 1.966176986694336, 2.9844508171081543, -0.015006963163614273, 2.071852684020996, 0.5221186280250549, -1.5915896892547607, -0.4586191773414612, 0.027885735034942627, 1.5384904146194458, 0.2658360004425049, -0.7370353937149048, 1.4504541158676147, -0.4064733684062958, 0.5371043682098389, -1.3343061208724976, -0.21989589929580688, -1.9214448928833008, 1.4587030410766602, 1.009344458580017, 1.1079641580581665, -0.07145035266876221, -0.5709485411643982, 0.19030320644378662, -1.5846318006515503, -2.793588399887085, 1.8356014490127563, 0.7101314067840576, 4.079638957977295, 0.4830610156059265, -0.9142231941223145, -0.9011071920394897, 0.5501949787139893, 1.3746764659881592, -1.234757661819458, 2.045149564743042, -0.45008474588394165, -1.0252034664154053, -0.8436344265937805, -0.988187313079834, -0.7411688566207886, -2.1975607872009277, 0.5204917788505554, 1.5533894300460815, 0.6207700967788696, 0.05783924460411072, 2.492870569229126, -3.5984888076782227, -0.9431455135345459, 2.143258571624756, 0.5006019473075867, 0.4068228006362915, 1.4527792930603027, 0.27924609184265137, -0.5548146963119507, 0.6608067154884338, 2.798401355743408, -0.5844500064849854, -0.013840436935424805, -0.537621021270752, 1.2562510967254639, -1.8572319746017456, -0.9373587965965271, 1.067662239074707, -1.3879530429840088, -0.41859352588653564, 2.066883087158203, -0.7941335439682007, -0.21350419521331787, -2.245789051055908, -1.142451286315918, 0.5684605836868286, -0.7719943523406982]'::vector(1024),\n",
      "    0.2::double precision,\n",
      "    3);\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "list_relevant_docs, llm_response,list_doc = test_rag_performance_2(ds.iloc[index].question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Document id=31 name=category_trees_Sầu_riêng.md uuid=f1a381d8-8c06-4c4c-a317-bf633cb94634>,\n",
       " <Document id=52 name=category_trees_Tre_mạnh_tông.md uuid=88ac50f5-7248-4bf9-8c57-009285c34e9e>,\n",
       " <Document id=55 name=2nong_introduction.md uuid=e8328004-813d-4858-9e6a-7d21d26044c6>,\n",
       " <Document id=50 name=category_trees_Thủy_trúc__lác_dù.md uuid=3421b192-8574-47a3-a628-2feeb9ac30c6>,\n",
       " <Document id=51 name=category_trees_Thủy_tùng.md uuid=3116c346-f248-4e23-9e9d-f4b7e1ca7640>,\n",
       " <Document id=43 name=category_trees_Lay_ơn.md uuid=1685a8e9-1ca6-467e-bcb6-06146b78b336>,\n",
       " <Document id=54 name=category_trees_Đủng_đỉnh.md uuid=ea03ee7e-61e5-4da6-a17a-f55b87130f80>,\n",
       " <Document id=28 name=category_trees_Dừa.md uuid=1ccf30d7-49e4-41f6-8f79-d855049774d8>,\n",
       " <Document id=46 name=category_trees_Lục_bình.md uuid=09a780cc-182a-4322-b78b-68960d4c2a21>,\n",
       " <Document id=33 name=category_trees_Cà_phê.md uuid=c3a92b09-9ec5-44f9-825a-0be9c5fc00c3>,\n",
       " <Document id=5 name=knowledge_handbooks_Cách_bón_lót_cho_cà_phê_trồng_mới.md uuid=942c4399-3714-4dac-b316-189fb444dff7>,\n",
       " <Document id=45 name=category_trees_Sen_đá.md uuid=248e7a31-246e-4870-bcd7-db2d150aa674>,\n",
       " <Document id=49 name=category_trees_Thốt_nốt.md uuid=1381ce97-6736-4be4-ac30-3df0886f5d64>,\n",
       " <Document id=44 name=category_trees_Lẻ_bạn__sò_huyết__bang_hoa.md uuid=f63176bc-c04d-4bf3-bb63-b12e350e3bd2>,\n",
       " <Document id=29 name=category_trees_Dừa_nước__dừa_lá.md uuid=0d9387d4-435b-43cc-9411-a315d0768b53>,\n",
       " <Document id=38 name=category_trees_Lan_Cattleya.md uuid=b81f2971-cf9a-4439-9ce8-7205102a695d>,\n",
       " <Document id=30 name=category_trees_Chuối_rẽ_quạt.md uuid=9a52dd3c-46c6-44b4-aaa0-17623cf6365c>,\n",
       " <Document id=7 name=knowledge_handbooks_Có_cần_bón_phân_hữu_cơ_cho_đất_phèn_không.md uuid=8847f0b2-fa25-42e7-9537-bbf9a8e0cdfd>,\n",
       " <Document id=48 name=category_trees_Thiên_tuế.md uuid=cf7fce02-3679-49ad-8654-291911f3269e>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sầu riêng\\r\\nDurio zibethinus (được gọi là Sầu riêng Đông Nam Á) là loài thực vật thuộc chi Sầu riêng phổ biến nhất.',\n",
       " 'Tre mạnh tông\\r\\nTre Mạnh Tông, tên khoa học Dendrocalamus asper, là một loài thực vật có hoa trong họ Hòa thảo. Loài này được (Schult.) Backer mô tả khoa học đầu tiên năm 1927.',\n",
       " 'Thông tin ứng dụng :\\r\\n\\r\\nWebsite : https://2nong.vn/\\r\\n\\r\\nLink adroi : https://play.google.com/store/apps/details?id=com.pvcfc.inong&hl=vi&gl=US\\r\\n\\r\\nLinhk ios : https://apple.co/3kzb6us\\r\\n\\r\\nLink fanpage : https://www.facebook.com/2nong.vn',\n",
       " 'Thủy trúc, lác dù\\r\\nCyperus alternifolius là một loài thực vật có hoa trong họ Cói. Loài này được L. mô tả khoa học đầu tiên năm 1767.',\n",
       " 'Thủy tùng\\r\\nMăng leo hay măng bàn tay, thủy tùng (danh pháp: Asparagus setaceus) là loài thực vật có hoa trong họ Măng tây. Loài này được (Kunth) Jessop mô tả khoa học đầu tiên năm 1966.',\n",
       " 'Lay ơn\\r\\nĐang cập nhật data về cây trồng này. Bạn có thể sử dụng tính năng đóng góp dữ liệu để giúp chúng tôi cải thiện mô hình nhận dạng ..',\n",
       " 'Đủng đỉnh\\r\\nCây đủng đỉnh, còn gọi là cây đùng đình, móc, tên khoa học là Caryota mitis; là loài thực vật có hoa thuộc họ Cau (Arecaceae). Loài này được Lour. miêu tả khoa học đầu tiên năm 1790.',\n",
       " 'Dừa\\r\\nĐang cập nhật data về cây trồng này. Bạn có thể sử dụng tính năng đóng góp dữ liệu để giúp chúng tôi cải thiện mô hình nhận dạng ..',\n",
       " 'Lục bình\\r\\nĐang cập nhật data về cây trồng này. Bạn có thể sử dụng tính năng đóng góp dữ liệu để giúp chúng tôi cải thiện mô hình nhận dạng ..',\n",
       " '.000 ha/410.000 ha). Hiện cà phê chè được trồng ở các tỉnh Lâm Đồng ở Tây Nguyên, vùng thành phố Sơn La, huyện Mai Sơn, huyện Thuận Châu (Sơn La) và Mường Ảng (Điện Biên) ở Tây Bắc.',\n",
       " 'Cách bón lót cho cà phê trồng mới?\\r\\nCho vào hố 15 – 20 kg phân chuồng hoai + 1 kg vôi + 0,5 kg lân nung chảy, trộn với đất mặt, đảo đều (trước khi trồng 1 tháng).',\n",
       " '## **Liên kết khác**\\r\\n\\r\\n<https://vi.wikipedia.org/wiki/Echeveria>\\r\\n\\r\\n<https://caycanhbancong.com/san-pham/cay-sen-da/>\\r\\n\\r\\n<https://noth.garden/echeveria-hoa-da-sen-da/>\\r\\n\\r\\n<https://joygarden.vn/cach-trong-sen-da-chuan-chia-se-cua-nguoi-7-nam-trong-sen-da/>',\n",
       " 'Thốt nốt\\r\\nThốt nốt hay cọ đường (danh pháp hai phần: Borassus flabellifer) là loài thực vật thuộc họ Cau, bản địa của Nam Á và Đông Nam Á, phân bố từ Indonesia đến Pakistan. Tuy nhiên, vùng Tây Song Bản Nạp và Cảnh Hồng, Vân Nam, Trung Quốc cũng trồng loại cây này.',\n",
       " 'Lẻ bạn, sò huyết, bang hoa\\r\\nCây sò huyết (hay sò tím, sắc màu, lẻ bạn, bạng hoa) là một loài thực vật có hoa trong họ Thài lài, danh pháp khoa học là Tradescantia spathacea hoặc Tradescantia discolor. Loài này được Sw. miêu tả khoa học đầu tiên năm 1788. Cây có tác dụng làm cảnh và thuốc trong y học.',\n",
       " 'Dừa nước, dừa lá',\n",
       " '2. <https://vi.wikipedia.org/wiki/Chi_CAt_lan>\\r\\n\\r\\n3. <https://nuoitrong.vn/lan-cattleya.html>',\n",
       " 'Cà phê',\n",
       " 'Chuối rẽ quạt\\r\\nChuối rẻ quạt còn được gọi là cây chuối quạt hay chuối cọ (danh pháp hai phần: Ravenala madagascariensis) là một loài thực vật thuộc họ Thiên điểu (Strelitziaceae), đặc hữu của Madagascar. Tuy vậy nó không phải là chuối (họ Musaceae) và cũng không phải là cọ (họ Arecaceae), Chuối rẻ quạt là loài duy nhất của chi Ravenala.',\n",
       " 'Có cần bón phân hữu cơ cho đất phèn không?\\r\\nNói chung đất phèn trồng lúa có chứa hàm lượng chất hữu cơ cao hơn nhiều loại đất khác. Nhưng vẫn rất cần bón phân hữu cơ. Vì chất hữu cơ có tác dụng như chất đệm ở trong đất, làm cho đất không chua thêm nhiều, ngay cả khi khô nước. Nhưng chú ý không bón các loại phân hữu cơ còn tươi.',\n",
       " 'Thiên tuế\\r\\nThiên tuế hay tuế lược (danh pháp hai phần: Cycas pectinata là loài thực vật phân bố ở đông bắc Ấn Độ, Nepal, Bhutan, bắc Myanma, Hoa Nam, bắc Thái Lan, Lào và Việt Nam. Đây là loài thứ tư của chi Cycasđược đặt tên khoa học với mô tả lần đầu năm 1826. Thiên tuế cao đến 40 foot (12 m) với tán rộng.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response.split(\"Trả lời: \")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SPECIFIC EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = \"E:/TrungPhanADVN/Code/LangChain_RAG/app/api/data/training_data/knowledge_handbooks_Cách_làm_cho_hoa_cà_phê_ra_đồng_loạt.md\"\n",
    "with open(source_file, \"r\") as f:\n",
    "    test_document = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cách làm cho hoa cà phê ra đồng loạt?\\nMuốn cà phê ra hoa đồng loạt cần xác định thời điểm tưới nước cho phù hợp. Việc xác định thời điểm tưới, lượng nước tưới, phương pháp tưới tùy thuộc vào điều kiện thời tiết, loại đất, tình trạng sinh trưởng của cây. Sau thời gian khô hạn, khi thấy nụ hoa có dạng mỏ sẻ xuất hiện đầy đủ ở đốt ngoài cùng của các cành thì tiến hành tưới nước cho cà phê. Việc tưới nước đúng thời điểm lần đầu (đợt 1) và đủ lượng nước tưới sẽ quyết định đến việc ra hoa đồng loạt.\\n\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = \"Cách làm cho hoa cà phê ra đồng loạt?\\nMuốn cà phê ra hoa đồng loạt cần xác định thời điểm tưới nước cho phù hợp. Việc xác định thời điểm tưới, lượng nước tưới, phương pháp tưới tùy thuộc vào điều kiện thời tiết, loại đất, tình trạng sinh trưởng của cây. Sau thời gian khô hạn, khi thấy nụ hoa có dạng mỏ sẻ xuất hiện đầy đủ ở đốt ngoài cùng của các cành thì tiến hành tưới nước cho cà phê. Việc tưới nước đúng thời điểm lần đầu (đợt 1) và đủ lượng nước tưới sẽ quyết định đến việc ra hoa đồng loạt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_output = torch.tensor(tokenizer_bartpho([test_document],padding='max_length', truncation=True, max_length=1024)['input_ids'])\n",
    "with torch.no_grad():\n",
    "    features = bartpho(token_output)\n",
    "\n",
    "last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "embeddings_docs = torch.mean(last_hidden_state, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2422, -3.0425, -0.7843,  ..., -1.6288,  0.7525, -0.6217]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_ebed.txt','w') as f:\n",
    "    f.write(str(embeddings_docs.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tokenizer_bartpho([test_document] ,truncation=True)['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_docs, embed_docs = get_embeddings_bartbert(test_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_docs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Làm thế nào để cà phê ra hoa đồng loạt?\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.iloc[index].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_test = '2Nong là app gì\\n'\n",
    "question_test = ds.iloc[index].question\n",
    "token_output = torch.tensor(tokenizer_bartpho([question_test],padding='max_length', truncation=True, max_length=1024)['input_ids'])\n",
    "with torch.no_grad():\n",
    "    features = bartpho(token_output)\n",
    "\n",
    "last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "embeddings_question = torch.mean(last_hidden_state, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('question_test_ebed.txt','w') as f:\n",
    "    f.write(str(embeddings_question.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_question, embed_question = get_embeddings_bartbert(question_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine(embeddings_docs[0],embeddings_question[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos(embeddings_docs[0],embeddings_question[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(embed_docs.shape[0]):\n",
    "    print(cosine(embed_docs[index],embed_question[0]))\n",
    "    # print(cos(embed_docs,embed_question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST GENERATED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-663c7bbb-562499331e279d775afda808;8901399b-4987-4edd-af05-d90f1c23d985)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1683\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    392\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 393\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    318\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m     )\n\u001b[1;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GatedRepoError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-663c7bbb-562499331e279d775afda808;8901399b-4987-4edd-af05-d90f1c23d985)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m tokenizer_generate \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model_generate \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:819\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 819\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    926\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    929\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    930\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADVN-DEV\\anaconda3\\envs\\dev\\lib\\site-packages\\transformers\\utils\\hub.py:416\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-663c7bbb-562499331e279d775afda808;8901399b-4987-4edd-af05-d90f1c23d985)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer_generate = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model_generate = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer_generate.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model_generate.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORE WITH GENERATED ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT =\"\"\"###Mô tả công việc:\n",
    "Một hướng dẫn (có thể bao gồm Đầu vào bên trong nó), một phản hồi để đánh giá, một câu trả lời tham khảo đạt điểm 5 và thang điểm đại diện cho tiêu chí đánh giá sẽ được đưa ra.\n",
    "1. Viết phản hồi chi tiết để đánh giá chất lượng phản hồi dựa trên thang điểm cho sẵn, không đánh giá chung chung.\n",
    "2. Sau khi viết phản hồi, hãy viết điểm là số nguyên từ 1 đến 5. Bạn nên tham khảo bảng đánh giá.\n",
    "3. Định dạng đầu ra sẽ như sau: \\\"Phản hồi: {{viết phản hồi cho tiêu chí}} [KẾT QUẢ] {{một số nguyên từ 1 đến 5}}\\\"\n",
    "4. Vui lòng không đưa ra bất kỳ lời mở đầu, kết thúc và giải thích nào khác. Hãy chắc chắn bao gồm [KẾT QUẢ] trong đầu ra của bạn.\n",
    "\n",
    "###Hướng dẫn đánh giá:\n",
    "{instruction}\n",
    "\n",
    "###Phản hồi để đánh giá:\n",
    "{response}\n",
    "\n",
    "###Câu trả lời tham khảo (Điểm 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Tiêu chí chấm điểm:\n",
    "[Câu trả lời có đúng, chính xác và thực tế dựa trên câu trả lời tham khảo không?]\n",
    "Điểm 1: Câu trả lời hoàn toàn không chính xác, không chính xác và/hoặc không thực tế.\n",
    "Điểm 2: Câu trả lời phần lớn là không chính xác, không chính xác và/hoặc không thực tế.\n",
    "Điểm 3: Câu trả lời có phần đúng, chính xác và/hoặc thực tế.\n",
    "Điểm 4: Câu trả lời hầu hết đều đúng, chính xác và thực tế.\n",
    "Điểm 5: Câu trả lời hoàn toàn đúng, chính xác và thực tế.\n",
    "\n",
    "###Nhận xét:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "# 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "# 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "# 3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "# 4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Is the response correct, accurate, and factual based on the reference answer?]\n",
    "# Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "# Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "# Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "# Score 4: The response is mostly correct, accurate, and factual.\n",
    "# Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Bạn là một mô hình đánh giá ngôn ngữ công bằng\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "# def evaluate_answers(\n",
    "#     answer_path: str,\n",
    "#     eval_chat_model: BaseChatModel,\n",
    "#     evaluator_name: str,\n",
    "#     evaluation_prompt_template: ChatPromptTemplate,\n",
    "# ) -> None:\n",
    "#     \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "#     answers = []\n",
    "# if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "#     answers = json.load(open(answer_path, \"r\"))\n",
    "answers= outputs\n",
    "evaluate_output = []\n",
    "for experiment in tqdm(answers):\n",
    "    if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "        continue\n",
    "\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        instruction=experiment[\"question\"],\n",
    "        response=experiment[\"generated_answer\"],\n",
    "        reference_answer=experiment[\"true_answer\"],\n",
    "    )\n",
    "    try:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    except:\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = None\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = None\n",
    "        continue\n",
    "    feedback, score = [item.strip() for item in eval_result.content.split(\"[KẾT QUẢ]\")]\n",
    "    experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "    experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "    # evaluate_output.append()\n",
    "    # with open(answer_path, \"w\") as f:\n",
    "    #     json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(answers,open('result_evaluate_1.json','w',encoding='utf-8'), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(json.load(open('result_evaluate_1.json', \"r\")))\n",
    "result = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] ) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result[\"eval_score_GPT4\"].mean()\n",
    "# average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import datasets\n",
    "import pandas as pd\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "# fig.update_layout(w\n",
    "#     width=1000,\n",
    "#     height=600,\n",
    "#     barmode=\"group\",\n",
    "#     yaxis_range=[0, 100],\n",
    "#     title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "#     xaxis_title=\"RAG settings\",\n",
    "#     font=dict(size=15),\n",
    "# )\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 2: https://freedium.cfd/https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-rag-applications-with-ragas-81d67b0ee31a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
