{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\"E:\\TrungPhanADVN\\Code\\LangChain_RAG\\scripts\\evaluat_dataset_2.xlsx\", index_col='Unnamed: 0').to_csv(\"E:\\TrungPhanADVN\\Code\\LangChain_RAG\\scripts\\evaluat_dataset_2.csv\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 1: https://huggingface.co/learn/cookbook/rag_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test = [\n",
    "\"2nong_introduction.md\",\n",
    "\"category_trees_Cà_phê.md\",\n",
    "\"category_trees_Chuối_rẽ_quạt.md\",\n",
    "\"category_trees_Dừa.md\",\n",
    "\"category_trees_Dừa_nước__dừa_lá.md\",\n",
    "\"category_trees_Đủng_đỉnh.md\",\n",
    "\"category_trees_Hoa_ly.md\",\n",
    "\"category_trees_Hồ_tiêu.md\",\n",
    "\"category_trees_Lan_Cattleya.md\",\n",
    "\"category_trees_Lan_Chi.md\",\n",
    "\"category_trees_Lan_chu_đính.md\",\n",
    "\"category_trees_Lan_dạ_hương.md\",\n",
    "\"category_trees_Lan_Vanda.md\",\n",
    "\"category_trees_Lay_ơn.md\",\n",
    "\"category_trees_Lẻ_bạn__sò_huyết__bang_hoa.md\",\n",
    "\"category_trees_Lô_hội__nha_đam.md\",\n",
    "\"category_trees_Lúa.md\",\n",
    "\"category_trees_Lục_bình.md\",\n",
    "\"category_trees_Mía.md\",\n",
    "\"category_trees_Phong_Lộc_Hoa.md\",\n",
    "\"category_trees_Sầu_riêng.md\",\n",
    "\"category_trees_Sen_đá.md\",\n",
    "\"category_trees_Thiên_tuế.md\",\n",
    "\"category_trees_Thốt_nốt.md\",\n",
    "\"category_trees_Thủy_trúc__lác_dù.md\",\n",
    "\"category_trees_Thủy_tùng.md\",\n",
    "\"category_trees_Trắc_bá_diệp.md\",\n",
    "\"category_trees_Tre_mạnh_tông.md\",\n",
    "\"knowledge_handbooks_3_bước_cải_tạo_đất_sau_thu_hoạch_đối_với_vườn_cây_ăn_trái.md\",\n",
    "\"knowledge_handbooks_Bón_đạm_cho_lúa_vào_thời_kỳ_nào_là_tốt_nhất.md\",\n",
    "\"knowledge_handbooks_Bón_vôi_đúng_quy_trình_cho_vườn_cây_ăn_trái.md\",\n",
    "\"knowledge_handbooks_Cách_bón_lót_cho_cà_phê_trồng_mới.md\",\n",
    "\"knowledge_handbooks_Cách_bón_phân_chuồng_cho_rau_màu_hiệu_quả_nhất.md\",\n",
    "\"knowledge_handbooks_Cách_chăm_sóc_lúa_giai_đoạn_đòng_trổ_giúp_tăng_năng_suất_hiệu_quả.md\",\n",
    "\"knowledge_handbooks_Cách_khắc_phục_bưởi_da_xanh_bị_vàng_đọt.md\",\n",
    "\"knowledge_handbooks_Cách_khắc_phục_hiện_tượng_nứt_trái_trên_cây_trồng.md\",\n",
    "\"knowledge_handbooks_Cách_khắc_phục_mít_xơ_đen.md\",\n",
    "\"knowledge_handbooks_Cách_làm_cho_hoa_cà_phê_ra_đồng_loạt.md\",\n",
    "\"knowledge_handbooks_Cách_phòng_trị_sâu_vẽ_bùa_trên_cây_cam.md\",\n",
    "\"knowledge_handbooks_Cách_phòng_trừ_bệnh_khô_cành_khô_quả_gây_hại_cây_cà_phê.md\",\n",
    "\"knowledge_handbooks_Cách_trị_sâu_vẽ_bùa_trên_bưởi.md\",\n",
    "\"knowledge_handbooks_Cách_tưới_nước_cho_mô_hình_trồng_hồ_tiêu_xen_cà_phê.md\",\n",
    "\"knowledge_handbooks_Cam_sành_ra_bông__bị_mưa_nhiều_cần_làm_gì.md\",\n",
    "\"knowledge_handbooks_Cần_làm_gì_sau_khi_thu_hoạch_sầu_riêng.md\",\n",
    "\"knowledge_handbooks_Cây_cà_phê_già_cỗi_thì_phải_làm_thế_nào.md\",\n",
    "\"knowledge_handbooks_Có_cần_bón_phân_hữu_cơ_cho_đất_phèn_không.md\",\n",
    "\"knowledge_handbooks_Dứt_điểm_rệp_sáp__rầy_trắng_ở_phần_rễ.md\",\n",
    "\"knowledge_handbooks_Giống_cà_phê_vối_nào_chất_lượng_tốt_nhất_hiện_nay.md\",\n",
    "\"knowledge_handbooks_Giữ_ẩm_cho_đất_trong_mùa_khô_như_thế_nào.md\",\n",
    "\"knowledge_handbooks_Kích_thước_phẳng_của_bầu_ươm_cà_phê_ra_sao.md\",\n",
    "\"knowledge_handbooks_Kinh_nghiệm_chăm_sóc_sầu_riêng_giai_đoạn_nuôi_trái_non_hiệu_quả.md\",\n",
    "\"knowledge_handbooks_Kỹ_thuật_cắt_tỉa_cành_và_tạo_tán_cho_cà_phê.md\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "list_total_doc = os.listdir(folder_doc)\n",
    "for each_doc in list_total_doc:\n",
    "    if each_doc not in documents_test:\n",
    "        os.remove(folder_doc + each_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_doc = \"../app/api/data/training_data/\"\n",
    "documents_content = []\n",
    "for name_doc in documents_test:\n",
    "    with open(folder_doc + name_doc, \"r\") as f:\n",
    "        documents_content.append((name_doc,f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs = [LangchainDocument(page_content=doc[1], metadata={\"source\": doc[0]}) for doc in tqdm(documents_content)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# repo_id = \"noah-ai/mt5-base-question-generation-vi\"\n",
    "# repo_id= \"NlpHUST/gpt2-vietnamese\"\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token='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',\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={ \n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( docs_processed[5].page_content, \"\\n________________\", call_llm(llm_client, docs_processed[5].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_generation_prompt = \"\"\"\n",
    "# Your task is to write a factoid question and an answer given a context.\n",
    "# Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "# Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "# This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Output:::\n",
    "# Factoid question: (your factoid question)\n",
    "# Answer: (your answer to the factoid question)\n",
    "\n",
    "# Now here is the context.\n",
    "\n",
    "# Context: {context}\\n\n",
    "# Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\" Nhiệm vụ của bạn là viết một câu hỏi thực tế và một câu trả lời dựa trên ngữ cảnh.\n",
    "Câu hỏi thực tế của bạn phải được trả lời bằng một đoạn thông tin thực tế cụ thể, ngắn gọn từ ngữ cảnh.\n",
    "Câu hỏi thực tế của bạn phải được xây dựng theo phong cách giống như những câu hỏi mà người dùng có thể hỏi trong công cụ tìm kiếm.\n",
    "Điều này có nghĩa là câu hỏi thực tế của bạn KHÔNG PHẢI đề cập đến những thứ như \"theo đoạn văn\" hoặc \"ngữ cảnh\".\n",
    "Toàn bộ câu trả lời và câu hỏi phải được viết bằng tiếng Việt.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Kết quả:::\n",
    "Câu hỏi thực tế: (câu hỏi thực tế của bạn)\n",
    "Trả lời: (câu trả lời của bạn cho câu hỏi thực tế)\n",
    "\n",
    "Đây là bối cảnh.\n",
    "Bối cảnh: {context}\\n\n",
    "Kết quả:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 50  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    # print(output_QA_couple)\n",
    "    # print(\"_______________________________________\")\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Câu hỏi thực tế: \")[-1].split(\"Trả lời: \")[0]\n",
    "        answer = output_QA_couple.split(\"Trả lời: \")[-1]\n",
    "        print(question,answer )\n",
    "        # assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pd.DataFrame(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_groundedness_critique_prompt = \"\"\"\n",
    "# You will be given a context and a question.\n",
    "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here are the question and context.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Context: {context}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_relevance_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_standalone_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "Bạn sẽ được cung cấp một bối cảnh và một câu hỏi.\n",
    "Nhiệm vụ của bạn là đưa ra 'Điểm đánh giá' cho điểm mức độ một người có thể trả lời câu hỏi đã cho một cách rõ ràng với bối cảnh nhất định.\n",
    "Đưa ra câu trả lời của bạn theo thang điểm từ 1 đến 5, trong đó 1 có nghĩa là câu hỏi hoàn toàn không thể trả lời được trong bối cảnh và 5 có nghĩa là câu hỏi có thể trả lời rõ ràng và rõ ràng với bối cảnh.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Trả lời:::\n",
    "Đánh giá: (lý do đánh giá của bạn, dưới dạng văn bản)\n",
    "Điểm đánh giá: (Điểm đánh giá của bạn, tính bằng số từ 1 đến 5)\n",
    "\n",
    "Bạn PHẢI cung cấp các giá trị cho 'Đánh giá:' và 'Điểm đánh giá:' trong câu trả lời của mình.\n",
    "\n",
    "Bây giờ đây là câu hỏi và bối cảnh.\n",
    "\n",
    "Câu hỏi: {question}\\n\n",
    "Bối cảnh: {context}\\n\n",
    "Trả lời::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "Bạn sẽ được đưa ra một câu hỏi.\n",
    "Nhiệm vụ của bạn là cung cấp 'Điểm xếp hạng' thể hiện mức độ hữu ích của câu hỏi này đối với các nhà phát triển máy học đang xây dựng các ứng dụng NLP với hệ sinh thái Hugging Face.\n",
    "Đưa ra câu trả lời của bạn theo thang điểm từ 1 đến 5, trong đó 1 có nghĩa là câu hỏi không hữu ích chút nào và 5 có nghĩa là câu hỏi cực kỳ hữu ích.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Trả lời:::\n",
    "Đánh giá: (lý do đánh giá của bạn, dưới dạng văn bản)\n",
    "Điểm đánh giá: (Điểm đánh giá của bạn, tính bằng số từ 1 đến 5)\n",
    "\n",
    "Bạn PHẢI cung cấp các giá trị cho 'Đánh giá:' và 'Điểm đánh giá:' trong câu trả lời của mình.\n",
    "\n",
    "Bây giờ đây là câu hỏi.\n",
    "\n",
    "Câu hỏi: {question}\\n\n",
    "Trả lời::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "Bạn sẽ được đưa ra một câu hỏi.\n",
    "Nhiệm vụ của bạn là cung cấp 'Điểm xếp hạng' thể hiện mức độ độc lập của câu hỏi này với bối cảnh.\n",
    "Đưa ra câu trả lời của bạn theo thang điểm từ 1 đến 5, trong đó 1 có nghĩa là câu hỏi phụ thuộc vào thông tin bổ sung để hiểu. 5 có nghĩa là bản thân câu hỏi có ý nghĩa và có thể hiểu được.\n",
    "Ví dụ: nếu câu hỏi đề cập đến một cài đặt cụ thể, như 'trong bối cảnh' hoặc 'trong tài liệu' thì xếp hạng phải là 1.\n",
    "Các câu hỏi có thể chứa các danh từ hoặc từ viết tắt kỹ thuật khó hiểu như Gradio, Hub, Hugging Face và vẫn ở mức 5: nó chỉ đơn giản là phải rõ ràng đối với người vận hành có quyền truy cập vào tài liệu về nội dung câu hỏi.\n",
    "\n",
    "Ví dụ: \"Tổ chức Hugging Face năm 2023 có bao nhiêu người dùng?\" sẽ nhận được điểm 1, vì có sự đề cập ngầm đến một ngữ cảnh, do đó câu hỏi không độc lập với ngữ cảnh.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Trả lời:::\n",
    "Đánh giá: (lý do đánh giá của bạn, dưới dạng văn bản)\n",
    "Điểm đánh giá: (Điểm đánh giá của bạn, tính bằng số từ 1 đến 5)\n",
    "\n",
    "Bạn PHẢI cung cấp các giá trị cho 'Đánh giá:' và 'Điểm đánh giá:' trong câu trả lời của mình.\n",
    "\n",
    "Bây giờ đây là câu hỏi.\n",
    "\n",
    "Câu hỏi: {question}\\n\n",
    "Trả lời::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "# outputs_2 = outputs[1:4]\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                evaluation.split(\"Điểm đánh giá: \")[-1].strip(),\n",
    "                evaluation.split(\"Điểm đánh giá: \")[-2].split(\"Đánh giá: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # print(\"______________________________________________________\")\n",
    "        continue\n",
    "    # print(output)\n",
    "    # print(\"______________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 20)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "generated_questions_process = generated_questions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_score(score_string):\n",
    "    if score_string is np.nan:\n",
    "        return np.nan\n",
    "    return int(str(score_string)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process.iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions_process['groundedness_score'] = generated_questions_process['groundedness_score'].apply(process_score)\n",
    "generated_questions_process['relevance_score'] = generated_questions_process['relevance_score'].apply(process_score)\n",
    "generated_questions_process['standalone_score'] = generated_questions_process['standalone_score'].apply(process_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions_process[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions_process_2 = generated_questions_process.loc[\n",
    "    (generated_questions_process[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions_process[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions_process[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions_process_2[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions_process_2, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.to_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG PIPELINE TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../app/api/')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llm import *\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "from helpers import *\n",
    "from models import *\n",
    "from config import *\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "import logging\n",
    "from typing import (\n",
    "    List,\n",
    "    Union,\n",
    "    Optional,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Any\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangChainDocument\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from fastapi import HTTPException\n",
    "from uuid import UUID, uuid4\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    MarkdownTextSplitter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER=\"postgres\"\n",
    "POSTGRES_PASSWORD=\"postgres\"\n",
    "POSTGRES_DB=\"postgres\"\n",
    "PGVECTOR_ADD_INDEX=True\n",
    "\n",
    "# DB_HOST=localhost\n",
    "DB_HOST=\"localhost\"\n",
    "DB_PORT=5432\n",
    "# DB_PORT=5132\n",
    "DB_USER=\"api\"\n",
    "DB_NAME=\"api\"\n",
    "DB_PASSWORD=123\n",
    "VECTOR_EMBEDDINGS_DIM = 768\n",
    "OPENAI_API_KEY='sk-proj-SP7z7Y29wCjNbtRnoRoRT3BlbkFJM4oUr3l8Mv0X6vBdKqF7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SU_DSN = (\n",
    "    f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engine_test(dsn: str = SU_DSN):\n",
    "    return create_engine(dsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_engine = Session(get_engine_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_random_agent():\n",
    "    return random.choice(AGENT_NAMES)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Get the count of tokens used\n",
    "# ----------------------------\n",
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def get_token_count(text: str):\n",
    "    if not text:\n",
    "        return 0\n",
    "\n",
    "    return OpenAI().get_num_tokens(text=text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VECTOR_EMBEDDINGS_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Query embedding search for similar documents\n",
    "# --------------------------------------------\n",
    "def get_nodes_by_embedding_test(\n",
    "    embeddings: List[float],\n",
    "    k: int = LLM_MIN_NODE_LIMIT,\n",
    "    distance_strategy: Optional[DISTANCE_STRATEGY] = LLM_DEFAULT_DISTANCE_STRATEGY,\n",
    "    distance_threshold: Optional[float] = LLM_DISTANCE_THRESHOLD,\n",
    "    session: Optional[Session] = None,\n",
    ") -> List[Node]:\n",
    "    # Convert embeddings array into sql string\n",
    "    embeddings_str = str(embeddings)\n",
    "\n",
    "    if distance_strategy == DISTANCE_STRATEGY.EUCLIDEAN:\n",
    "        distance_fn = \"match_node_euclidean\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.COSINE:\n",
    "        distance_fn = \"match_node_cosine\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.MAX_INNER_PRODUCT:\n",
    "        distance_fn = \"match_node_max_inner_product\"\n",
    "    else:\n",
    "        raise Exception(f\"Invalid distance strategy {distance_strategy}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Lets do a similarity search\n",
    "    # ---------------------------\n",
    "    sql = f\"\"\"SELECT * FROM {distance_fn}(\n",
    "    '{embeddings_str}'::vector({VECTOR_EMBEDDINGS_DIM}),\n",
    "    {float(distance_threshold)}::double precision,\n",
    "    {int(k)});\"\"\"\n",
    "    # print(sql)\n",
    "    # logger.debug(f'🔍 Query: {sql}')\n",
    "\n",
    "    # Execute query, convert results to Node objects\n",
    "    if not session:\n",
    "        with Session(get_engine_test()) as session:\n",
    "            nodes = session.exec(text(sql)).all()\n",
    "    else:\n",
    "        nodes = session.exec(text(sql)).all()\n",
    "    return [Node.by_uuid(str(node[0])) for node in nodes] if nodes else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Retrieve prompt template\n",
    "# ------------------------\n",
    "def get_prompt_template_2(\n",
    "    user_query: str = None,\n",
    "    context_str: str = None,\n",
    "    project: Optional[Project] = None,\n",
    "    organization: Optional[Organization] = None,\n",
    "    agent: str = None,\n",
    ") -> str:\n",
    "    agent = f\"{agent}, \" if agent else \"\"\n",
    "    user_query = user_query if user_query else \"\"\n",
    "    context_str = context_str if context_str else \"\"\n",
    "    organization = (\n",
    "        project.organization.display_name\n",
    "        if project\n",
    "        else organization.display_name\n",
    "        if organization\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    if not context_str or not user_query:\n",
    "        raise ValueError(\n",
    "            \"Missing required arguments context_str, user_query, organization, agent\"\n",
    "        )\n",
    "    system_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"[AGENT]:\n",
    "    Tôi sẽ trả lời các câu hỏi của [USER] chỉ bằng cách sử dụng  [DOCUMENT] và tuân theo [Quy tắc].\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context_str}\n",
    "\n",
    "    [QUY TẮC]:\n",
    "    Tôi sẽ chỉ trả lời các câu hỏi của người dùng bằng  [DOCUMENT] được cung cấp. Tôi sẽ tuân thủ các quy tắc sau:\n",
    "    - Tôi là nhân viên hỗ trợ khách hàng tốt nhất hiện nay\n",
    "    - Tôi sẽ trả lời toàn bộ nội dung trong [DOCUMENT]\n",
    "    - Tôi không bao giờ nói dối hay bịa ra những câu trả lời không được nêu rõ ràng trong [DOCUMENT]\n",
    "    - Nếu tôi không chắc chắn về câu trả lời hoặc câu trả lời không có rõ ràng trong [DOCUMENT], tôi sẽ nói: \"Tôi xin lỗi, tôi không biết phải trợ giúp điều đó như thế nào\".\n",
    "    - Tôi luôn giữ câu trả lời dài, phù hợp và súc tích.\n",
    "    - Tôi sẽ luôn phản hồi ở định dạng JSON bằng các khóa sau: \"message\" phản hồi của tôi cho người dùng.\n",
    "    \"\"\",\n",
    "            }\n",
    "        ]\n",
    "#     system_prompt = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": f\"\"\"[AGENT]:\n",
    "#  I will answer the [USER] questions using only the [DOCUMENT] and following the [RULES].\n",
    "\n",
    "# [DOCUMENT]:\n",
    "# {context_str}\n",
    "\n",
    "# [RULES]:\n",
    "# I will answer the user's questions using only the [DOCUMENT] provided. I will abide by the following rules:\n",
    "# - I am a kind and helpful human, the best customer support agent in existence\n",
    "# - I will answer all content  in [DOCUMENT]\n",
    "# - I never lie or invent answers not explicitly provided in [DOCUMENT]\n",
    "# - If I am unsure of the answer response or the answer is not explicitly contained in [DOCUMENT], I will say: \"I apologize, I'm not sure how to help with that\".\n",
    "# - I always keep my answers long, relevant and concise.\n",
    "# - I will always respond in JSON format with the following keys: \"message\" my response to the user, \"tags\" an array of short labels categorizing user input, \"is_escalate\" a boolean, returning false if I am unsure and true if I do have a relevant answer\n",
    "# \"\"\",\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "    return (system_prompt, f\"[USER]:\\n{user_query}\")\n",
    "# f\"[USER]:\\n{user_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_bartbert(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    LLM_CHUNK_SIZE = 1024\n",
    "    LLM_CHUNK_OVERLAP = 100\n",
    "    logger.debug(documents)\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=LLM_CHUNK_SIZE,\n",
    "                        chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                        add_start_index=True,\n",
    "                        separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                    )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # encoding = tokenizer_bartpho(arr_documents,padding='max_length',return_tensors='pt', truncation=True, max_length=1024)\n",
    "    # input_ids = encoding['input_ids']\n",
    "    # attention_mask = encoding['attention_mask'] \n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     features = bartpho(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # last_hidden_state, _ = features[0], features[1]\n",
    "    # embeddings = last_hidden_state.mean(dim=1)\n",
    "\n",
    "\n",
    "    # output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    # # output_segment = [each[0] for each in output_segment]\n",
    "    output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    # output_segment = [each[0] for each in output_segment]\n",
    "\n",
    "    total_segment_doc = []\n",
    "    for list_segment_in_doc in output_segment_doc:\n",
    "        segment_doc = \"\"\n",
    "        for each in list_segment_in_doc:\n",
    "            segment_doc += each + \" \"\n",
    "        total_segment_doc.append(segment_doc.strip())\n",
    "\n",
    "    encoding_doc = tokenizer_bartpho(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=int(MAX_TOKEN_EMBEDDINGS))\n",
    "    input_ids_doc = encoding_doc['input_ids']\n",
    "    attention_mask_doc = encoding_doc['attention_mask'] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = bartpho(input_ids_doc,attention_mask=attention_mask_doc)\n",
    "\n",
    "    last_hidden_state, _ = features[0], features[1]\n",
    "\n",
    "    embeddings = last_hidden_state.mean(dim=1)\n",
    "    print(arr_documents) \n",
    "    print(len(arr_documents))\n",
    "    print(total_segment_doc)\n",
    "    print(len(arr_documents))\n",
    "    print(embeddings)\n",
    "    print(embeddings.shape)\n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "import torch\n",
    "import numpy as nps\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/model.safetensors HTTP/1.1\" 404 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /vinai/phobert-base-v2/resolve/main/vocab.txt HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer_phobert = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')\n",
    "\n",
    "def get_embeddings_photbert(\n",
    "    document_data: str,\n",
    "    document_type: DOCUMENT_TYPE = DOCUMENT_TYPE.PLAINTEXT,\n",
    ") -> Tuple[List[str], List[float]]:\n",
    "    documents = [LangChainDocument(page_content=document_data)]\n",
    "    LLM_CHUNK_SIZE = 256\n",
    "    LLM_CHUNK_OVERLAP = 20\n",
    "    logger.debug(documents)\n",
    "    if document_type == DOCUMENT_TYPE.MARKDOWN:\n",
    "        doc_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        )\n",
    "    else:\n",
    "        # doc_splitter = CharacterTextSplitter(separator='\\r\\n',\n",
    "        #     chunk_size=LLM_CHUNK_SIZE, chunk_overlap=LLM_CHUNK_OVERLAP\n",
    "        # )\n",
    "        doc_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=LLM_CHUNK_SIZE,\n",
    "                        chunk_overlap=LLM_CHUNK_OVERLAP,\n",
    "                        add_start_index=True,\n",
    "                        separators=[\"\\r\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                    )\n",
    "\n",
    "    # Returns an array of Documents\n",
    "    split_documents = doc_splitter.split_documents(documents)\n",
    "    # Lets convert them into an array of strings for OpenAI\n",
    "    arr_documents = [doc.page_content for doc in split_documents]\n",
    "    output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in arr_documents]\n",
    "    total_segment_doc = []\n",
    "    for list_segment_in_doc in output_segment_doc:\n",
    "        segment_doc = \"\"\n",
    "        for each in list_segment_in_doc:\n",
    "            segment_doc += each + \" \"\n",
    "        total_segment_doc.append(segment_doc.strip())\n",
    "\n",
    "    encoding_doc = tokenizer_phobert(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=int(256))\n",
    "    input_ids_doc = encoding_doc['input_ids']\n",
    "    attention_mask_doc = encoding_doc['attention_mask'] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids_doc,attention_mask=attention_mask_doc)\n",
    "\n",
    "    last_hidden_state, _ = features[0], features[1]\n",
    "    embeddings = last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # print(arr_documents) \n",
    "    # print(len(arr_documents))\n",
    "    # print(total_segment_doc)\n",
    "    # print(len(arr_documents))\n",
    "    # print(embeddings)\n",
    "    # print(embeddings.shape)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return arr_documents, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Làm thế nào để tạo điều kiện đảm bảo thoát nước tốt và hạn chế ngập úng cho cây sầu riêng?\\n'\n",
    "# arr_documents, embeddings = get_embeddings_bartbert(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_documents, embeddings = get_embeddings_photbert(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance(question, embedding_question = None, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'Làm thế nào để tạo điều kiện đảm bảo thoát nước tốt và hạn chế ngập úng cho cây sầu riêng?\\n'\n",
    "    # if embedding_question is None:\n",
    "    #     try:\n",
    "    #         # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    #         arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    #     except Exception as e: \n",
    "    #         print(e)\n",
    "    #         return None, None ,None\n",
    "    #     query_embeddings = embeddings[0].tolist()\n",
    "    # else:\n",
    "    #     query_embeddings = embedding_question\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    print(question)\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    \n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        document = session.get(Node, nodes[0].id).document\n",
    "        if document not in list_doc:\n",
    "            list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        system_prompt, user_prompt = get_prompt_template_2(\n",
    "            user_query=question,\n",
    "            context_str=context_str,\n",
    "            project=project,\n",
    "            organization=organization,\n",
    "            # agent=agent_name,\n",
    "        )\n",
    "        try:\n",
    "            llm_response = json.loads(\n",
    "                            retrieve_llm_response_test(\n",
    "                            user_prompt,\n",
    "                            model=model,\n",
    "                            # max_output_tokens=256,\n",
    "                            prefix_messages=system_prompt,\n",
    "                            )\n",
    "                            )\n",
    "            return list_relevant_docs, llm_response.get('message'), query_embeddings\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            return None ,None, query_embeddings\n",
    "\n",
    "    return None, None, query_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "# try:  # load previous generations if they exist\n",
    "#     with open(output_file, \"r\") as f:\n",
    "#         outputs = json.load(f)\n",
    "# except:\n",
    "#     outputs = []\n",
    "number_error = 0\n",
    "while number_error < 1:\n",
    "    # number_error+=1 \n",
    "    outputs = []\n",
    "    # ds_continue = generated_questions.iloc[45:]\n",
    "\n",
    "    for index in tqdm(range(len(ds))):\n",
    "        if number_error >10:\n",
    "            break\n",
    "        example = ds.iloc[index]\n",
    "        question = example[\"question\"]\n",
    "        embedding_question = example[\"embedding_question\"]\n",
    "        print(example['generated_answer'],111111111111111111)\n",
    "        if example['generated_answer'] is not None:\n",
    "            continue\n",
    "        relevant_docs, answer,embedding_question  = test_rag_performance(question,embedding_question)\n",
    "        if answer is None:\n",
    "            number_error+=1\n",
    "\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "            break\n",
    "        print(\"=======================================================\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f'True answer: {example[\"answer\"]}')\n",
    "        ds.at[index,'generated_answer'] = answer\n",
    "        ds.at[index,'retrieved_docs'] = relevant_docs\n",
    "        ds.at[index,'embedding_question'] = embedding_question\n",
    "\n",
    "        # result = {\n",
    "        #     \"question\": question,\n",
    "        #     \"true_answer\": example[\"answer\"],\n",
    "        #     \"source_doc\": example[\"source_doc\"],\n",
    "        #     \"generated_answer\": answer,\n",
    "        #     \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        # }\n",
    "        # if result not in outputs:\n",
    "        #     outputs.append(result)\n",
    "\n",
    "        # result = {\n",
    "        #     \"question\": question,\n",
    "        #     \"true_answer\": example[\"answer\"],\n",
    "        #     \"source_doc\": example[\"source_doc\"],\n",
    "        #     \"generated_answer\": None,\n",
    "        #     \"retrieved_docs\": None,\n",
    "        # }\n",
    "        \n",
    "        # if result not in outputs:\n",
    "        #     outputs.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\" Nhiệm vụ của bạn là viết một câu trả lời để trả lời câu hỏi từ người dùng dựa trên bối cảnh.\n",
    "Câu trả lời toàn bộ nội dung trong bối  cảnh\n",
    "Bạn không bao giờ nói dối hay bịa ra những câu trả lời không được nêu rõ ràng trong bối cảnh\n",
    "Nếu bạn không chắc chắn về câu trả lời hoặc câu trả lời không có rõ ràng trong bối cảnh, bạn sẽ nói: \"Tôi xin lỗi, tôi không biết phải trợ giúp điều đó như thế nào\".\n",
    "Bạn luôn giữ câu trả lời dài, phù hợp và chính xa\n",
    "Câu trả lời bằng một đoạn thông tin thực tế cụ thể, ngắn gọn từ ngữ cảnh.\n",
    "Toàn bộ câu trả lời và câu hỏi phải được viết bằng tiếng Việt.\n",
    "\n",
    "Cung cấp câu trả lời của bạn như sau:\n",
    "\n",
    "Kết quả:::\n",
    "Trả lời: (câu trả lời của bạn cho câu hỏi thực tế)\n",
    "\n",
    "Đây là bối cảnh.\n",
    "Bối cảnh: {context}\\n\n",
    "Đây là câu hỏi.\n",
    "Câu hỏi của người dùng: {question}\\n\n",
    "Kết quả:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_answer_prompt = \"\"\"\n",
    "   [INST]\n",
    "    Bạn sẽ trả lời các câu hỏi của [USER] chỉ bằng cách sử dụng  [DOCUMENT] và tuân theo [Quy tắc].\n",
    "    [USER]:\\n{question}\n",
    "\n",
    "    [DOCUMENT]:\n",
    "    {context}\n",
    "\n",
    "    [QUY TẮC]:\n",
    "    Tôi sẽ chỉ trả lời các câu hỏi của người dùng bằng  [DOCUMENT] được cung cấp. Tôi sẽ tuân thủ các quy tắc sau:\n",
    "    - Bạn là nhân viên hỗ trợ khách hàng tốt nhất hiện nay\n",
    "    - Bạn sẽ trả lời toàn bộ nội dung trong [DOCUMENT]\n",
    "    - Bạn không bao giờ nói dối hay bịa ra những câu trả lời không được nêu rõ ràng trong [DOCUMENT]\n",
    "    - Nếu Bạn không chắc chắn về câu trả lời hoặc câu trả lời không có rõ ràng trong [DOCUMENT], Bạn sẽ nói: \"Tôi xin lỗi, tôi không biết phải trợ giúp điều đó như thế nào\".\n",
    "    - Bạn luôn giữ câu trả lời dài, phù hợp và súc tích.\n",
    "    - Bạn sẽ luôn phản hồi kết quả của bạn ở định dạng JSON \n",
    "    - Chỉ cần tạo đối tượng JSON bằng khóa sau: \"message\" là phản hồi của bạn  cho người dùng. Không cần giải thích:\n",
    "    [/INST]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_performance_2(question, model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "    # question = 'Làm thế nào để tạo điều kiện đảm bảo thoát nước tốt và hạn chế ngập úng cho cây sầu riêng?\\n'\n",
    "    # arr_documents, embeddings = get_embeddings_test(question)\n",
    "    arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    print(question)\n",
    "    session= main_engine\n",
    "    LLM_MIN_NODE_LIMIT=3\n",
    "    LLM_DEFAULT_DISTANCE_STRATEGY=\"EUCLIDEAN\"\n",
    "    LLM_DISTANCE_THRESHOLD = 0.2\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "\n",
    "    nodes = get_nodes_by_embedding_test(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                                    distance_threshold=LLM_DISTANCE_THRESHOLD,\n",
    "                                    session=session\n",
    "                                )\n",
    "    print(nodes)\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        for node in nodes:\n",
    "            document = session.get(Node, node.id).document\n",
    "            if document not in list_doc:\n",
    "                list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "\n",
    "        # system_prompt, user_prompt = get_prompt_template_2(\n",
    "        #     user_query=question,\n",
    "        #     context_str=context_str,\n",
    "        #     project=project,\n",
    "        #     organization=organization,\n",
    "        #     # agent=agent_name,\n",
    "        # )\n",
    "        # try:\n",
    "        # repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "        # llm_client = InferenceClient(\n",
    "        #     model=repo_id,\n",
    "        #     timeout=120,\n",
    "        #     token='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',\n",
    "        # )\n",
    "\n",
    "        \n",
    "        # llm_response = call_llm(llm_client,QA_generation_answer_prompt.format(context=context_str, question=question))\n",
    "        llm_response = None\n",
    "        # llm_response = json.loads(\n",
    "        #                 retrieve_llm_response_test(\n",
    "        #                 user_prompt,\n",
    "        #                 model=model,\n",
    "        #                 # max_output_tokens=256,\n",
    "        #                 prefix_messages=system_prompt,\n",
    "        #                 )\n",
    "                        # )\n",
    "                \n",
    "        return list_relevant_docs, llm_response,list_doc\n",
    "        # except:\n",
    "        #     return None ,None\n",
    "\n",
    "    return None, None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions\n",
    "ds['generated_answer'] = None\n",
    "ds['retrieved_docs'] = None\n",
    "ds['embedding_question'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index =3\n",
    "ds.iloc[index].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_relevant_docs, llm_response,list_doc = test_rag_performance_2(ds.iloc[index].question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SPECIFIC EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as nps\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'E:/TrungPhanADVN/Code/LangChain_RAG/scripts/bartpho-word/'\n",
    "bartpho = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_bartpho = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Query embedding search for similar documents\n",
    "# --------------------------------------------\n",
    "def get_nodes_by_embedding_custom(\n",
    "    embeddings: List[float],\n",
    "    k: int = LLM_MIN_NODE_LIMIT,\n",
    "    distance_strategy: Optional[DISTANCE_STRATEGY] = LLM_DEFAULT_DISTANCE_STRATEGY,\n",
    "    distance_threshold: Optional[float] = LLM_DISTANCE_THRESHOLD,\n",
    "    session: Optional[Session] = main_engine,\n",
    ") -> List[Node]:\n",
    "    # Convert embeddings array into sql string\n",
    "    embeddings_str = str(embeddings)\n",
    "\n",
    "    if distance_strategy == DISTANCE_STRATEGY.EUCLIDEAN:\n",
    "        distance_fn = \"match_node_euclidean_2\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.COSINE:\n",
    "        distance_fn = \"match_node_cosine_2\"\n",
    "    elif distance_strategy == DISTANCE_STRATEGY.MAX_INNER_PRODUCT:\n",
    "        distance_fn = \"match_node_max_inner_product_2\"\n",
    "    else:\n",
    "        raise Exception(f\"Invalid distance strategy {distance_strategy}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Lets do a similarity search\n",
    "    # ---------------------------\n",
    "    sql = f\"\"\"SELECT * FROM {distance_fn}(\n",
    "    '{embeddings_str}'::vector({VECTOR_EMBEDDINGS_DIM}),\n",
    "    {float(distance_threshold)}::double precision,\n",
    "    {int(k)});\"\"\"\n",
    "    # print(sql)\n",
    "    # logger.debug(f'🔍 Query: {sql}')\n",
    "\n",
    "    # Execute query, convert results to Node objects\n",
    "    if not session:\n",
    "        with Session(get_engine_test()) as session:\n",
    "            nodes = session.exec(text(sql)).all()\n",
    "    else:\n",
    "        nodes = session.exec(text(sql)).all()\n",
    "    return [Node.by_uuid(str(node[0])) for node in nodes] if nodes else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_embedding_performance(question, strategy = DISTANCE_STRATEGY.EUCLIDEAN, threshold = 0.3,model: Optional[LLM_MODELS] = LLM_MODELS.GPT_35_TURBO):\n",
    "\n",
    "\n",
    "    arr_documents, embeddings = get_embeddings_photbert(question)\n",
    "    # arr_documents, embeddings = get_embeddings_bartbert(question)\n",
    "    # session= Session(get_engine_test())\n",
    "    LLM_MIN_NODE_LIMIT = 5\n",
    "    session = main_engine\n",
    "    query_embeddings = embeddings[0].tolist()\n",
    "    # print(query_embeddings)\n",
    "\n",
    "    nodes = get_nodes_by_embedding_custom(query_embeddings,\n",
    "                                    LLM_MIN_NODE_LIMIT,\n",
    "                                    distance_strategy=strategy,\n",
    "                                    distance_threshold=threshold,\n",
    "                                    session=main_engine\n",
    "                                )\n",
    "\n",
    "    # nodes = get_nodes(query_embeddings,\n",
    "    #                             LLM_MIN_NODE_LIMIT,\n",
    "    #                             distance_strategy=strategy,\n",
    "    #                             distance_threshold=threshold,\n",
    "    #                             session=session\n",
    "    #                         )\n",
    "    list_doc = []\n",
    "    list_relevant_docs = []\n",
    "    if len(nodes) > 0:\n",
    "        # if (not project or not organization) and session:\n",
    "            # get document from Node via session object:\n",
    "        for node in nodes:\n",
    "            document = session.get(Node, node.id).document\n",
    "            if document not in list_doc:\n",
    "                list_doc.append(document)\n",
    "        project = document.project\n",
    "        organization = project.organization\n",
    "        context_str = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        list_relevant_docs = [node.text for node in nodes]\n",
    "    return list_doc, list_relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Công dụng của cây nha đam đối với sức khỏe?\\n')]\n",
      "[Document(page_content='Công dụng của cây nha đam đối với sức khỏe?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/29 [00:01<00:49,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi nào là thời điểm tốt nhất để thực hiện kỹ thuật cắt tỉa và tạo tán cho cây cà phê?\\n')]\n",
      "[Document(page_content='Khi nào là thời điểm tốt nhất để thực hiện kỹ thuật cắt tỉa và tạo tán cho cây cà phê?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/29 [00:02<00:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để cà phê ra hoa đồng loạt?\\n')]\n",
      "[Document(page_content='Làm thế nào để cà phê ra hoa đồng loạt?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/29 [00:04<00:33,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Chuối rẽ quạt có phải là loại chuối?\\n')]\n",
      "[Document(page_content='Chuối rẽ quạt có phải là loại chuối?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 4/29 [00:05<00:32,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Cách phun loại thuốc lưu dẫn ở giai đoạn mới nhú đọt non?\\n')]\n",
      "[Document(page_content='Cách phun loại thuốc lưu dẫn ở giai đoạn mới nhú đọt non?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/29 [00:06<00:33,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để khắc phục nứt trái trên cây trồng?\\n')]\n",
      "[Document(page_content='Làm thế nào để khắc phục nứt trái trên cây trồng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 6/29 [00:08<00:32,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Tại sao 2Nông hỗ trợ truy xuất nguồn gốc sản phẩm?\\n')]\n",
      "[Document(page_content='Tại sao 2Nông hỗ trợ truy xuất nguồn gốc sản phẩm?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 7/29 [00:09<00:30,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để phòng trừ côn trùng và nấm bệnh cho cây sầu riêng?\\n')]\n",
      "[Document(page_content='Làm thế nào để phòng trừ côn trùng và nấm bệnh cho cây sầu riêng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 8/29 [00:10<00:27,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi nào là thời điểm tốt nhất để bón đạm cho lúa?\\n')]\n",
      "[Document(page_content='Khi nào là thời điểm tốt nhất để bón đạm cho lúa?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 9/29 [00:12<00:26,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để trồng và chăm sóc cây chu đinh lan?\\n')]\n",
      "[Document(page_content='Làm thế nào để trồng và chăm sóc cây chu đinh lan?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 10/29 [00:13<00:24,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Cách khắc phục mít xơ đen?\\n')]\n",
      "[Document(page_content='Cách khắc phục mít xơ đen?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 11/29 [00:14<00:23,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Các biện pháp phòng trừ bệnh khô cành khô quả cho cây cà phê?\\n')]\n",
      "[Document(page_content='Các biện pháp phòng trừ bệnh khô cành khô quả cho cây cà phê?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 12/29 [00:16<00:22,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Thủy tùng là loài thực vật nào?\\n')]\n",
      "[Document(page_content='Thủy tùng là loài thực vật nào?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 13/29 [00:17<00:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi nào Tre Mạnh Tông được mô tả khoa học đầu tiên?\\n')]\n",
      "[Document(page_content='Khi nào Tre Mạnh Tông được mô tả khoa học đầu tiên?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 14/29 [00:18<00:19,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để chăm sóc đúng cây lan Vanda?\\n')]\n",
      "[Document(page_content='Làm thế nào để chăm sóc đúng cây lan Vanda?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 15/29 [00:19<00:17,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Sản lượng lúa thu hoạch năm 2022 của Việt Nam ước đạt bao nhiêu tấn?\\n')]\n",
      "[Document(page_content='Sản lượng lúa thu hoạch năm 2022 của Việt Nam ước đạt bao nhiêu tấn?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 16/29 [00:21<00:16,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Loài thực vật thuộc chi Sầu riêng nào là loài phổ biến nhất?\\n')]\n",
      "[Document(page_content='Loài thực vật thuộc chi Sầu riêng nào là loài phổ biến nhất?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 17/29 [00:22<00:15,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Tại sao cần loại bỏ các cành không mang trái, cành già yếu, cành bị nhiễm bệnh?\\n')]\n",
      "[Document(page_content='Tại sao cần loại bỏ các cành không mang trái, cành già yếu, cành bị nhiễm bệnh?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 18/29 [00:23<00:13,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Cách phòng trị sâu vẽ bùa trên cây cam?\\n')]\n",
      "[Document(page_content='Cách phòng trị sâu vẽ bùa trên cây cam?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 19/29 [00:24<00:12,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Sau khi thu hoạch sầu riêng, cần làm gì để chăm sóc cây?\\n')]\n",
      "[Document(page_content='Sau khi thu hoạch sầu riêng, cần làm gì để chăm sóc cây?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 20/29 [00:26<00:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Đâu là tên gọi tiếng Việt của loài dừa nước?\\n')]\n",
      "[Document(page_content='Đâu là tên gọi tiếng Việt của loài dừa nước?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 21/29 [00:27<00:10,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để điều trị sâu ăn lá hoặc bám vào cây hoa Lan Cattleya?\\n')]\n",
      "[Document(page_content='Làm thế nào để điều trị sâu ăn lá hoặc bám vào cây hoa Lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 22/29 [00:29<00:09,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để bổ sung chất hữu cơ cho đất sau thu hoạch đối với vườn cây ăn trái?\\n')]\n",
      "[Document(page_content='Làm thế nào để bổ sung chất hữu cơ cho đất sau thu hoạch đối với vườn cây ăn trái?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 23/29 [00:30<00:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để chăm sóc hoa lan Cattleya?\\n')]\n",
      "[Document(page_content='Làm thế nào để chăm sóc hoa lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 24/29 [00:31<00:06,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Lượng nước tưới cho một cây hồ tiêu trong mô hình trồng hồ tiêu xen cà phê?\\n')]\n",
      "[Document(page_content='Lượng nước tưới cho một cây hồ tiêu trong mô hình trồng hồ tiêu xen cà phê?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 25/29 [00:33<00:05,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Đâu là nơi có nguồn gốc của mía?\\n')]\n",
      "[Document(page_content='Đâu là nơi có nguồn gốc của mía?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 26/29 [00:34<00:03,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Tên khoa học của cây đủng đỉnh là gì?\\n')]\n",
      "[Document(page_content='Tên khoa học của cây đủng đỉnh là gì?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 27/29 [00:35<00:02,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để chăm sóc đúng cây lan chi?\\n')]\n",
      "[Document(page_content='Làm thế nào để chăm sóc đúng cây lan chi?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [00:36<00:01,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Thốt nốt có phân bố ở đâu?\\n')]\n",
      "[Document(page_content='Thốt nốt có phân bố ở đâu?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:38<00:00,  1.32s/it]\n",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Công dụng của cây nha đam đối với sức khỏe?\\n')]\n",
      "[Document(page_content='Công dụng của cây nha đam đối với sức khỏe?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/29 [00:01<00:44,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi nào là thời điểm tốt nhất để thực hiện kỹ thuật cắt tỉa và tạo tán cho cây cà phê?\\n')]\n",
      "[Document(page_content='Khi nào là thời điểm tốt nhất để thực hiện kỹ thuật cắt tỉa và tạo tán cho cây cà phê?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/29 [00:02<00:38,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để cà phê ra hoa đồng loạt?\\n')]\n",
      "[Document(page_content='Làm thế nào để cà phê ra hoa đồng loạt?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/29 [00:04<00:37,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Chuối rẽ quạt có phải là loại chuối?\\n')]\n",
      "[Document(page_content='Chuối rẽ quạt có phải là loại chuối?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 4/29 [00:05<00:35,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Cách phun loại thuốc lưu dẫn ở giai đoạn mới nhú đọt non?\\n')]\n",
      "[Document(page_content='Cách phun loại thuốc lưu dẫn ở giai đoạn mới nhú đọt non?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/29 [00:06<00:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để khắc phục nứt trái trên cây trồng?\\n')]\n",
      "[Document(page_content='Làm thế nào để khắc phục nứt trái trên cây trồng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 6/29 [00:08<00:30,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Tại sao 2Nông hỗ trợ truy xuất nguồn gốc sản phẩm?\\n')]\n",
      "[Document(page_content='Tại sao 2Nông hỗ trợ truy xuất nguồn gốc sản phẩm?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 7/29 [00:09<00:28,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để phòng trừ côn trùng và nấm bệnh cho cây sầu riêng?\\n')]\n",
      "[Document(page_content='Làm thế nào để phòng trừ côn trùng và nấm bệnh cho cây sầu riêng?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 8/29 [00:10<00:25,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi nào là thời điểm tốt nhất để bón đạm cho lúa?\\n')]\n",
      "[Document(page_content='Khi nào là thời điểm tốt nhất để bón đạm cho lúa?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 9/29 [00:11<00:24,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để trồng và chăm sóc cây chu đinh lan?\\n')]\n",
      "[Document(page_content='Làm thế nào để trồng và chăm sóc cây chu đinh lan?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 10/29 [00:13<00:24,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Cách khắc phục mít xơ đen?\\n')]\n",
      "[Document(page_content='Cách khắc phục mít xơ đen?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 11/29 [00:14<00:23,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Các biện pháp phòng trừ bệnh khô cành khô quả cho cây cà phê?\\n')]\n",
      "[Document(page_content='Các biện pháp phòng trừ bệnh khô cành khô quả cho cây cà phê?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 12/29 [00:15<00:21,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Thủy tùng là loài thực vật nào?\\n')]\n",
      "[Document(page_content='Thủy tùng là loài thực vật nào?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 13/29 [00:16<00:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Khi nào Tre Mạnh Tông được mô tả khoa học đầu tiên?\\n')]\n",
      "[Document(page_content='Khi nào Tre Mạnh Tông được mô tả khoa học đầu tiên?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 14/29 [00:18<00:19,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để chăm sóc đúng cây lan Vanda?\\n')]\n",
      "[Document(page_content='Làm thế nào để chăm sóc đúng cây lan Vanda?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 15/29 [00:19<00:19,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Sản lượng lúa thu hoạch năm 2022 của Việt Nam ước đạt bao nhiêu tấn?\\n')]\n",
      "[Document(page_content='Sản lượng lúa thu hoạch năm 2022 của Việt Nam ước đạt bao nhiêu tấn?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 16/29 [00:21<00:17,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Loài thực vật thuộc chi Sầu riêng nào là loài phổ biến nhất?\\n')]\n",
      "[Document(page_content='Loài thực vật thuộc chi Sầu riêng nào là loài phổ biến nhất?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 17/29 [00:22<00:16,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Tại sao cần loại bỏ các cành không mang trái, cành già yếu, cành bị nhiễm bệnh?\\n')]\n",
      "[Document(page_content='Tại sao cần loại bỏ các cành không mang trái, cành già yếu, cành bị nhiễm bệnh?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 18/29 [00:23<00:14,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Cách phòng trị sâu vẽ bùa trên cây cam?\\n')]\n",
      "[Document(page_content='Cách phòng trị sâu vẽ bùa trên cây cam?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 19/29 [00:25<00:13,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Sau khi thu hoạch sầu riêng, cần làm gì để chăm sóc cây?\\n')]\n",
      "[Document(page_content='Sau khi thu hoạch sầu riêng, cần làm gì để chăm sóc cây?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 20/29 [00:26<00:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Đâu là tên gọi tiếng Việt của loài dừa nước?\\n')]\n",
      "[Document(page_content='Đâu là tên gọi tiếng Việt của loài dừa nước?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 21/29 [00:28<00:11,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để điều trị sâu ăn lá hoặc bám vào cây hoa Lan Cattleya?\\n')]\n",
      "[Document(page_content='Làm thế nào để điều trị sâu ăn lá hoặc bám vào cây hoa Lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 22/29 [00:29<00:09,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để bổ sung chất hữu cơ cho đất sau thu hoạch đối với vườn cây ăn trái?\\n')]\n",
      "[Document(page_content='Làm thế nào để bổ sung chất hữu cơ cho đất sau thu hoạch đối với vườn cây ăn trái?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 23/29 [00:30<00:08,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để chăm sóc hoa lan Cattleya?\\n')]\n",
      "[Document(page_content='Làm thế nào để chăm sóc hoa lan Cattleya?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 24/29 [00:32<00:06,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Lượng nước tưới cho một cây hồ tiêu trong mô hình trồng hồ tiêu xen cà phê?\\n')]\n",
      "[Document(page_content='Lượng nước tưới cho một cây hồ tiêu trong mô hình trồng hồ tiêu xen cà phê?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 25/29 [00:33<00:05,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Đâu là nơi có nguồn gốc của mía?\\n')]\n",
      "[Document(page_content='Đâu là nơi có nguồn gốc của mía?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 26/29 [00:35<00:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Tên khoa học của cây đủng đỉnh là gì?\\n')]\n",
      "[Document(page_content='Tên khoa học của cây đủng đỉnh là gì?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 27/29 [00:36<00:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Làm thế nào để chăm sóc đúng cây lan chi?\\n')]\n",
      "[Document(page_content='Làm thế nào để chăm sóc đúng cây lan chi?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [00:38<00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:config:[Document(page_content='Thốt nốt có phân bố ở đâu?\\n')]\n",
      "[Document(page_content='Thốt nốt có phân bố ở đâu?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:39<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "# time.sleep(5*60*60)\n",
    "generated_questions = pd.read_csv('E:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluat_dataset_2.csv',sep=\"|\")\n",
    "# ds = Dataset.from_pandas(generated_questions)\n",
    "ds = generated_questions.copy()\n",
    "ds['retrieved_docs_source'] = None\n",
    "ds['relevant_docs_text'] = None\n",
    "main_engine = Session(get_engine_test())\n",
    "VECTOR_EMBEDDINGS_DIM = 768\n",
    "dict_strategy = {1:DISTANCE_STRATEGY.EUCLIDEAN,\n",
    "                    2:DISTANCE_STRATEGY.COSINE,\n",
    "                    # 3:DISTANCE_STRATEGY.MAX_INNER_PRODUCT\n",
    "                    }\n",
    "for strategy_index in dict_strategy:\n",
    "    name_strategy = dict_strategy[strategy_index].value\n",
    "    model_name = 'vinai/phobert-base-v2' + \"_vncore_segment\"\n",
    "\n",
    "    # model_name = 'vinai/bartpho-word' + \"_vncore_segment\"\n",
    "    model_name = model_name.replace(\"/\",\"__\")\n",
    "    folder_name = \"e:/TrungPhanADVN/Code/LangChain_RAG/scripts/evaluate/embeddings_models/\"  + model_name + \"/\"\n",
    "    os.makedirs(folder_name , exist_ok=True)\n",
    "    chunk_size = 256\n",
    "    chunk_overlap = 20 \n",
    "    ds = generated_questions.copy()\n",
    "    ds['retrieved_docs_source'] = None\n",
    "    ds['relevant_docs_text'] = None\n",
    "    file_name_excel = model_name + \"_\" + str(chunk_size) + \"_\" + str(chunk_overlap) + \"_\" + name_strategy + '.xlsx'\n",
    "    file_name_csv = model_name + \"_\" + str(chunk_size) + \"_\" + str(chunk_overlap) + \"_\" + name_strategy + '.csv'\n",
    "\n",
    "    for index in tqdm(range(len(ds))):\n",
    "        example = ds.iloc[index]\n",
    "        question = example[\"question\"]\n",
    "        list_doc, list_relevant_docs = test_embedding_performance(question, dict_strategy[1])\n",
    "        ds.at[index,'retrieved_docs_source'] = [each.display_name for each in list_doc]\n",
    "        ds.at[index,'relevant_docs_text'] = list_relevant_docs\n",
    "    ds.to_csv(folder_name + file_name_csv,sep=\"|\", header=True, index=False)\n",
    "    ds.to_excel(folder_name + file_name_excel)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BARTBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import py_vncorenlp\n",
    "\n",
    "# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrsegmenter.word_segment(\"Đại học bách khoa Hồ Chí Minh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'E:/TrungPhanADVN/Code/LangChain_RAG/scripts/bartpho-word/'\n",
    "bartpho = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_bartpho = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document= 'Cách chăm sóc lúa giai đoạn đòng trổ giúp tăng năng suất hiệu quả'\n",
    "question = 'Cách phòng trị sâu vẽ bùa trên cây cam?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_ques = [rdrsegmenter.word_segment(doc) for doc in [question]]\n",
    "\n",
    "total_segment_ques = []\n",
    "for list_segment_ques in output_segment_ques:\n",
    "    segment_doc = \"\"\n",
    "    for each in list_segment_ques:\n",
    "        segment_doc += each + \" \"\n",
    "    total_segment_ques.append(segment_doc.strip()) \n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding__ques = tokenizer_bartpho(total_segment_ques,padding='max_length',return_tensors='pt', truncation=True, max_length=1024)\n",
    "\n",
    "input_ids_ques = encoding_ques['input_ids']\n",
    "attention_mask_ques = encoding_ques['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_ques = bartpho(input_ids_ques, attention_mask=attention_mask_ques)\n",
    "\n",
    "last_hidden_state_ques, _ = features_ques[0], features_ques[1]\n",
    "\n",
    "embeddings_ques = last_hidden_state_ques.mean(dim=1)\n",
    "print(question)\n",
    "print(total_segment_ques)\n",
    "print(last_hidden_state_ques.shape)\n",
    "print(embeddings_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in [test_document]]\n",
    "\n",
    "total_segment_doc = []\n",
    "for list_segment_doc in output_segment_doc:\n",
    "    segment_doc = \"\"\n",
    "    for each in list_segment_doc:\n",
    "        segment_doc += each + \" \"\n",
    "    total_segment_doc.append(segment_doc.strip()) \n",
    "\n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding_doc = tokenizer_bartpho(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=1024)\n",
    "\n",
    "input_ids_doc = encoding_doc['input_ids']\n",
    "attention_mask_doc = encoding_doc['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_doc = bartpho(input_ids_doc, attention_mask=attention_mask_doc)\n",
    "\n",
    "last_hidden_state_doc, _ = features_doc[0], features_doc[1]\n",
    "\n",
    "embeddings_doc = last_hidden_state_doc.mean(dim=1)\n",
    "print(test_document)\n",
    "print(total_segment_doc)\n",
    "print(last_hidden_state_doc.shape)\n",
    "print(embeddings_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_doc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(embeddings_doc[0],embeddings_ques[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(embeddings[0],embeddings_2[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.739711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(embeddings[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_documents = ['Loài thực vật thuộc chi Sầu riêng nào là loài phổ biến nhất?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHOBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "# tokenizer_phobert = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = 'Cách phòng trị sâu vẽ bùa trên cây cam?\\r\\n– Điều khiển đọt non ra đồng loạt\\r\\n\\r\\n– Phun loại thuốc lưu dẫn hoặc quét thuốc lưu dẫn ở giai đoạn mới nhú đọt non (bằng hạt gạo)'\n",
    "question = 'Cách phòng trị sâu vẽ bùa trên cây cam?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr_doc_ques, embed_ques = get_embeddings_photbert(question)\n",
    "arr_doc, embed_doc = get_embeddings_photbert(test_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_ques[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(embed_ques[0],embed_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document= 'Cách làm cho hoa cà phê ra đồng loạt?\\r\\nMuốn cà phê ra hoa đồng loạt cần xác định thời điểm tưới nước cho phù hợp. Việc xác định thời điểm tưới, lượng nước tưới, phương pháp tưới tùy thuộc vào điều kiện thời tiết, loại đất, tình trạng sinh trưởng của cây. Sau thời gian khô hạn, khi thấy nụ hoa có dạng mỏ sẻ xuất hiện đầy đủ ở đốt ngoài cùng của các cành thì tiến hành tưới nước cho cà phê. Việc tưới nước đúng thời điểm lần đầu (đợt 1) và đủ lượng nước tưới sẽ quyết định đến việc ra hoa đồng loạt.'\n",
    "# test_document = \"Cách phòng trị sâu vẽ bùa trên cây cam?\\r\\n– Điều khiển đọt non ra đồng loạt\\r\\n\\r\\n– Phun loại thuốc lưu dẫn hoặc quét thuốc lưu dẫn ở giai đoạn mới nhú đọt non (bằng hạt gạo)\"\n",
    "question = 'Cách phòng trị sâu vẽ bùa trên cây cam?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_ques = [rdrsegmenter.word_segment(doc) for doc in [question]]\n",
    "\n",
    "\n",
    "\n",
    "encoding_ques = tokenizer_phobert(output_segment_ques[0],padding='max_length',return_tensors='pt', truncation=True, max_length=256)\n",
    "input_ids_ques = encoding_ques['input_ids']\n",
    "attention_mask_ques = encoding_ques['attention_mask'] \n",
    "with torch.no_grad():\n",
    "    features_ques = phobert(input_ids_ques, attention_mask=attention_mask_ques)\n",
    "\n",
    "last_hidden_state_ques, _ = features_ques[0], features_ques[1]\n",
    "\n",
    "embeddings_ques = last_hidden_state_ques.mean(dim=1).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_ques.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in [test_document]]\n",
    "\n",
    "# total_segment_doc = []\n",
    "# for list_segment_doc in output_segment_doc:\n",
    "#     segment_doc = \"\"\n",
    "#     for each in list_segment_doc:\n",
    "#         segment_doc += each + \"\"\n",
    "#     total_segment_doc.append(segment_doc.strip()) \n",
    "output_segment_doc = [each for each in output_segment_doc]\n",
    "# print(total_segment_doc)\n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding_doc = tokenizer_phobert(output_segment_doc[0],padding='max_length',return_tensors='pt', truncation=True, max_length=256)\n",
    "\n",
    "input_ids_doc = encoding_doc['input_ids']\n",
    "attention_mask_doc = encoding_doc['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_doc = phobert(input_ids_doc, attention_mask=attention_mask_doc)\n",
    "\n",
    "last_hidden_state_doc, _ = features_doc[0], features_doc[1]\n",
    "\n",
    "embeddings_doc = last_hidden_state_doc.mean(dim=1).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_doc.mean(dim=1).mean(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_doc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document_1 = 'Cách làm cho hoa cà phê ra đồng loạt?\\r\\nMuốn cà phê ra hoa đồng loạt cần xác định thời điểm tưới nước cho phù hợp. Việc xác định thời điểm tưới, lượng nước tưới, phương pháp tưới tùy thuộc vào điều kiện thời tiết, loại đất, tình trạng sinh trưởng của cây. Sau thời gian khô hạn, khi thấy nụ hoa có dạng mỏ sẻ xuất hiện đầy đủ ở đốt ngoài cùng của các cành thì tiến hành tưới nước cho cà phê. Việc tưới nước đúng thời điểm lần đầu (đợt 1) và đủ lượng nước tưới sẽ quyết định đến việc ra hoa đồng loạt.'\n",
    "\n",
    "test_document_2 = 'Cách làm cho hoa cà phê ra đồng loạt?\\r\\nMuốn cà phê ra hoa đồng loạt cần xác định thời điểm tưới nước cho phù hợp. Việc xác định thời điểm tưới, lượng nước tưới, phương pháp tưới tùy thuộc vào điều kiện thời tiết, loại đất, tình trạng sinh trưởng của cây.'\n",
    "\n",
    "list_doc = [test_document_1, test_document_2]\n",
    "output_segment_doc = [rdrsegmenter.word_segment(doc) for doc in list_doc]\n",
    "\n",
    "total_segment_doc = []\n",
    "for list_segment_doc in output_segment_doc:\n",
    "    segment_doc = \"\"\n",
    "    for each in list_segment_doc:\n",
    "        segment_doc += each + \" \"\n",
    "    total_segment_doc.append(segment_doc.strip()) \n",
    "output_segment_doc = [each for each in output_segment_doc]\n",
    "\n",
    "# print(output_segment_doc)\n",
    "encoding_doc = tokenizer_phobert(total_segment_doc,padding='max_length',return_tensors='pt', truncation=True, max_length=256)\n",
    "\n",
    "input_ids_doc = encoding_doc['input_ids']\n",
    "attention_mask_doc = encoding_doc['attention_mask'] \n",
    "\n",
    "with torch.no_grad():\n",
    "    features_doc = phobert(input_ids_doc, attention_mask=attention_mask_doc)\n",
    "\n",
    "last_hidden_state_doc, _ = features_doc[0], features_doc[1]\n",
    "\n",
    "embeddings_doc = last_hidden_state_doc.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = \"\"\n",
    "for segment in rdrsegmenter.word_segment(test_document_1):\n",
    "    total_text += segment + \" \"\n",
    "total_text = total_text.strip()\n",
    "len(tokenizer_phobert.encode(total_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_segment_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state_doc.mean(dim=1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ques.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(last_hidden_state_doc.mean(dim=1)[0],embeddings_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_segment_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST GENERATED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import py_vncorenlp\n",
    "\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='./models/vncorenlp')\n",
    "\n",
    "\n",
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# tokenizer_generate = AutoTokenizer.from_pretrained(model_id)\n",
    "# model_generate = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer_generate.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model_generate.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORE WITH GENERATED ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT =\"\"\"###Mô tả công việc:\n",
    "Một hướng dẫn (có thể bao gồm Đầu vào bên trong nó), một phản hồi để đánh giá, một câu trả lời tham khảo đạt điểm 5 và thang điểm đại diện cho tiêu chí đánh giá sẽ được đưa ra.\n",
    "1. Viết phản hồi chi tiết để đánh giá chất lượng phản hồi dựa trên thang điểm cho sẵn, không đánh giá chung chung.\n",
    "2. Sau khi viết phản hồi, hãy viết điểm là số nguyên từ 1 đến 5. Bạn nên tham khảo bảng đánh giá.\n",
    "3. Định dạng đầu ra sẽ như sau: \\\"Phản hồi: {{viết phản hồi cho tiêu chí}} [KẾT QUẢ] {{một số nguyên từ 1 đến 5}}\\\"\n",
    "4. Vui lòng không đưa ra bất kỳ lời mở đầu, kết thúc và giải thích nào khác. Hãy chắc chắn bao gồm [KẾT QUẢ] trong đầu ra của bạn.\n",
    "\n",
    "###Hướng dẫn đánh giá:\n",
    "{instruction}\n",
    "\n",
    "###Phản hồi để đánh giá:\n",
    "{response}\n",
    "\n",
    "###Câu trả lời tham khảo (Điểm 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Tiêu chí chấm điểm:\n",
    "[Câu trả lời có đúng, chính xác và thực tế dựa trên câu trả lời tham khảo không?]\n",
    "Điểm 1: Câu trả lời hoàn toàn không chính xác, không chính xác và/hoặc không thực tế.\n",
    "Điểm 2: Câu trả lời phần lớn là không chính xác, không chính xác và/hoặc không thực tế.\n",
    "Điểm 3: Câu trả lời có phần đúng, chính xác và/hoặc thực tế.\n",
    "Điểm 4: Câu trả lời hầu hết đều đúng, chính xác và thực tế.\n",
    "Điểm 5: Câu trả lời hoàn toàn đúng, chính xác và thực tế.\n",
    "\n",
    "###Nhận xét:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "# 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "# 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "# 3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "# 4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Is the response correct, accurate, and factual based on the reference answer?]\n",
    "# Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "# Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "# Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "# Score 4: The response is mostly correct, accurate, and factual.\n",
    "# Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Bạn là một mô hình đánh giá ngôn ngữ công bằng\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "eval_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,openai_api_key=OPENAI_API_KEY)\n",
    "evaluator_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "# def evaluate_answers(\n",
    "#     answer_path: str,\n",
    "#     eval_chat_model: BaseChatModel,\n",
    "#     evaluator_name: str,\n",
    "#     evaluation_prompt_template: ChatPromptTemplate,\n",
    "# ) -> None:\n",
    "#     \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "#     answers = []\n",
    "# if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "#     answers = json.load(open(answer_path, \"r\"))\n",
    "answers= outputs\n",
    "evaluate_output = []\n",
    "for experiment in tqdm(answers):\n",
    "    if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "        continue\n",
    "\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        instruction=experiment[\"question\"],\n",
    "        response=experiment[\"generated_answer\"],\n",
    "        reference_answer=experiment[\"true_answer\"],\n",
    "    )\n",
    "    try:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    except:\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = None\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = None\n",
    "        continue\n",
    "    feedback, score = [item.strip() for item in eval_result.content.split(\"[KẾT QUẢ]\")]\n",
    "    experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "    experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "    # evaluate_output.append()\n",
    "    # with open(answer_path, \"w\") as f:\n",
    "    #     json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(answers,open('result_evaluate_1.json','w',encoding='utf-8'), indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(json.load(open('result_evaluate_1.json', \"r\")))\n",
    "result = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] ) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores = result[\"eval_score_GPT4\"].mean()\n",
    "# average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import datasets\n",
    "import pandas as pd\n",
    "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
    "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    scores,\n",
    "    color=scores,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "# fig.update_layout(w\n",
    "#     width=1000,\n",
    "#     height=600,\n",
    "#     barmode=\"group\",\n",
    "#     yaxis_range=[0, 100],\n",
    "#     title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "#     xaxis_title=\"RAG settings\",\n",
    "#     font=dict(size=15),\n",
    "# )\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 2: https://freedium.cfd/https://medium.com/m/global-identity-2?redirectUrl=https%3A%2F%2Ftowardsdatascience.com%2Fevaluating-rag-applications-with-ragas-81d67b0ee31a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
